{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "animal_model_woman",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AuZP58d8644",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "920e22e9-6a8b-45fb-ffd9-30f2e823cbbf"
      },
      "source": [
        "\n",
        "import os # miscellaneous operating system interfaces\n",
        "import shutil # high-level file operations\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import random\n",
        "from itertools import product\n",
        "from keras.models import Sequential,load_model\n",
        "from keras.layers import Conv2D \n",
        "from keras.layers import MaxPooling2D \n",
        "from keras.layers import Flatten \n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.applications import MobileNetV2, Xception, DenseNet121,ResNet50V2,NASNetMobile,InceptionV3, InceptionResNetV2,VGG19\n",
        "from keras.applications.mobilenet_v2 import preprocess_input\n",
        "from keras.layers import  Input, Conv2D, Conv2DTranspose, ReLU,AveragePooling2D, MaxPooling2D, Dense, BatchNormalization, GlobalAveragePooling2D, Concatenate,Softmax\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "\n",
        "base_dir = \"/content/drive/My Drive/Colab Notebooks/datasets/woman\"\n",
        "\n",
        "train_img_dir = \"/content/drive/My Drive/Colab Notebooks/datasets/woman/train\"\n",
        "\n",
        "test_img_dir = \"/content/drive/My Drive/Colab Notebooks/datasets/woman/test\"\n",
        "\n",
        "val_img_dir = \"/content/drive/My Drive/Colab Notebooks/datasets/woman/val\"\n",
        "\n",
        "\n",
        "learning_rate = 0.001\n",
        "training_epochs = 20 # traindata개수/batchsize\n",
        "batch_size = 512\n",
        "validation_steps = 15 # valdata개수/batchsize\n",
        "\n",
        "\n",
        "\n",
        "print(f'batchSIZE is {batch_size}, Learning Rate is {learning_rate}')\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "val_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "categories = ['dog','cat','rabbit','squirrel','deer','fox','penguin','snake']\n",
        "\n",
        "training_set = train_datagen.flow_from_directory(train_img_dir, target_size=(128,128), \n",
        "                                             classes=categories, \n",
        "                                             batch_size=batch_size)\n",
        "\n",
        "test_set = test_datagen.flow_from_directory(test_img_dir,\n",
        "                                        target_size=(128,128), \n",
        "                                        classes=categories, \n",
        "                                        batch_size=batch_size)\n",
        "\n",
        "val_set = test_datagen.flow_from_directory(val_img_dir,\n",
        "                                        target_size=(128,128), \n",
        "                                        classes=categories, \n",
        "                                        batch_size=batch_size)\n",
        "\n",
        "x_train, y_train = next(training_set)\n",
        "x_test, y_test = next(test_set)\n",
        "x_val,y_val = next(val_set)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batchSIZE is 512, Learning Rate is 0.001\n",
            "Found 1098 images belonging to 8 classes.\n",
            "Found 244 images belonging to 8 classes.\n",
            "Found 262 images belonging to 8 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDS8HPAA9CrJ"
      },
      "source": [
        "def create_model():\n",
        "    inputs = Input(shape=(128, 128, 3))\n",
        "    \n",
        "\n",
        "    conv2_1 = Conv2D(32, 3, 1, 'SAME')(inputs)\n",
        "    conv2_2 = Conv2D(32, 3, 1, 'SAME')(conv2_1)\n",
        "    pool2_3 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv2_2)\n",
        "    br1 = BatchNormalization()(pool2_3)\n",
        "    \n",
        "    conv2_1 = Conv2D(32, 3, 1, 'SAME')(br1)\n",
        "    conv2_2 = Conv2D(32, 3, 1, 'SAME')(conv2_1)\n",
        "    pool2_3 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv2_2)\n",
        "    br1 = BatchNormalization()(pool2_3)\n",
        "    \n",
        "    \n",
        "    conv3_1 = Conv2D(64, 3, 1, 'SAME')(br1)\n",
        "    conv3_2 = Conv2D(64, 3, 1, 'SAME')(conv3_1)\n",
        "    pool3_2 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv3_2)\n",
        "    br2 = BatchNormalization()(pool3_2)\n",
        "    \n",
        "    conv3_1 = Conv2D(64, 3, 1, 'SAME')(br2)\n",
        "    conv3_2 = Conv2D(64, 3, 1, 'SAME')(conv3_1)\n",
        "    pool3_2 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv3_2)\n",
        "    br2 = BatchNormalization()(pool3_2)\n",
        "    \n",
        "    \n",
        "    conv4_1 = Conv2D(128, 3, 1, 'SAME')(br2)\n",
        "    conv4_2 = Conv2D(128, 3, 1, 'SAME')(conv4_1)\n",
        "    pool4_2 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv4_2)\n",
        "    br3 = BatchNormalization()(pool4_2)\n",
        "    \n",
        "    conv4_1 = Conv2D(128, 3, 1, 'SAME')(br3)\n",
        "    conv4_2 = Conv2D(128, 3, 1, 'SAME')(conv4_1)\n",
        "    pool4_2 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv4_2)\n",
        "    br3 = BatchNormalization()(pool4_2)\n",
        "    \n",
        "    flatten1 = Flatten()(pool4_2)\n",
        "    dense1 = Dense(units = 512, activation = 'relu')(flatten1)\n",
        "    dense2 = Dense(units = 1024, activation = 'relu')(dense1)\n",
        "    dr1 = Dropout(0.7)(dense2)\n",
        "    dense3 = Dense(units = 5, activation = 'sigmoid')(dr1)\n",
        "    \n",
        "    return keras.Model(inputs=inputs, outputs=dense3) \n",
        "def mobile_net():\n",
        "    inputs = Input(shape=(128, 128, 3))\n",
        "    mobileNet = MobileNetV2(weights = \"imagenet\", include_top=False,input_shape = (128,128,3)\n",
        "                            ,input_tensor = inputs)\n",
        "    for layer in mobileNet.layers:\n",
        "        layer.trainable = True\n",
        "        \n",
        "    output = mobileNet.output\n",
        "    pooling = AveragePooling2D(pool_size=(16,16),padding='SAME')(output)\n",
        "    flatten1 = Flatten()(pooling)\n",
        "    dense1 = Dense(units = 256)(flatten1)\n",
        "    batch1 = BatchNormalization()(dense1)\n",
        "    relu1 = ReLU()(batch1)\n",
        "    dense2 = Dense(units = 64)(relu1)\n",
        "    batch2 = BatchNormalization()(dense2)\n",
        "    relu2 = ReLU()(batch2)\n",
        "    dense3 = Dense(units = 6, activation = 'sigmoid')(relu2)\n",
        "    \n",
        "    return keras.Model(inputs=inputs, outputs=dense3)\n",
        "\n",
        "def xception():\n",
        "    inputs = Input(shape=(128, 128, 3))\n",
        "    xception = Xception(weights = \"imagenet\", include_top=False,input_shape = (128,128,3)\n",
        "                            ,input_tensor = inputs)\n",
        "    for layer in xception.layers:\n",
        "        layer.trainable = True\n",
        "        \n",
        "    output = xception.output\n",
        "    pooling = AveragePooling2D(pool_size=(8,8),padding='SAME')(output)\n",
        "    flatten1 = Flatten()(pooling)\n",
        "    dense1 = Dense(units = 256)(flatten1)\n",
        "    batch1 = BatchNormalization()(dense1)\n",
        "    relu1 = ReLU()(batch1)\n",
        "    dense2 = Dense(units = 64)(relu1)\n",
        "    batch2 = BatchNormalization()(dense2)\n",
        "    relu2 = ReLU()(batch2)\n",
        "    dense3 = Dense(units = 6, activation = 'sigmoid')(relu2)\n",
        "    \n",
        "    return keras.Model(inputs=inputs, outputs=dense3)\n",
        "    # pooling = AveragePooling2D(pool_size=(4,4),padding='SAME')(output)\n",
        "    \n",
        "    # flatten1 = Flatten()(pooling)\n",
        "    # dense1 = Dense(units = 512, activation = 'relu')(flatten1)\n",
        "    # dense2 = Dense(units = 1024, activation = 'relu')(dense1)\n",
        "    # dr1 = Dropout(0.7)(dense2)\n",
        "    # dense3 = Dense(units = 5, activation = 'sigmoid')(dr1)\n",
        "    \n",
        "    # return keras.Model(inputs=inputs, outputs=dense3)\n",
        "\n",
        "def resnet():\n",
        "    inputs = Input(shape=(128, 128, 3))\n",
        "    resnet = ResNet50V2 (weights = \"imagenet\", include_top=False,input_shape = (128,128,3)\n",
        "                            ,input_tensor = inputs)\n",
        "    for layer in resnet.layers:\n",
        "        layer.trainable = True\n",
        "        \n",
        "    output = resnet.output\n",
        "    pooling = AveragePooling2D(pool_size=(8,8),padding='SAME')(output)\n",
        "    flatten1 = Flatten()(pooling)\n",
        "    dense1 = Dense(units = 128)(flatten1)\n",
        "    batch1 = BatchNormalization()(dense1)\n",
        "    relu1 = ReLU()(batch1)\n",
        "    dense2 = Dense(units = 32)(relu1)\n",
        "    batch2 = BatchNormalization()(dense2)\n",
        "    relu2 = ReLU()(batch2)\n",
        "    dense3 = Dense(units = 7, activation = 'sigmoid')(relu2)\n",
        "    \n",
        "    return keras.Model(inputs=inputs, outputs=dense3)\n",
        "\n",
        "def densenet():\n",
        "    inputs = Input(shape=(128, 128, 3))\n",
        "    densenet = DenseNet121(weights = \"imagenet\", include_top=False,input_shape = (128,128,3)\n",
        "                            ,input_tensor = inputs)\n",
        "    for layer in densenet.layers:\n",
        "        layer.trainable = True\n",
        "        \n",
        "    output = densenet.output\n",
        "    pooling = MaxPooling2D(pool_size=(32,32),padding='SAME')(output)\n",
        "    flatten1 = Flatten()(pooling)\n",
        "    dense1 = Dense(units = 128)(flatten1)\n",
        "    batch1 = BatchNormalization()(dense1)\n",
        "    relu1 = ReLU()(batch1)\n",
        "    dense2 = Dense(units = 32)(relu1)\n",
        "    batch2 = BatchNormalization()(dense2)\n",
        "    relu2 = ReLU()(batch2)\n",
        "    dense3 = Dense(units = 8, activation = 'sigmoid')(relu2)\n",
        "    \n",
        "    return keras.Model(inputs=inputs, outputs=dense3)\n",
        "\n",
        "def inception():\n",
        "    inputs = Input(shape=(128, 128, 3))\n",
        "    inception = InceptionV3(weights = \"imagenet\", include_top=False,input_shape = (128,128,3)\n",
        "                            ,input_tensor = inputs)\n",
        "    for layer in inception.layers:\n",
        "        layer.trainable = True\n",
        "        \n",
        "    output = inception.output\n",
        "    pooling = AveragePooling2D(pool_size=(32,32),padding='SAME')(output)\n",
        "    flatten1 = Flatten()(pooling)\n",
        "    dense1 = Dense(units = 96)(flatten1)\n",
        "    batch1 = BatchNormalization()(dense1)\n",
        "    relu1 = ReLU()(batch1)\n",
        "    dense2 = Dense(units = 24)(relu1)\n",
        "    batch2 = BatchNormalization()(dense2)\n",
        "    relu2 = ReLU()(batch2)\n",
        "    dense3 = Dense(units = 6, activation = 'sigmoid')(relu2)\n",
        "    \n",
        "    return keras.Model(inputs=inputs, outputs=dense3)\n",
        "\n",
        "def inceptionresnet():\n",
        "    inputs = Input(shape=(128, 128, 3))\n",
        "    inceptionresnet = InceptionResNetV2(weights = \"imagenet\", include_top=False,input_shape = (128,128,3)\n",
        "                            ,input_tensor = inputs)\n",
        "    for layer in inceptionresnet.layers:\n",
        "        layer.trainable = True\n",
        "        \n",
        "    output = inceptionresnet.output\n",
        "    pooling = MaxPooling2D(pool_size=(16,16),padding='SAME')(output)\n",
        "    flatten1 = Flatten()(pooling)\n",
        "    dense1 = Dense(units = 1024)(flatten1)\n",
        "    batch1 = BatchNormalization()(dense1)\n",
        "    relu1 = ReLU()(batch1)\n",
        "    dense2 = Dense(units = 512)(relu1)\n",
        "    batch2 = BatchNormalization()(dense2)\n",
        "    relu2 = ReLU()(batch2)\n",
        "    dense3 = Dense(units = 8, activation = 'sigmoid')(relu2)\n",
        "    \n",
        "    return keras.Model(inputs=inputs, outputs=dense3)\n",
        "\n",
        "def vgg():\n",
        "    inputs = Input(shape=(128, 128, 3))\n",
        "    vgg = VGG19(weights = \"imagenet\", include_top=False,input_shape = (128,128,3)\n",
        "                            ,input_tensor = inputs)\n",
        "    for layer in vgg.layers:\n",
        "        layer.trainable = True\n",
        "        \n",
        "    output = vgg.output\n",
        "    pooling = AveragePooling2D(pool_size=(32,32),padding='SAME')(output)\n",
        "    flatten1 = Flatten()(pooling)\n",
        "    dense1 = Dense(units = 96)(flatten1)\n",
        "    batch1 = BatchNormalization()(dense1)\n",
        "    relu1 = ReLU()(batch1)\n",
        "    dense2 = Dense(units = 24)(relu1)\n",
        "    batch2 = BatchNormalization()(dense2)\n",
        "    relu2 = ReLU()(batch2)\n",
        "    dense3 = Dense(units = 6, activation = 'sigmoid')(relu2)\n",
        "    \n",
        "    return keras.Model(inputs=inputs, outputs=dense3)\n",
        "\n",
        "\n",
        "class ResidualUnit(tf.keras.Model):\n",
        "    def __init__(self, filter_in, filter_out, kernel_size):\n",
        "        super(ResidualUnit, self).__init__()\n",
        "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
        "        self.conv1 = tf.keras.layers.Conv2D(filter_out, kernel_size, padding='same')\n",
        "        \n",
        "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
        "        self.conv2 = tf.keras.layers.Conv2D(filter_out, kernel_size, padding='same')\n",
        "        \n",
        "        if filter_in == filter_out:\n",
        "            self.identity = lambda x: x\n",
        "        else:\n",
        "            self.identity = tf.keras.layers.Conv2D(filter_out, (1,1), padding='same')\n",
        "\n",
        "    def call(self, x, training=False, mask=None):\n",
        "        h = self.bn1(x, training=training)\n",
        "        h = tf.nn.relu(h)\n",
        "        h = self.conv1(h)\n",
        "        \n",
        "        h = self.bn2(h, training=training)\n",
        "        h = tf.nn.relu(h)\n",
        "        h = self.conv2(h)\n",
        "        return self.identity(x) + h\n",
        "    \n",
        "class ResnetLayer(tf.keras.Model):\n",
        "    def __init__(self, filter_in, filters, kernel_size):\n",
        "        super(ResnetLayer, self).__init__()\n",
        "        self.sequence = list()\n",
        "        for f_in, f_out in zip([filter_in] + list(filters), filters):\n",
        "            self.sequence.append(ResidualUnit(f_in, f_out, kernel_size))\n",
        "\n",
        "    def call(self, x, training=False, mask=None):\n",
        "        for unit in self.sequence:\n",
        "            x = unit(x, training=training)\n",
        "        return x\n",
        "    \n",
        "class ResNet(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.conv1 = tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu') # 28x28x8\n",
        "        \n",
        "        self.res1 = ResnetLayer(64, (16, 16), (3, 3)) # 28x28x16\n",
        "        self.pool1 = tf.keras.layers.MaxPool2D((2, 2)) # 14x14x16\n",
        "        \n",
        "        \n",
        "        self.res2 = ResnetLayer(128, (32, 32), (3, 3)) # 14x14x32\n",
        "        self.pool2 = tf.keras.layers.MaxPool2D((2, 2)) # 7x7x32\n",
        "    \n",
        "        \n",
        "        self.res3 = ResnetLayer(256, (64, 64), (3, 3)) # 7x7x64\n",
        "        self.pool = tf.keras.layers.MaxPool2D((2, 2)) # 7x7x32\n",
        "        \n",
        "        \n",
        "        self.res4 = ResnetLayer(512, (64, 64), (3, 3)) # 7x7x64\n",
        "        self.pool = tf.keras.layers.MaxPool2D((2, 2)) # 7x7x32\n",
        "        \n",
        "        \n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "        self.dense1 = tf.keras.layers.Dense(1024, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(5, activation='softmax')\n",
        "        \n",
        "    def call(self, x, training=False, mask=None):\n",
        "        x = self.conv1(x)\n",
        "        \n",
        "        x = self.res1(x, training=training)\n",
        "        x = self.pool1(x)\n",
        "        x = self.res2(x, training=training)\n",
        "        x = self.pool2(x)\n",
        "        x = self.res3(x, training=training)\n",
        "        \n",
        "        x = self.flatten(x)\n",
        "        x = self.dense1(x)\n",
        "        return self.dense2(x)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVIEcU_U9C-h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfb4b011-d379-4a0b-92bd-479680fc3118"
      },
      "source": [
        "def recall(y_target, y_pred):\n",
        "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
        "    # round : 반올림한다\n",
        "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "\n",
        "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
        "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
        "\n",
        "    # (True Positive + False Negative) = 실제 값이 1(Positive) 전체\n",
        "    count_true_positive_false_negative = K.sum(y_target_yn)\n",
        "\n",
        "    # Recall =  (True Positive) / (True Positive + False Negative)\n",
        "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
        "    recall = count_true_positive / (count_true_positive_false_negative + K.epsilon())\n",
        "\n",
        "    # return a single tensor value\n",
        "    return recall\n",
        "\n",
        "\n",
        "def precision(y_target, y_pred):\n",
        "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
        "    # round : 반올림한다\n",
        "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "\n",
        "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
        "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
        "\n",
        "    # (True Positive + False Positive) = 예측 값이 1(Positive) 전체\n",
        "    count_true_positive_false_positive = K.sum(y_pred_yn)\n",
        "\n",
        "    # Precision = (True Positive) / (True Positive + False Positive)\n",
        "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
        "    precision = count_true_positive / (count_true_positive_false_positive + K.epsilon())\n",
        "\n",
        "    # return a single tensor value\n",
        "    return precision\n",
        "\n",
        "\n",
        "def f1score(y_target, y_pred):\n",
        "    _recall = recall(y_target, y_pred)\n",
        "    _precision = precision(y_target, y_pred)\n",
        "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
        "    _f1score = ( 2 * _recall * _precision) / (_recall + _precision+ K.epsilon())\n",
        "    \n",
        "    # return a single tensor value\n",
        "    return _f1score\n",
        "\n",
        "\n",
        "\n",
        "#model = create_model()\n",
        "#model = ResNet()\n",
        "#model = mobile_net()\n",
        "#model = xception()\n",
        "#model = densenet()\n",
        "#model = resnet()\n",
        "#model = inception()\n",
        "model =inceptionresnet()\n",
        "#model = vgg()\n",
        "\n",
        "lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=learning_rate,\n",
        "                                                          decay_steps=training_epochs * 10,\n",
        "                                                          decay_rate=0.5,\n",
        "                                                          staircase=True)\n",
        "\n",
        "\n",
        "\n",
        "# lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "#     initial_learning_rate=learning_rate,\n",
        "#     decay_steps=100000,\n",
        "#     decay_rate=0.96)\n",
        "\n",
        "# optimizer는 Adam, loss는 sparse categorical crossentropy 사용\n",
        "# label이 ont-hot으로 encoding 안 된 경우에 sparse categorical corssentropy 및 sparse categorical accuracy 사용\n",
        "model.compile(keras.optimizers.Adam(lr_schedule), loss = 'binary_crossentropy', metrics=['accuracy', precision, recall, f1score])\n",
        "\n",
        "# model.compile(\n",
        "#     optimizer='adam',\n",
        "#     loss='binary_crossentropy', \n",
        "#     metrics=['accuracy', precision, recall, f1score],\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Train!\n",
        "history = model.fit(x_train, y_train, steps_per_epoch=training_epochs,\n",
        "         epochs=120, validation_data = (x_val,y_val),validation_steps=validation_steps)\n",
        "model.save('animal_model_woman.h5')\n",
        "# epochs = 30\n",
        "# history = model.fit(\n",
        "#     training_set, \n",
        "#     epochs=epochs,\n",
        "#     steps_per_epoch=training_set.samples / epochs, \n",
        "#     validation_data=val_set,\n",
        "#     validation_steps=val_set.samples / epochs,\n",
        "# )"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/120\n",
            "20/20 [==============================] - 7s 338ms/step - loss: 0.4820 - accuracy: 0.2480 - precision: 0.3284 - recall: 0.2115 - f1score: 0.1771 - val_loss: 1228.0635 - val_accuracy: 0.1412 - val_precision: 0.1230 - val_recall: 0.3689 - val_f1score: 0.1844\n",
            "Epoch 2/120\n",
            "20/20 [==============================] - 4s 194ms/step - loss: 0.2632 - accuracy: 0.5508 - precision: 0.7346 - recall: 0.3188 - f1score: 0.4250 - val_loss: 13.4483 - val_accuracy: 0.1069 - val_precision: 0.1037 - val_recall: 0.1037 - val_f1score: 0.1037\n",
            "Epoch 3/120\n",
            "20/20 [==============================] - 4s 196ms/step - loss: 0.1413 - accuracy: 0.8145 - precision: 0.8664 - recall: 0.7600 - f1score: 0.8071 - val_loss: 2.2297 - val_accuracy: 0.1145 - val_precision: 0.1802 - val_recall: 0.2119 - val_f1score: 0.1946\n",
            "Epoch 4/120\n",
            "20/20 [==============================] - 4s 196ms/step - loss: 0.0878 - accuracy: 0.8965 - precision: 0.8903 - recall: 0.8590 - f1score: 0.8735 - val_loss: 0.5093 - val_accuracy: 0.2519 - val_precision: 0.3589 - val_recall: 0.1422 - val_f1score: 0.2013\n",
            "Epoch 5/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 0.0601 - accuracy: 0.9297 - precision: 0.9369 - recall: 0.8921 - f1score: 0.9128 - val_loss: 0.3736 - val_accuracy: 0.3931 - val_precision: 0.4798 - val_recall: 0.3185 - val_f1score: 0.3786\n",
            "Epoch 6/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 0.0768 - accuracy: 0.9004 - precision: 0.9080 - recall: 0.8677 - f1score: 0.8867 - val_loss: 0.3626 - val_accuracy: 0.4313 - val_precision: 0.4617 - val_recall: 0.4007 - val_f1score: 0.4281\n",
            "Epoch 7/120\n",
            "20/20 [==============================] - 4s 200ms/step - loss: 0.0492 - accuracy: 0.9414 - precision: 0.9401 - recall: 0.9261 - f1score: 0.9327 - val_loss: 0.4545 - val_accuracy: 0.1641 - val_precision: 0.2115 - val_recall: 0.1170 - val_f1score: 0.1489\n",
            "Epoch 8/120\n",
            "20/20 [==============================] - 4s 200ms/step - loss: 0.0538 - accuracy: 0.9258 - precision: 0.9380 - recall: 0.9263 - f1score: 0.9315 - val_loss: 0.3942 - val_accuracy: 0.3855 - val_precision: 0.5849 - val_recall: 0.2244 - val_f1score: 0.3207\n",
            "Epoch 9/120\n",
            "20/20 [==============================] - 4s 201ms/step - loss: 0.0484 - accuracy: 0.9395 - precision: 0.9361 - recall: 0.9338 - f1score: 0.9346 - val_loss: 0.6039 - val_accuracy: 0.2443 - val_precision: 0.2708 - val_recall: 0.2467 - val_f1score: 0.2573\n",
            "Epoch 10/120\n",
            "20/20 [==============================] - 4s 201ms/step - loss: 0.0303 - accuracy: 0.9727 - precision: 0.9685 - recall: 0.9596 - f1score: 0.9636 - val_loss: 0.5788 - val_accuracy: 0.3626 - val_precision: 0.3524 - val_recall: 0.3430 - val_f1score: 0.3469\n",
            "Epoch 11/120\n",
            "20/20 [==============================] - 4s 201ms/step - loss: 0.0182 - accuracy: 0.9824 - precision: 0.9759 - recall: 0.9827 - f1score: 0.9791 - val_loss: 0.3896 - val_accuracy: 0.4809 - val_precision: 0.4909 - val_recall: 0.4393 - val_f1score: 0.4623\n",
            "Epoch 12/120\n",
            "20/20 [==============================] - 4s 200ms/step - loss: 0.0088 - accuracy: 0.9980 - precision: 0.9882 - recall: 0.9981 - f1score: 0.9930 - val_loss: 0.4199 - val_accuracy: 0.4733 - val_precision: 0.4639 - val_recall: 0.4622 - val_f1score: 0.4620\n",
            "Epoch 13/120\n",
            "20/20 [==============================] - 4s 199ms/step - loss: 0.0047 - accuracy: 0.9980 - precision: 0.9962 - recall: 0.9981 - f1score: 0.9971 - val_loss: 0.3055 - val_accuracy: 0.5954 - val_precision: 0.6015 - val_recall: 0.5422 - val_f1score: 0.5694\n",
            "Epoch 14/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 0.0044 - accuracy: 0.9961 - precision: 0.9963 - recall: 0.9981 - f1score: 0.9971 - val_loss: 0.2145 - val_accuracy: 0.7023 - val_precision: 0.7272 - val_recall: 0.6459 - val_f1score: 0.6817\n",
            "Epoch 15/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 0.0030 - accuracy: 1.0000 - precision: 1.0000 - recall: 0.9981 - f1score: 0.9990 - val_loss: 0.1836 - val_accuracy: 0.7176 - val_precision: 0.7861 - val_recall: 0.6852 - val_f1score: 0.7303\n",
            "Epoch 16/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 0.0033 - accuracy: 0.9980 - precision: 0.9981 - recall: 1.0000 - f1score: 0.9991 - val_loss: 0.1686 - val_accuracy: 0.7672 - val_precision: 0.8307 - val_recall: 0.7252 - val_f1score: 0.7716\n",
            "Epoch 17/120\n",
            "20/20 [==============================] - 4s 197ms/step - loss: 0.0022 - accuracy: 1.0000 - precision: 1.0000 - recall: 0.9981 - f1score: 0.9990 - val_loss: 0.1594 - val_accuracy: 0.7634 - val_precision: 0.8212 - val_recall: 0.7215 - val_f1score: 0.7661\n",
            "Epoch 18/120\n",
            "20/20 [==============================] - 4s 197ms/step - loss: 0.0024 - accuracy: 1.0000 - precision: 0.9981 - recall: 1.0000 - f1score: 0.9991 - val_loss: 0.1447 - val_accuracy: 0.7901 - val_precision: 0.8231 - val_recall: 0.7407 - val_f1score: 0.7784\n",
            "Epoch 19/120\n",
            "20/20 [==============================] - 4s 199ms/step - loss: 0.0018 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1363 - val_accuracy: 0.7977 - val_precision: 0.8191 - val_recall: 0.7407 - val_f1score: 0.7770\n",
            "Epoch 20/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 0.0016 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.3802 - val_accuracy: 0.2748 - val_precision: 0.3885 - val_recall: 0.1630 - val_f1score: 0.2275\n",
            "Epoch 21/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 0.0012 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1749 - val_accuracy: 0.7405 - val_precision: 0.8607 - val_recall: 0.5978 - val_f1score: 0.6947\n",
            "Epoch 22/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 0.0012 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1384 - val_accuracy: 0.7939 - val_precision: 0.8467 - val_recall: 0.7222 - val_f1score: 0.7776\n",
            "Epoch 23/120\n",
            "20/20 [==============================] - 4s 199ms/step - loss: 0.0013 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1379 - val_accuracy: 0.7824 - val_precision: 0.8325 - val_recall: 0.7437 - val_f1score: 0.7846\n",
            "Epoch 24/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 0.0015 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1409 - val_accuracy: 0.7863 - val_precision: 0.8256 - val_recall: 0.7437 - val_f1score: 0.7815\n",
            "Epoch 25/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 0.0014 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1445 - val_accuracy: 0.7901 - val_precision: 0.8166 - val_recall: 0.7259 - val_f1score: 0.7672\n",
            "Epoch 26/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 0.0011 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1462 - val_accuracy: 0.7863 - val_precision: 0.8122 - val_recall: 0.7222 - val_f1score: 0.7631\n",
            "Epoch 27/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 0.0015 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1462 - val_accuracy: 0.7863 - val_precision: 0.8248 - val_recall: 0.7259 - val_f1score: 0.7708\n",
            "Epoch 28/120\n",
            "20/20 [==============================] - 4s 199ms/step - loss: 0.0015 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1461 - val_accuracy: 0.7863 - val_precision: 0.8207 - val_recall: 0.7259 - val_f1score: 0.7697\n",
            "Epoch 29/120\n",
            "20/20 [==============================] - 4s 199ms/step - loss: 0.0012 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1475 - val_accuracy: 0.7939 - val_precision: 0.8190 - val_recall: 0.7222 - val_f1score: 0.7669\n",
            "Epoch 30/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 0.0010 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1490 - val_accuracy: 0.7939 - val_precision: 0.8145 - val_recall: 0.7185 - val_f1score: 0.7627\n",
            "Epoch 31/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 0.0014 - accuracy: 1.0000 - precision: 1.0000 - recall: 0.9972 - f1score: 0.9986 - val_loss: 0.1509 - val_accuracy: 0.7939 - val_precision: 0.8198 - val_recall: 0.7222 - val_f1score: 0.7671\n",
            "Epoch 32/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 0.0011 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1574 - val_accuracy: 0.7710 - val_precision: 0.8152 - val_recall: 0.7148 - val_f1score: 0.7605\n",
            "Epoch 33/120\n",
            "20/20 [==============================] - 4s 199ms/step - loss: 8.7458e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1564 - val_accuracy: 0.7748 - val_precision: 0.8075 - val_recall: 0.7148 - val_f1score: 0.7571\n",
            "Epoch 34/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 9.2636e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1546 - val_accuracy: 0.7863 - val_precision: 0.8118 - val_recall: 0.7222 - val_f1score: 0.7633\n",
            "Epoch 35/120\n",
            "20/20 [==============================] - 4s 197ms/step - loss: 0.0012 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1526 - val_accuracy: 0.7901 - val_precision: 0.8003 - val_recall: 0.7185 - val_f1score: 0.7567\n",
            "Epoch 36/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 7.9488e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1504 - val_accuracy: 0.7901 - val_precision: 0.7970 - val_recall: 0.7119 - val_f1score: 0.7511\n",
            "Epoch 37/120\n",
            "20/20 [==============================] - 4s 197ms/step - loss: 0.0015 - accuracy: 1.0000 - precision: 1.0000 - recall: 0.9972 - f1score: 0.9986 - val_loss: 0.1531 - val_accuracy: 0.7824 - val_precision: 0.8086 - val_recall: 0.7111 - val_f1score: 0.7560\n",
            "Epoch 38/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 8.7303e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1562 - val_accuracy: 0.7901 - val_precision: 0.8273 - val_recall: 0.7111 - val_f1score: 0.7639\n",
            "Epoch 39/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 8.9684e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1561 - val_accuracy: 0.7939 - val_precision: 0.8182 - val_recall: 0.7215 - val_f1score: 0.7656\n",
            "Epoch 40/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 0.0012 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1570 - val_accuracy: 0.7863 - val_precision: 0.8140 - val_recall: 0.7252 - val_f1score: 0.7660\n",
            "Epoch 41/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 7.9756e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1565 - val_accuracy: 0.7901 - val_precision: 0.8201 - val_recall: 0.7289 - val_f1score: 0.7708\n",
            "Epoch 42/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 8.7401e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1568 - val_accuracy: 0.7863 - val_precision: 0.8193 - val_recall: 0.7252 - val_f1score: 0.7685\n",
            "Epoch 43/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 9.4634e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1571 - val_accuracy: 0.7863 - val_precision: 0.8163 - val_recall: 0.7252 - val_f1score: 0.7672\n",
            "Epoch 44/120\n",
            "20/20 [==============================] - 4s 199ms/step - loss: 8.9906e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1570 - val_accuracy: 0.7863 - val_precision: 0.8152 - val_recall: 0.7252 - val_f1score: 0.7666\n",
            "Epoch 45/120\n",
            "20/20 [==============================] - 4s 199ms/step - loss: 7.1087e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1568 - val_accuracy: 0.7863 - val_precision: 0.8152 - val_recall: 0.7252 - val_f1score: 0.7666\n",
            "Epoch 46/120\n",
            "20/20 [==============================] - 4s 200ms/step - loss: 9.5244e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1567 - val_accuracy: 0.7901 - val_precision: 0.8223 - val_recall: 0.7289 - val_f1score: 0.7718\n",
            "Epoch 47/120\n",
            "20/20 [==============================] - 4s 200ms/step - loss: 0.0015 - accuracy: 0.9980 - precision: 1.0000 - recall: 0.9981 - f1score: 0.9990 - val_loss: 0.1562 - val_accuracy: 0.7786 - val_precision: 0.8104 - val_recall: 0.7230 - val_f1score: 0.7629\n",
            "Epoch 48/120\n",
            "20/20 [==============================] - 4s 199ms/step - loss: 9.6536e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1566 - val_accuracy: 0.7824 - val_precision: 0.8119 - val_recall: 0.7156 - val_f1score: 0.7597\n",
            "Epoch 49/120\n",
            "20/20 [==============================] - 4s 199ms/step - loss: 9.0146e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1567 - val_accuracy: 0.7863 - val_precision: 0.8172 - val_recall: 0.7259 - val_f1score: 0.7678\n",
            "Epoch 50/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 7.3799e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1575 - val_accuracy: 0.7786 - val_precision: 0.8126 - val_recall: 0.7326 - val_f1score: 0.7694\n",
            "Epoch 51/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 9.3048e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1574 - val_accuracy: 0.7786 - val_precision: 0.8099 - val_recall: 0.7259 - val_f1score: 0.7644\n",
            "Epoch 52/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 9.2965e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1571 - val_accuracy: 0.7824 - val_precision: 0.8095 - val_recall: 0.7326 - val_f1score: 0.7679\n",
            "Epoch 53/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 8.4568e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1569 - val_accuracy: 0.7863 - val_precision: 0.8155 - val_recall: 0.7326 - val_f1score: 0.7708\n",
            "Epoch 54/120\n",
            "20/20 [==============================] - 4s 197ms/step - loss: 7.7345e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1571 - val_accuracy: 0.7863 - val_precision: 0.8203 - val_recall: 0.7326 - val_f1score: 0.7729\n",
            "Epoch 55/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 8.2707e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1577 - val_accuracy: 0.7824 - val_precision: 0.8217 - val_recall: 0.7363 - val_f1score: 0.7757\n",
            "Epoch 56/120\n",
            "20/20 [==============================] - 4s 197ms/step - loss: 7.7068e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1572 - val_accuracy: 0.7863 - val_precision: 0.8170 - val_recall: 0.7326 - val_f1score: 0.7715\n",
            "Epoch 57/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 7.9353e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1573 - val_accuracy: 0.7863 - val_precision: 0.8259 - val_recall: 0.7326 - val_f1score: 0.7753\n",
            "Epoch 58/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 8.0454e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1567 - val_accuracy: 0.7901 - val_precision: 0.8102 - val_recall: 0.7363 - val_f1score: 0.7703\n",
            "Epoch 59/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 8.4029e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1572 - val_accuracy: 0.7901 - val_precision: 0.8146 - val_recall: 0.7363 - val_f1score: 0.7724\n",
            "Epoch 60/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 6.9505e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1575 - val_accuracy: 0.7901 - val_precision: 0.8172 - val_recall: 0.7363 - val_f1score: 0.7734\n",
            "Epoch 61/120\n",
            "20/20 [==============================] - 4s 199ms/step - loss: 6.8638e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1583 - val_accuracy: 0.7901 - val_precision: 0.8242 - val_recall: 0.7363 - val_f1score: 0.7765\n",
            "Epoch 62/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 6.2867e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1579 - val_accuracy: 0.7901 - val_precision: 0.8205 - val_recall: 0.7363 - val_f1score: 0.7749\n",
            "Epoch 63/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 6.3363e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1571 - val_accuracy: 0.7901 - val_precision: 0.8242 - val_recall: 0.7363 - val_f1score: 0.7765\n",
            "Epoch 64/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 0.0017 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1573 - val_accuracy: 0.7863 - val_precision: 0.8302 - val_recall: 0.7363 - val_f1score: 0.7794\n",
            "Epoch 65/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 7.0947e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1571 - val_accuracy: 0.7863 - val_precision: 0.8310 - val_recall: 0.7400 - val_f1score: 0.7819\n",
            "Epoch 66/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 8.5827e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1574 - val_accuracy: 0.7863 - val_precision: 0.8251 - val_recall: 0.7400 - val_f1score: 0.7790\n",
            "Epoch 67/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 0.0010 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1571 - val_accuracy: 0.7863 - val_precision: 0.8251 - val_recall: 0.7400 - val_f1score: 0.7793\n",
            "Epoch 68/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 6.4169e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1573 - val_accuracy: 0.7863 - val_precision: 0.8249 - val_recall: 0.7363 - val_f1score: 0.7771\n",
            "Epoch 69/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 0.0012 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1582 - val_accuracy: 0.7863 - val_precision: 0.8218 - val_recall: 0.7400 - val_f1score: 0.7778\n",
            "Epoch 70/120\n",
            "20/20 [==============================] - 4s 197ms/step - loss: 7.3958e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1573 - val_accuracy: 0.7901 - val_precision: 0.8276 - val_recall: 0.7437 - val_f1score: 0.7826\n",
            "Epoch 71/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 7.1257e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1571 - val_accuracy: 0.7901 - val_precision: 0.8276 - val_recall: 0.7437 - val_f1score: 0.7826\n",
            "Epoch 72/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 6.1104e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1565 - val_accuracy: 0.7939 - val_precision: 0.8305 - val_recall: 0.7437 - val_f1score: 0.7839\n",
            "Epoch 73/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 6.7521e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1570 - val_accuracy: 0.7901 - val_precision: 0.8268 - val_recall: 0.7400 - val_f1score: 0.7802\n",
            "Epoch 74/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 8.0256e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1571 - val_accuracy: 0.7901 - val_precision: 0.8282 - val_recall: 0.7437 - val_f1score: 0.7830\n",
            "Epoch 75/120\n",
            "20/20 [==============================] - 4s 199ms/step - loss: 0.0011 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1572 - val_accuracy: 0.7901 - val_precision: 0.8268 - val_recall: 0.7400 - val_f1score: 0.7802\n",
            "Epoch 76/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 6.8601e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1570 - val_accuracy: 0.7901 - val_precision: 0.8231 - val_recall: 0.7326 - val_f1score: 0.7746\n",
            "Epoch 77/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 6.6014e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1572 - val_accuracy: 0.7901 - val_precision: 0.8289 - val_recall: 0.7363 - val_f1score: 0.7791\n",
            "Epoch 78/120\n",
            "20/20 [==============================] - 4s 197ms/step - loss: 7.7667e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1580 - val_accuracy: 0.7901 - val_precision: 0.8213 - val_recall: 0.7400 - val_f1score: 0.7779\n",
            "Epoch 79/120\n",
            "20/20 [==============================] - 4s 197ms/step - loss: 0.0012 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1574 - val_accuracy: 0.7901 - val_precision: 0.8270 - val_recall: 0.7400 - val_f1score: 0.7804\n",
            "Epoch 80/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 7.6133e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1570 - val_accuracy: 0.7939 - val_precision: 0.8303 - val_recall: 0.7400 - val_f1score: 0.7818\n",
            "Epoch 81/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 0.0012 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1574 - val_accuracy: 0.7901 - val_precision: 0.8319 - val_recall: 0.7474 - val_f1score: 0.7866\n",
            "Epoch 82/120\n",
            "20/20 [==============================] - 4s 199ms/step - loss: 6.3737e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1570 - val_accuracy: 0.7939 - val_precision: 0.8335 - val_recall: 0.7400 - val_f1score: 0.7832\n",
            "Epoch 83/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 7.6728e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1567 - val_accuracy: 0.7977 - val_precision: 0.8280 - val_recall: 0.7437 - val_f1score: 0.7828\n",
            "Epoch 84/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 9.4331e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1567 - val_accuracy: 0.7977 - val_precision: 0.8291 - val_recall: 0.7474 - val_f1score: 0.7853\n",
            "Epoch 85/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 0.0011 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1569 - val_accuracy: 0.7977 - val_precision: 0.8313 - val_recall: 0.7437 - val_f1score: 0.7843\n",
            "Epoch 86/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 0.0010 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1562 - val_accuracy: 0.7977 - val_precision: 0.8225 - val_recall: 0.7400 - val_f1score: 0.7783\n",
            "Epoch 87/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 5.6846e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1565 - val_accuracy: 0.7901 - val_precision: 0.8215 - val_recall: 0.7363 - val_f1score: 0.7758\n",
            "Epoch 88/120\n",
            "20/20 [==============================] - 4s 198ms/step - loss: 7.7916e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1572 - val_accuracy: 0.7901 - val_precision: 0.8174 - val_recall: 0.7363 - val_f1score: 0.7739\n",
            "Epoch 89/120\n",
            "20/20 [==============================] - 4s 199ms/step - loss: 7.9963e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1570 - val_accuracy: 0.7901 - val_precision: 0.8203 - val_recall: 0.7363 - val_f1score: 0.7752\n",
            "Epoch 90/120\n",
            "20/20 [==============================] - 4s 199ms/step - loss: 7.2359e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1577 - val_accuracy: 0.7901 - val_precision: 0.8205 - val_recall: 0.7363 - val_f1score: 0.7754\n",
            "Epoch 91/120\n",
            "20/20 [==============================] - 4s 199ms/step - loss: 7.3830e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1584 - val_accuracy: 0.7863 - val_precision: 0.8218 - val_recall: 0.7400 - val_f1score: 0.7781\n",
            "Epoch 92/120\n",
            "20/20 [==============================] - 4s 199ms/step - loss: 6.8550e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1578 - val_accuracy: 0.7901 - val_precision: 0.8188 - val_recall: 0.7400 - val_f1score: 0.7766\n",
            "Epoch 93/120\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 7.5344e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1572 - val_accuracy: 0.7901 - val_precision: 0.8217 - val_recall: 0.7400 - val_f1score: 0.7780\n",
            "Epoch 94/120\n",
            "20/20 [==============================] - 4s 200ms/step - loss: 8.3609e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1568 - val_accuracy: 0.7939 - val_precision: 0.8203 - val_recall: 0.7363 - val_f1score: 0.7752\n",
            "Epoch 95/120\n",
            "20/20 [==============================] - 4s 201ms/step - loss: 0.0013 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1575 - val_accuracy: 0.7901 - val_precision: 0.8217 - val_recall: 0.7400 - val_f1score: 0.7780\n",
            "Epoch 96/120\n",
            "20/20 [==============================] - 4s 201ms/step - loss: 7.9446e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1575 - val_accuracy: 0.7901 - val_precision: 0.8215 - val_recall: 0.7363 - val_f1score: 0.7758\n",
            "Epoch 97/120\n",
            "20/20 [==============================] - 4s 200ms/step - loss: 6.3947e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1574 - val_accuracy: 0.7901 - val_precision: 0.8185 - val_recall: 0.7363 - val_f1score: 0.7745\n",
            "Epoch 98/120\n",
            "20/20 [==============================] - 4s 200ms/step - loss: 7.9033e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1570 - val_accuracy: 0.7939 - val_precision: 0.8215 - val_recall: 0.7363 - val_f1score: 0.7758\n",
            "Epoch 99/120\n",
            "20/20 [==============================] - 4s 200ms/step - loss: 6.8202e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1576 - val_accuracy: 0.7901 - val_precision: 0.8218 - val_recall: 0.7400 - val_f1score: 0.7781\n",
            "Epoch 100/120\n",
            "20/20 [==============================] - 4s 201ms/step - loss: 8.2490e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1581 - val_accuracy: 0.7901 - val_precision: 0.8185 - val_recall: 0.7363 - val_f1score: 0.7745\n",
            "Epoch 101/120\n",
            "20/20 [==============================] - 4s 200ms/step - loss: 6.4748e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1578 - val_accuracy: 0.7901 - val_precision: 0.8185 - val_recall: 0.7400 - val_f1score: 0.7767\n",
            "Epoch 102/120\n",
            "20/20 [==============================] - 4s 200ms/step - loss: 7.3143e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1572 - val_accuracy: 0.7901 - val_precision: 0.8214 - val_recall: 0.7400 - val_f1score: 0.7780\n",
            "Epoch 103/120\n",
            "20/20 [==============================] - 4s 200ms/step - loss: 0.0010 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1569 - val_accuracy: 0.7939 - val_precision: 0.8212 - val_recall: 0.7363 - val_f1score: 0.7759\n",
            "Epoch 104/120\n",
            "20/20 [==============================] - 4s 200ms/step - loss: 7.6629e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1574 - val_accuracy: 0.7901 - val_precision: 0.8237 - val_recall: 0.7363 - val_f1score: 0.7768\n",
            "Epoch 105/120\n",
            "20/20 [==============================] - 4s 200ms/step - loss: 6.0851e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1571 - val_accuracy: 0.7901 - val_precision: 0.8268 - val_recall: 0.7363 - val_f1score: 0.7782\n",
            "Epoch 106/120\n",
            "20/20 [==============================] - 4s 200ms/step - loss: 6.7371e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1573 - val_accuracy: 0.7901 - val_precision: 0.8212 - val_recall: 0.7363 - val_f1score: 0.7759\n",
            "Epoch 107/120\n",
            "20/20 [==============================] - 4s 200ms/step - loss: 7.6784e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1576 - val_accuracy: 0.7901 - val_precision: 0.8183 - val_recall: 0.7363 - val_f1score: 0.7746\n",
            "Epoch 108/120\n",
            "20/20 [==============================] - 4s 199ms/step - loss: 7.8367e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1574 - val_accuracy: 0.7901 - val_precision: 0.8183 - val_recall: 0.7363 - val_f1score: 0.7746\n",
            "Epoch 109/120\n",
            "20/20 [==============================] - 4s 200ms/step - loss: 8.0323e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1580 - val_accuracy: 0.7901 - val_precision: 0.8241 - val_recall: 0.7400 - val_f1score: 0.7790\n",
            "Epoch 110/120\n",
            "20/20 [==============================] - 4s 200ms/step - loss: 7.2804e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1579 - val_accuracy: 0.7901 - val_precision: 0.8243 - val_recall: 0.7400 - val_f1score: 0.7790\n",
            "Epoch 111/120\n",
            "20/20 [==============================] - 4s 201ms/step - loss: 8.5126e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1573 - val_accuracy: 0.7901 - val_precision: 0.8240 - val_recall: 0.7400 - val_f1score: 0.7789\n",
            "Epoch 112/120\n",
            "20/20 [==============================] - 4s 200ms/step - loss: 5.8878e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1568 - val_accuracy: 0.7977 - val_precision: 0.8262 - val_recall: 0.7326 - val_f1score: 0.7757\n",
            "Epoch 113/120\n",
            "20/20 [==============================] - 4s 200ms/step - loss: 6.8248e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1566 - val_accuracy: 0.7977 - val_precision: 0.8272 - val_recall: 0.7363 - val_f1score: 0.7782\n",
            "Epoch 114/120\n",
            "20/20 [==============================] - 4s 200ms/step - loss: 6.1292e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1565 - val_accuracy: 0.7977 - val_precision: 0.8269 - val_recall: 0.7363 - val_f1score: 0.7782\n",
            "Epoch 115/120\n",
            "20/20 [==============================] - 4s 200ms/step - loss: 6.3539e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1565 - val_accuracy: 0.7977 - val_precision: 0.8277 - val_recall: 0.7400 - val_f1score: 0.7807\n",
            "Epoch 116/120\n",
            "20/20 [==============================] - 4s 200ms/step - loss: 7.5376e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1563 - val_accuracy: 0.7977 - val_precision: 0.8311 - val_recall: 0.7400 - val_f1score: 0.7821\n",
            "Epoch 117/120\n",
            "20/20 [==============================] - 4s 201ms/step - loss: 8.7062e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1565 - val_accuracy: 0.7977 - val_precision: 0.8214 - val_recall: 0.7400 - val_f1score: 0.7778\n",
            "Epoch 118/120\n",
            "20/20 [==============================] - 4s 200ms/step - loss: 6.6621e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1563 - val_accuracy: 0.7977 - val_precision: 0.8247 - val_recall: 0.7400 - val_f1score: 0.7792\n",
            "Epoch 119/120\n",
            "20/20 [==============================] - 4s 200ms/step - loss: 6.4059e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1567 - val_accuracy: 0.7977 - val_precision: 0.8244 - val_recall: 0.7400 - val_f1score: 0.7793\n",
            "Epoch 120/120\n",
            "20/20 [==============================] - 4s 200ms/step - loss: 0.0010 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1567 - val_accuracy: 0.7977 - val_precision: 0.8277 - val_recall: 0.7400 - val_f1score: 0.7807\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkq1yTnE9CYr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "760fcc9e-7c02-4745-d849-69b4722e4f45"
      },
      "source": [
        "# output = classifier.predict_generator(test_set, steps=1)\n",
        "# print(test_set.class_indices)\n",
        "# print(output)\n",
        "size = y_test[:,-1]\n",
        "print(size.size)\n",
        "\n",
        "\n",
        "# predict 10 random hand-writing data\n",
        "y_predicted = model.predict(x_test)\n",
        "for x in range(0,size.size):\n",
        "    np.set_printoptions(suppress=True)\n",
        "    print(\"index:\", x,\n",
        "          \" actual y:\", np.argmax(y_test[x]),\n",
        "          \" answer y:\", np.argmax(y_predicted[x]),\n",
        "            \" prediction:\", np.array(y_predicted[x] * 100))\n",
        "\n",
        "_loss, _acc, _precision, _recall, _f1score = model.evaluate(x_test, y_test)\n",
        "print('loss: {:.3f}, accuracy: {:.3f}, precision: {:.3f}, recall: {:.3f}, f1score: {:.3f}'.format(_loss, _acc, _precision, _recall, _f1score))\n",
        "# print('loss: ', evaluation[0])\n",
        "# print('accuracy', evaluation[1])\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 손실 그래프\n",
        "def plot_loss(history):\n",
        "   # 선 그리기\n",
        "    plt.plot(history.history['loss'], 'y', label='train loss')\n",
        "    plt.plot(history.history['val_loss'], 'r', label='val loss')\n",
        "   # 그래프 제목\n",
        "    plt.title('Model Loss')\n",
        "   # x,y축 이름 표시\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "   # 각 라인 표식 표시\n",
        "    plt.legend(['Train','Validation'],loc=0)\n",
        "\n",
        "# 정확도 그래프\n",
        "def plot_acc(history):\n",
        "  # dir(history.history)\n",
        "    plt.plot(history.history['accuracy'], 'b', label='train accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], 'g', label='val accuracy')\n",
        "    plt.title('Model accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc=0)\n",
        "\n",
        "plot_loss(history)\n",
        "plt.show()\n",
        "plot_acc(history)\n",
        "plt.show()"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "244\n",
            "index: 0  actual y: 2  answer y: 2  prediction: [ 0.34233415  0.08812946 98.23897     0.3101529   0.40478328  1.4679302\n",
            "  0.05555573  0.0340163 ]\n",
            "index: 1  actual y: 3  answer y: 7  prediction: [ 0.04243046  0.04064774  0.09060386  0.68079215  0.0890474   0.0506782\n",
            "  0.06611931 99.531555  ]\n",
            "index: 2  actual y: 1  answer y: 1  prediction: [ 0.01989464 99.953804    0.00674411  0.01090318  0.02422811  0.03860253\n",
            "  0.051491    0.08879107]\n",
            "index: 3  actual y: 1  answer y: 1  prediction: [ 0.0143606  99.98153     0.00897662  0.01261714  0.03152959  0.01056124\n",
            "  0.01135811  0.01285374]\n",
            "index: 4  actual y: 7  answer y: 7  prediction: [ 0.01988043  0.04919659  3.9127052   1.5035355   8.361067    7.5785823\n",
            "  0.5339849  15.598302  ]\n",
            "index: 5  actual y: 7  answer y: 7  prediction: [ 1.4018567   1.0287529   0.0485475   0.34383     1.3350599   0.5425039\n",
            "  0.05671584 84.014854  ]\n",
            "index: 6  actual y: 3  answer y: 3  prediction: [ 0.07984597  0.05070219  0.01502537 95.36104     2.5042899   0.08598202\n",
            "  1.0291204   0.01006036]\n",
            "index: 7  actual y: 7  answer y: 7  prediction: [ 0.00669845  0.04899704  0.01432839  0.03083178  0.05111598  0.03371307\n",
            "  0.03704851 99.93423   ]\n",
            "index: 8  actual y: 3  answer y: 3  prediction: [ 0.04410259  0.01819706  0.01176104 41.984093    0.46548247  1.6636457\n",
            " 28.460022    0.16484492]\n",
            "index: 9  actual y: 0  answer y: 0  prediction: [96.80456     0.02113803  0.3010475   2.2309792   0.57865524  0.01581606\n",
            "  0.16052328  0.02820169]\n",
            "index: 10  actual y: 6  answer y: 6  prediction: [ 0.03858664  0.0349949   0.01870599 12.533402    0.4683569   1.081627\n",
            " 87.963036    0.28126287]\n",
            "index: 11  actual y: 7  answer y: 3  prediction: [ 0.03043668  0.03008953  0.02133407 62.326633    0.9684913  15.28003\n",
            "  0.29806003  1.291897  ]\n",
            "index: 12  actual y: 4  answer y: 4  prediction: [ 0.06229064  0.01241952  0.02159492  0.03287115 99.9407      0.01189258\n",
            "  0.01105317  0.02743087]\n",
            "index: 13  actual y: 4  answer y: 4  prediction: [ 0.07316449  0.02657398  0.05601237  0.13192005 99.66265     0.02696192\n",
            "  0.30716106  0.10161187]\n",
            "index: 14  actual y: 4  answer y: 4  prediction: [ 0.23551422  0.02195693  0.94639194  0.6680977  96.94924     0.4044329\n",
            "  0.6295108   0.07259408]\n",
            "index: 15  actual y: 6  answer y: 6  prediction: [ 0.01739485  0.35787636  0.08799703  7.712907    0.1587618   0.48555303\n",
            " 95.735435    0.02327836]\n",
            "index: 16  actual y: 4  answer y: 4  prediction: [ 0.1171927   1.5189654   0.2585846  28.625547   58.15931     0.11593335\n",
            "  0.02823607  0.0610893 ]\n",
            "index: 17  actual y: 6  answer y: 6  prediction: [ 3.2618802   0.06879721  0.7900154   0.19447456  4.323173    0.37802085\n",
            " 76.090614    0.02603398]\n",
            "index: 18  actual y: 1  answer y: 1  prediction: [ 3.6526327  34.225903    0.00896007  1.9571224   0.11796992  0.13805065\n",
            "  1.5858746   2.0332189 ]\n",
            "index: 19  actual y: 1  answer y: 1  prediction: [ 0.03038481 99.96847     0.01519229  0.02989199  0.0387796   0.00948356\n",
            "  0.017724    0.00889495]\n",
            "index: 20  actual y: 3  answer y: 3  prediction: [ 0.10107528  0.7821057   0.02977111 71.172935    0.11673015 11.849371\n",
            "  0.2400301   0.0986073 ]\n",
            "index: 21  actual y: 1  answer y: 1  prediction: [ 0.13676278 98.75952     1.7464317   0.39736074  0.16855787  0.07963622\n",
            "  0.04801872  0.03177295]\n",
            "index: 22  actual y: 0  answer y: 0  prediction: [99.98189     0.01342698  0.03221538  0.00711367  0.01827318  0.00979763\n",
            "  0.0099521   0.03781947]\n",
            "index: 23  actual y: 4  answer y: 4  prediction: [ 0.11599345  0.1240005   1.4860452   0.3151011  98.788956    0.04818371\n",
            "  0.21706149  0.23919232]\n",
            "index: 24  actual y: 3  answer y: 3  prediction: [ 0.03562694  0.04515423  0.03604668 99.963974    0.00868513  0.01595329\n",
            "  0.27813736  0.0256162 ]\n",
            "index: 25  actual y: 5  answer y: 3  prediction: [ 0.1656369   0.04133118  0.02767507 95.6406      1.1668992   0.09092431\n",
            "  0.41102436  1.2061664 ]\n",
            "index: 26  actual y: 0  answer y: 0  prediction: [99.65637     0.0118392   0.22060154  0.03049969  0.09519895  0.02527642\n",
            "  0.2860277   0.15330696]\n",
            "index: 27  actual y: 5  answer y: 5  prediction: [ 0.10407244  0.08232025  0.09337292  0.05664831  0.06206349 98.571045\n",
            "  0.01401868  3.1327446 ]\n",
            "index: 28  actual y: 0  answer y: 0  prediction: [99.990776    0.00436724  0.01465734  0.01149407  0.02267868  0.00704682\n",
            "  0.01490126  0.02686316]\n",
            "index: 29  actual y: 0  answer y: 0  prediction: [99.97055     0.00534514  0.04202336  0.01225911  0.15502873  0.00269292\n",
            "  0.00972915  0.02195879]\n",
            "index: 30  actual y: 1  answer y: 1  prediction: [ 0.04118008 86.09027     0.8864737   3.7049863   0.36030105  0.4052813\n",
            "  1.5425153   0.09352566]\n",
            "index: 31  actual y: 3  answer y: 5  prediction: [ 0.01545817  0.2933773   0.0371985   0.03604051  0.04531376 99.9212\n",
            "  0.02332917  0.03099721]\n",
            "index: 32  actual y: 7  answer y: 2  prediction: [ 0.2770955   0.22614272 92.08059     0.05690248  0.03627516  0.75764483\n",
            "  0.13587749  4.3784456 ]\n",
            "index: 33  actual y: 5  answer y: 5  prediction: [ 0.02595017  0.02496187  0.01744515  0.00857762  0.01550433 99.88018\n",
            "  0.0493517   0.33225167]\n",
            "index: 34  actual y: 6  answer y: 3  prediction: [ 0.2650551   0.0247339   0.0245843  98.82031     0.06931244  0.03132322\n",
            "  2.275677    0.0384267 ]\n",
            "index: 35  actual y: 1  answer y: 1  prediction: [ 0.19347316 98.17228     0.04130101  0.13360482  0.17113326  0.42088437\n",
            "  0.05367693  1.3482729 ]\n",
            "index: 36  actual y: 0  answer y: 0  prediction: [99.814415    0.02743955  0.10581137  0.034723    0.03731864  0.07897869\n",
            "  0.03923703  0.12781091]\n",
            "index: 37  actual y: 5  answer y: 5  prediction: [ 0.3741926   0.25926757  0.36770308  0.45426258  0.8725953  98.679825\n",
            "  0.02600443  0.11880729]\n",
            "index: 38  actual y: 5  answer y: 5  prediction: [ 0.09194718  0.13558404  0.07330487  0.47370368  0.02658577 99.69235\n",
            "  0.01746582  0.05203598]\n",
            "index: 39  actual y: 5  answer y: 5  prediction: [ 0.04374706  0.04508458  0.03739696  0.09775655  0.31194937 99.4869\n",
            "  0.05749791  0.6860249 ]\n",
            "index: 40  actual y: 2  answer y: 2  prediction: [ 0.32488668  0.02398364 98.79762     0.1080221   1.2091559   0.0787655\n",
            "  0.39896417  0.05208503]\n",
            "index: 41  actual y: 2  answer y: 3  prediction: [ 0.29384407  0.05110041  0.52462125 24.162424    0.86339563 14.192714\n",
            "  7.265743    0.02304783]\n",
            "index: 42  actual y: 2  answer y: 2  prediction: [ 0.0832137   0.03648691 94.90404     2.2857244   0.24019775  7.0079236\n",
            "  0.03963415  0.27419755]\n",
            "index: 43  actual y: 5  answer y: 5  prediction: [ 1.2551508   0.01609178  0.30535418  0.2816987   7.8087277  91.91016\n",
            "  0.07831466  0.09393316]\n",
            "index: 44  actual y: 4  answer y: 4  prediction: [ 0.03437172  0.01062506  0.06142828  0.06104856 99.89047     0.01822154\n",
            "  0.0898666   0.02031812]\n",
            "index: 45  actual y: 1  answer y: 1  prediction: [ 0.0715411  99.876785    0.1034653   0.08108415  0.23222908  0.02079237\n",
            "  0.06974951  0.02722405]\n",
            "index: 46  actual y: 6  answer y: 5  prediction: [0.25507292 0.06032254 4.2044034  0.89919835 1.9193746  5.224441\n",
            " 4.123608   0.8015    ]\n",
            "index: 47  actual y: 3  answer y: 3  prediction: [ 0.0238574   0.07291714  0.07447638 99.09955     0.29241517  2.1400423\n",
            "  0.10349868  0.05399437]\n",
            "index: 48  actual y: 4  answer y: 4  prediction: [ 0.10803635  0.26723817  0.13574077  1.4998621  98.08627     0.03170771\n",
            "  0.19763477  0.01177975]\n",
            "index: 49  actual y: 0  answer y: 0  prediction: [99.970894    0.04447974  0.02917103  0.00753851  0.0229033   0.03184404\n",
            "  0.01137645  0.01697432]\n",
            "index: 50  actual y: 2  answer y: 2  prediction: [ 0.07261144  0.14846829 99.565445    0.31010646  0.08043105  0.01652836\n",
            "  0.03489     0.01376097]\n",
            "index: 51  actual y: 7  answer y: 7  prediction: [ 0.72849274  0.06386957  0.02658985  0.03595837  0.02647472  1.2052177\n",
            "  0.05786278 98.37579   ]\n",
            "index: 52  actual y: 6  answer y: 6  prediction: [ 0.48409417  0.26197943  4.0780363   0.06641991  4.499388    0.04246473\n",
            " 74.57634     0.0619649 ]\n",
            "index: 53  actual y: 3  answer y: 5  prediction: [ 0.02810607  0.1171009   0.12313359  8.835734    0.03497097 71.27804\n",
            "  0.06440821  4.9818325 ]\n",
            "index: 54  actual y: 7  answer y: 7  prediction: [ 4.139151    1.6299946   0.03285702  0.15765744  4.792388    0.18555924\n",
            "  0.2502149  70.650696  ]\n",
            "index: 55  actual y: 4  answer y: 4  prediction: [ 0.02998455  0.0127838   0.12694877  0.02056165 99.9174      0.00789763\n",
            "  0.0322191   0.09009504]\n",
            "index: 56  actual y: 4  answer y: 4  prediction: [ 0.14357099  0.01083362  0.00787387  0.18944201 99.350716    0.0626344\n",
            "  0.3979332   0.10664845]\n",
            "index: 57  actual y: 6  answer y: 6  prediction: [ 0.02173185  0.01240821  0.03267291  0.00781996  0.09117819  0.03135767\n",
            " 99.94874     0.01318721]\n",
            "index: 58  actual y: 6  answer y: 6  prediction: [ 0.02985257  0.01701973  0.03693156  0.00453686  0.03637213  0.03192709\n",
            " 99.957924    0.02007676]\n",
            "index: 59  actual y: 4  answer y: 1  prediction: [ 4.2517886  29.403862    9.0952      1.3276986   1.5067072   0.14218824\n",
            "  0.05175874  0.06090605]\n",
            "index: 60  actual y: 3  answer y: 3  prediction: [ 1.1214721   0.19490683  0.05970727 92.961464    0.194229    1.682105\n",
            "  0.12538388  0.09560858]\n",
            "index: 61  actual y: 1  answer y: 1  prediction: [ 0.00709475 99.894295    0.07967368  0.10632819  0.03152172  0.02486138\n",
            "  0.01870717  0.02015263]\n",
            "index: 62  actual y: 7  answer y: 7  prediction: [ 0.01174455  0.01282864  0.04898126  0.04696417  0.04432238  0.08105224\n",
            "  0.02451964 99.890755  ]\n",
            "index: 63  actual y: 3  answer y: 3  prediction: [ 0.05093288  0.01624998  0.04127867 99.971596    0.01478433  0.01983897\n",
            "  0.01755064  0.04439278]\n",
            "index: 64  actual y: 7  answer y: 7  prediction: [ 0.00479112  0.0120158   0.03846669  0.06660733  0.11564048  0.03737922\n",
            "  0.01665595 99.93618   ]\n",
            "index: 65  actual y: 1  answer y: 1  prediction: [ 0.04583306 99.682106    0.06189632  0.12480083  0.33332995  0.14385456\n",
            "  0.06445491  0.09910106]\n",
            "index: 66  actual y: 1  answer y: 7  prediction: [ 0.02307675 55.93396     0.03492334  0.20625551  0.02823335  0.4560223\n",
            "  0.0500147  56.963062  ]\n",
            "index: 67  actual y: 6  answer y: 6  prediction: [ 0.37846828  0.02443938  0.03147586  0.38162103  0.3618179   0.30855528\n",
            " 98.46264     0.00813829]\n",
            "index: 68  actual y: 2  answer y: 2  prediction: [11.663807    0.06353526 93.31555     0.0375459   0.04402605  0.36391288\n",
            "  0.03973892  0.01443462]\n",
            "index: 69  actual y: 6  answer y: 6  prediction: [ 0.07196922  0.03355036  0.23275907  4.0974317   0.04516326  0.15893544\n",
            " 52.692677   19.316275  ]\n",
            "index: 70  actual y: 1  answer y: 7  prediction: [ 0.03217392  1.1687043   2.3453486   4.9173136   0.03845239 10.382475\n",
            "  0.1084325  19.534277  ]\n",
            "index: 71  actual y: 5  answer y: 5  prediction: [ 0.12056356  0.28197607  0.02969044  0.24828766  0.23133746 98.68209\n",
            "  0.07983886  0.56716263]\n",
            "index: 72  actual y: 3  answer y: 7  prediction: [ 0.05485328  0.50039387  4.279942    0.53200597  0.13623694  4.89197\n",
            "  3.755586   42.12793   ]\n",
            "index: 73  actual y: 2  answer y: 2  prediction: [ 0.0229342   0.0284217  97.92827     0.9671115   0.6857595   0.12268341\n",
            "  0.04291591  2.3684583 ]\n",
            "index: 74  actual y: 4  answer y: 4  prediction: [ 0.07379396  0.00819305  0.01350962  0.01236477 99.949646    0.00495853\n",
            "  0.03838584  0.02716753]\n",
            "index: 75  actual y: 3  answer y: 3  prediction: [ 0.1052354   0.25064525  0.9775195  97.08647     0.98031497  0.05814997\n",
            "  0.09216817  0.4293874 ]\n",
            "index: 76  actual y: 4  answer y: 3  prediction: [ 0.18953119  0.02117127  0.49751735 87.72839     7.3014894   0.01639636\n",
            "  0.86808765  0.01509249]\n",
            "index: 77  actual y: 6  answer y: 6  prediction: [ 0.01147243  0.02738355  0.71422476  0.00889996  0.05024421  0.03185499\n",
            " 99.755684    0.09828977]\n",
            "index: 78  actual y: 4  answer y: 4  prediction: [ 0.08163626  0.0162097   0.02598163  0.1426076  99.74444     0.03212478\n",
            "  0.13905723  0.12316428]\n",
            "index: 79  actual y: 5  answer y: 0  prediction: [8.283294   0.12198635 0.06775477 6.7528086  0.5643867  4.9878373\n",
            " 2.4640014  0.10531522]\n",
            "index: 80  actual y: 5  answer y: 5  prediction: [ 0.1910824   0.28943592  0.0396888   0.08827536  0.18806842 98.79871\n",
            "  1.7795203   0.07703699]\n",
            "index: 81  actual y: 0  answer y: 0  prediction: [99.34163     0.11363249  1.1692066   0.0330651   0.07597799  0.00973467\n",
            "  0.03230343  0.3571227 ]\n",
            "index: 82  actual y: 5  answer y: 5  prediction: [ 0.04075624  0.01635632  0.00861742  0.01702679  0.02470931 99.95998\n",
            "  0.03443934  0.01841375]\n",
            "index: 83  actual y: 2  answer y: 2  prediction: [ 0.04402538  0.20964825 87.24455     0.4856915   8.4966135   1.2554883\n",
            "  0.29150137  0.12278207]\n",
            "index: 84  actual y: 4  answer y: 3  prediction: [ 0.11627018  0.06555855  0.33511052 81.996994   15.104097    0.01393045\n",
            "  2.1761227   0.25650933]\n",
            "index: 85  actual y: 1  answer y: 1  prediction: [ 0.04397239 99.93111     0.01296656  0.0924272   0.0775985   0.02356197\n",
            "  0.02211878  0.04813064]\n",
            "index: 86  actual y: 7  answer y: 7  prediction: [ 0.00558097  0.03568973  0.03585812  0.01119536  0.01499935  0.00992353\n",
            "  0.01749189 99.976166  ]\n",
            "index: 87  actual y: 4  answer y: 4  prediction: [ 0.17508967  0.09317195  0.5225992   0.21685967 97.52766     0.00828936\n",
            "  2.2003791   0.03513188]\n",
            "index: 88  actual y: 4  answer y: 4  prediction: [ 0.08374614  0.01807573  0.0918938   0.04227284 99.82922     0.07326525\n",
            "  0.06304576  0.07403839]\n",
            "index: 89  actual y: 2  answer y: 7  prediction: [ 0.02897365  0.06576627 49.953575    0.07674054  0.47729412  0.05616824\n",
            "  0.0757561  81.84082   ]\n",
            "index: 90  actual y: 4  answer y: 4  prediction: [ 0.23394625  0.03598135  0.03068094 41.82853    86.21991     0.05133358\n",
            "  0.04447084  0.02289413]\n",
            "index: 91  actual y: 4  answer y: 4  prediction: [ 0.2742651   0.00981887  0.05837015  0.27879342 87.9784      0.01741165\n",
            "  3.7282004   4.3206983 ]\n",
            "index: 92  actual y: 6  answer y: 6  prediction: [ 0.03203822  0.03041368  0.33709925  0.5091959   0.36137325  0.5965642\n",
            " 97.78749     0.09476583]\n",
            "index: 93  actual y: 2  answer y: 2  prediction: [ 0.02690902  2.999863   42.65636     0.3201986   0.6528617   0.01407239\n",
            "  0.05188299 23.057716  ]\n",
            "index: 94  actual y: 1  answer y: 1  prediction: [ 0.02407985 99.94652     0.02004908  0.02968195  0.09446315  0.03368947\n",
            "  0.03675764  0.08538097]\n",
            "index: 95  actual y: 3  answer y: 0  prediction: [23.61681    20.704533    0.09571786  4.7839165   0.02097871  0.0566856\n",
            "  0.2976296   1.1207838 ]\n",
            "index: 96  actual y: 3  answer y: 3  prediction: [ 0.09345952  0.02306863  0.03592214 95.63169     0.15062383  0.29916838\n",
            "  0.25934488  1.1490343 ]\n",
            "index: 97  actual y: 0  answer y: 0  prediction: [99.97887     0.01235141  0.0158367   0.01340473  0.03300117  0.03183642\n",
            "  0.02735072  0.02220561]\n",
            "index: 98  actual y: 0  answer y: 4  prediction: [11.285569    0.03549685  0.9751472   2.4271507  83.93017     0.00972261\n",
            "  0.01502655  0.05256875]\n",
            "index: 99  actual y: 4  answer y: 4  prediction: [ 0.03630896  0.02186691  0.72902745  1.6335508  38.12396     0.0417204\n",
            "  5.6218686  10.295298  ]\n",
            "index: 100  actual y: 3  answer y: 3  prediction: [ 0.02215346  0.10160707  0.06203868 99.6252      0.03682858  0.07936935\n",
            "  0.09669956  0.17543855]\n",
            "index: 101  actual y: 2  answer y: 0  prediction: [26.600815    0.02194466  9.951587    1.7451624  14.949727    0.00907126\n",
            "  1.9916363   0.02782524]\n",
            "index: 102  actual y: 2  answer y: 2  prediction: [ 0.19484764  0.24190776 97.148094    4.8851995   0.13628684  0.13223515\n",
            "  0.02321026  0.17520137]\n",
            "index: 103  actual y: 0  answer y: 0  prediction: [99.88211     0.03753638  0.07374767  0.0066261   0.02356994  0.03275543\n",
            "  0.1306687   0.03243462]\n",
            "index: 104  actual y: 2  answer y: 2  prediction: [ 0.04844179  0.04995027 99.75216     0.02337609  0.44800133  0.01278616\n",
            "  0.12150898  0.02270849]\n",
            "index: 105  actual y: 0  answer y: 0  prediction: [99.96808     0.01361665  0.04651273  0.00505139  0.03608478  0.00865551\n",
            "  0.0321359   0.01240768]\n",
            "index: 106  actual y: 5  answer y: 5  prediction: [ 0.19069387  0.12193082  0.02118342  0.02509599  0.01180134 98.24655\n",
            "  0.09193986  2.205936  ]\n",
            "index: 107  actual y: 6  answer y: 6  prediction: [ 0.74098563  0.04887599 15.743117    0.30306432  0.16903614  0.02212245\n",
            " 71.61848     0.04090538]\n",
            "index: 108  actual y: 3  answer y: 3  prediction: [ 0.09206083  0.02747888  0.02183859 99.74478     0.04097157  0.03514855\n",
            "  0.07082634  0.1053747 ]\n",
            "index: 109  actual y: 6  answer y: 6  prediction: [ 0.27061287  0.00823713  0.04269848  0.01175185  0.07522646  0.01389996\n",
            " 99.772804    0.07909446]\n",
            "index: 110  actual y: 3  answer y: 3  prediction: [19.450653    0.10261314  0.20140624 70.358696    0.07301943  1.1148744\n",
            "  0.12412629  0.04320042]\n",
            "index: 111  actual y: 1  answer y: 1  prediction: [ 0.04397239 99.93111     0.01296656  0.0924272   0.0775985   0.02356197\n",
            "  0.02211878  0.04813064]\n",
            "index: 112  actual y: 6  answer y: 6  prediction: [ 0.2978657   0.00654525  0.04733072  1.1230891  24.1656      0.07686796\n",
            " 61.71148     0.18484429]\n",
            "index: 113  actual y: 1  answer y: 4  prediction: [ 0.02738295  3.6365933   2.182235    0.11792487 58.936535    0.06929061\n",
            "  0.50674546  5.048305  ]\n",
            "index: 114  actual y: 4  answer y: 3  prediction: [ 0.45620045  0.20518221  3.0470982  54.965446   21.857132    0.00796539\n",
            "  0.02303661  0.16340424]\n",
            "index: 115  actual y: 0  answer y: 0  prediction: [99.51578     0.03094377  0.02322982  0.02934968  0.50147504  0.08629415\n",
            "  0.72716635  0.01154989]\n",
            "index: 116  actual y: 6  answer y: 6  prediction: [ 0.03973845  0.01702473  0.07261743  0.07327805  0.04303058  0.04077066\n",
            " 99.872955    0.01576122]\n",
            "index: 117  actual y: 4  answer y: 4  prediction: [ 0.02863136  0.02130835  0.07613215  0.04628519 99.87826     0.02441672\n",
            "  0.08906216  0.06228067]\n",
            "index: 118  actual y: 1  answer y: 1  prediction: [ 0.06673656 99.83143     0.07066789  0.14112775  0.06003389  0.03184817\n",
            "  0.02065142  0.0360012 ]\n",
            "index: 119  actual y: 5  answer y: 5  prediction: [ 0.118387    0.02927595  0.02031045  0.24033205  0.04547783 99.62134\n",
            "  0.0940752   0.14193909]\n",
            "index: 120  actual y: 1  answer y: 1  prediction: [ 0.05846213 99.945656    0.01700528  0.06277092  0.10206147  0.01499886\n",
            "  0.03642885  0.04528763]\n",
            "index: 121  actual y: 6  answer y: 6  prediction: [ 0.10600429  0.04007666  8.940136    0.05396951  0.47732425  0.19063452\n",
            " 91.31493     0.05697776]\n",
            "index: 122  actual y: 1  answer y: 1  prediction: [ 0.0782778  99.90471     0.01632644  0.04502604  0.10216604  0.03338474\n",
            "  0.01976096  0.07511857]\n",
            "index: 123  actual y: 2  answer y: 2  prediction: [ 0.02757083  0.04638192 55.90068     1.6521798   0.07741223  0.36565635\n",
            "  1.1040992  15.895092  ]\n",
            "index: 124  actual y: 5  answer y: 5  prediction: [ 0.07891862  0.0721624   0.0275871   2.0571187   0.63947284 70.658714\n",
            "  0.10848179 12.576464  ]\n",
            "index: 125  actual y: 0  answer y: 0  prediction: [99.82538     0.02086084  0.10941473  0.0844693   0.136158    0.01995742\n",
            "  0.1620924   0.01357805]\n",
            "index: 126  actual y: 3  answer y: 3  prediction: [ 0.08436132  0.02083174  0.03997365 99.5294      0.05491564  0.63089514\n",
            "  0.19789615  0.02205799]\n",
            "index: 127  actual y: 5  answer y: 5  prediction: [ 0.0156857   0.01753777  0.02853052  0.05305558  0.04382339 99.8817\n",
            "  0.03892063  0.11503001]\n",
            "index: 128  actual y: 5  answer y: 7  prediction: [ 0.05880147  5.813344    0.01665019  0.03692931  0.3815119  26.182117\n",
            "  0.91905195 27.058462  ]\n",
            "index: 129  actual y: 2  answer y: 2  prediction: [ 0.06131165  0.03661065 99.6826      0.01957617  0.28358608  0.6885753\n",
            "  0.03135713  0.04543473]\n",
            "index: 130  actual y: 5  answer y: 5  prediction: [ 0.09149357  0.49496317  0.11947773  1.6256843  16.44662    55.51244\n",
            "  0.13770294  1.3260653 ]\n",
            "index: 131  actual y: 7  answer y: 3  prediction: [ 0.13528308  0.09189682  0.03774665 47.443024    1.426958    2.474184\n",
            "  0.12212355  5.49799   ]\n",
            "index: 132  actual y: 7  answer y: 7  prediction: [ 0.01736021  0.42432913  1.9354459   0.7743298   2.689951    0.12010798\n",
            "  3.7506201  64.37923   ]\n",
            "index: 133  actual y: 5  answer y: 5  prediction: [ 0.00604765  0.18720682  0.03546648  0.6317555   0.19218828 86.48038\n",
            "  0.01225254 14.887616  ]\n",
            "index: 134  actual y: 0  answer y: 0  prediction: [98.32722     0.05114271  0.0749329   0.33080772  0.09727083  0.04349164\n",
            "  0.2998078   0.22522555]\n",
            "index: 135  actual y: 7  answer y: 7  prediction: [ 0.04976201  0.21352486  0.10308532  0.09146325  0.10880393  0.21154699\n",
            "  0.02606279 99.45612   ]\n",
            "index: 136  actual y: 1  answer y: 1  prediction: [ 0.03825752 99.9491      0.02495514  0.0668228   0.04205449  0.02118354\n",
            "  0.03080677  0.02898703]\n",
            "index: 137  actual y: 2  answer y: 5  prediction: [ 0.07944328  1.5352671   0.06447683  0.14332774 13.060143   54.345036\n",
            "  2.5128646   0.6357808 ]\n",
            "index: 138  actual y: 0  answer y: 0  prediction: [99.912544    0.03285379  0.08835815  0.07765955  0.1336437   0.0070444\n",
            "  0.01104122  0.00758269]\n",
            "index: 139  actual y: 0  answer y: 3  prediction: [ 7.2829857   0.01295609  0.30410558 42.622604    0.08933593  0.21391627\n",
            "  2.5114362   0.3922848 ]\n",
            "index: 140  actual y: 7  answer y: 1  prediction: [ 2.6509984  10.403003    0.02166105  2.9398394   0.12265251  0.1915581\n",
            "  1.629089    5.053085  ]\n",
            "index: 141  actual y: 4  answer y: 4  prediction: [ 0.18270402  0.02018425  0.03556349  0.16833392 96.918106    0.08582474\n",
            "  5.3172493   0.02403743]\n",
            "index: 142  actual y: 0  answer y: 5  prediction: [ 0.02562648  0.4618341   2.9978049   0.19499181  0.03575762 49.37783\n",
            "  0.05528843 32.62288   ]\n",
            "index: 143  actual y: 1  answer y: 1  prediction: [ 0.03254393 99.93977     0.04560932  0.02977946  0.06648761  0.01257723\n",
            "  0.03688234  0.05656818]\n",
            "index: 144  actual y: 2  answer y: 2  prediction: [ 0.42037508  0.06560354 99.6867      0.03480172  0.00807215  0.06712896\n",
            "  0.01071376  0.03066325]\n",
            "index: 145  actual y: 2  answer y: 2  prediction: [ 3.6282823   0.28678268 86.653824    2.92802     0.58604723  0.02973545\n",
            "  0.24034223  0.03319216]\n",
            "index: 146  actual y: 7  answer y: 7  prediction: [ 1.1545664   0.04727713  0.02151805  0.14096233  0.20043565  0.03475237\n",
            "  0.05558798 99.19041   ]\n",
            "index: 147  actual y: 3  answer y: 3  prediction: [ 0.21663855  5.1988463   0.08359776 93.15454     0.17987509  0.01116321\n",
            "  0.13121457  0.04259627]\n",
            "index: 148  actual y: 1  answer y: 4  prediction: [ 0.08063033 13.256047    1.2121632   1.8878552  27.950352    0.11113575\n",
            "  0.07894179  4.012477  ]\n",
            "index: 149  actual y: 3  answer y: 6  prediction: [ 0.0495344   0.03735833  0.17287165 11.123847    0.07176001  0.04234288\n",
            " 92.30387     0.25023475]\n",
            "index: 150  actual y: 2  answer y: 2  prediction: [ 0.18119463  0.87303585 98.28888     0.00668214  0.09816282  0.01752688\n",
            "  0.01584436  0.52291936]\n",
            "index: 151  actual y: 1  answer y: 1  prediction: [ 0.03921341 99.93618     0.03997412  0.08449458  0.05416563  0.02331619\n",
            "  0.03675154  0.02757262]\n",
            "index: 152  actual y: 7  answer y: 7  prediction: [ 0.39905608  0.12827481  0.5285356   0.38029385  2.5698292   0.06857501\n",
            "  0.18669598 97.339386  ]\n",
            "index: 153  actual y: 7  answer y: 7  prediction: [ 0.00656717  0.00665186  0.03574903  0.04028875  0.04560085  0.0104215\n",
            "  0.04209071 99.96813   ]\n",
            "index: 154  actual y: 0  answer y: 2  prediction: [11.771217    1.7327346  63.95606     0.2689314   0.17844118  0.49128878\n",
            "  0.09035978  0.2002834 ]\n",
            "index: 155  actual y: 6  answer y: 6  prediction: [ 0.5741026   0.05783797  0.1114915  14.714382    0.03007614  0.06335199\n",
            " 83.5715      0.08099569]\n",
            "index: 156  actual y: 6  answer y: 0  prediction: [86.57383     0.06024494  3.518708    0.02586056  0.05566951  0.7155922\n",
            "  0.6579903   0.26932535]\n",
            "index: 157  actual y: 4  answer y: 4  prediction: [ 0.11926556  0.0401478   0.68146926  0.06823777 99.73922     0.00615576\n",
            "  0.03788276  0.01298742]\n",
            "index: 158  actual y: 2  answer y: 4  prediction: [ 0.0860073   0.05337593  6.325041    0.386307   95.60469     0.144874\n",
            "  0.348872    0.1213844 ]\n",
            "index: 159  actual y: 0  answer y: 0  prediction: [99.40971     0.23420137  0.4023055   0.2368973   0.06318722  0.00792373\n",
            "  0.0237669   0.01672217]\n",
            "index: 160  actual y: 4  answer y: 4  prediction: [ 0.130785    2.832694    0.13264605  7.4823947  69.82484     0.01229154\n",
            "  0.02911063  0.37160003]\n",
            "index: 161  actual y: 1  answer y: 1  prediction: [ 0.02851561 99.9532      0.05879525  0.0403116   0.05628872  0.0148849\n",
            "  0.02912554  0.01621545]\n",
            "index: 162  actual y: 3  answer y: 1  prediction: [ 0.02222932 99.70657     0.02301022  0.3300246   0.05558481  0.03845016\n",
            "  0.02822921  0.04353701]\n",
            "index: 163  actual y: 5  answer y: 5  prediction: [ 0.0057483   0.02916708  0.2501228   0.02239767  0.02481371 99.96192\n",
            "  0.03047646  0.03638796]\n",
            "index: 164  actual y: 5  answer y: 5  prediction: [ 0.02424993  0.01148854  0.00981244  0.09360406  0.01198567 99.875595\n",
            "  0.3111977   0.03585551]\n",
            "index: 165  actual y: 3  answer y: 3  prediction: [ 1.0850894   0.04137684  0.07796513 98.777534    0.03305451  0.11463206\n",
            "  1.1317627   0.03111864]\n",
            "index: 166  actual y: 4  answer y: 4  prediction: [ 0.01129999  0.00963424  0.02601584  0.03431609 99.916565    0.01432162\n",
            "  0.03350775  0.03475636]\n",
            "index: 167  actual y: 3  answer y: 3  prediction: [ 0.4521567   0.02210127  0.09501681 96.61714     0.7082059   0.05753157\n",
            "  1.0503175   0.35897443]\n",
            "index: 168  actual y: 7  answer y: 7  prediction: [ 0.25762242  0.25451156  0.05011349  0.03613128  0.07325798  6.9092145\n",
            "  0.01965806 91.05124   ]\n",
            "index: 169  actual y: 5  answer y: 5  prediction: [ 1.5190254   0.15413013  6.9649568   2.1449537   0.08584689 49.17672\n",
            "  0.10334187  1.109768  ]\n",
            "index: 170  actual y: 7  answer y: 7  prediction: [ 1.5395591   0.07323345  0.07840508  0.03563211  0.4782843   0.35808092\n",
            "  0.3502518  97.10778   ]\n",
            "index: 171  actual y: 7  answer y: 7  prediction: [ 0.05197966  0.10713955  0.05991295  0.24995296  0.81569386  0.03965869\n",
            "  0.04861572 99.366585  ]\n",
            "index: 172  actual y: 7  answer y: 7  prediction: [ 0.74939686  0.02201871  0.03392817  1.3991767  10.499538    2.2750652\n",
            "  0.68005365 15.691357  ]\n",
            "index: 173  actual y: 0  answer y: 0  prediction: [94.706985    0.11972722  0.6206052   0.40222698  3.06024     0.26232755\n",
            "  0.1814278   0.02438084]\n",
            "index: 174  actual y: 0  answer y: 0  prediction: [94.28075     0.01831564  9.6935835   0.0217985   0.41997707  0.3002404\n",
            "  0.03121239  0.13706288]\n",
            "index: 175  actual y: 3  answer y: 3  prediction: [ 0.09679109  0.00959434  0.04669349 99.415695    0.0197144   0.0409862\n",
            "  0.05241188  1.0277483 ]\n",
            "index: 176  actual y: 7  answer y: 1  prediction: [ 0.09559136 99.80875     0.06494904  0.02366258  0.06671444  0.03323593\n",
            "  0.10292404  0.3430308 ]\n",
            "index: 177  actual y: 0  answer y: 0  prediction: [68.484406    0.38709736  0.41585103  0.02189223  0.15083948  0.22432755\n",
            "  0.14181073 32.154606  ]\n",
            "index: 178  actual y: 0  answer y: 6  prediction: [ 3.923512    0.22478431  0.09149074  4.6812043   4.663342    0.06008486\n",
            " 25.422066    0.02755816]\n",
            "index: 179  actual y: 7  answer y: 6  prediction: [ 1.9656336   0.4978926   0.10085938  0.06226731 11.889094    0.03833797\n",
            " 26.482868    5.171277  ]\n",
            "index: 180  actual y: 4  answer y: 4  prediction: [ 0.05252301  0.09344835  0.07457859 47.028397   56.320942    0.04164266\n",
            "  0.53741944  0.18118677]\n",
            "index: 181  actual y: 4  answer y: 4  prediction: [ 0.06229064  0.01241952  0.02159492  0.03287115 99.9407      0.01189258\n",
            "  0.01105317  0.02743087]\n",
            "index: 182  actual y: 7  answer y: 7  prediction: [ 0.00429131  0.00396826  0.02500394  0.01841898  0.01720743  0.01263\n",
            "  0.01838379 99.98277   ]\n",
            "index: 183  actual y: 0  answer y: 0  prediction: [99.89094     0.02023854  0.18955374  0.04156875  0.0224416   0.01944153\n",
            "  0.01449001  0.00877671]\n",
            "index: 184  actual y: 5  answer y: 5  prediction: [ 1.4668527   0.27647245  0.01087755  0.03732812  0.0174768  97.97846\n",
            "  0.01069838  0.78649956]\n",
            "index: 185  actual y: 2  answer y: 2  prediction: [ 0.025175    0.01586203 73.17034     0.00832948  0.17746337  0.11463382\n",
            " 42.003014    0.68339866]\n",
            "index: 186  actual y: 7  answer y: 7  prediction: [ 0.00722406  0.07604007  0.20496354  0.02249993  1.3777432   0.09059177\n",
            "  0.05338648 99.55887   ]\n",
            "index: 187  actual y: 7  answer y: 7  prediction: [ 0.01856839  0.14241786  0.10353043  0.04063782  0.07678512  0.11687158\n",
            "  0.01676978 99.82623   ]\n",
            "index: 188  actual y: 6  answer y: 6  prediction: [ 0.01315727  0.00295617  0.01002547  0.03297372  0.01567595  0.03189958\n",
            " 99.97124     0.00286225]\n",
            "index: 189  actual y: 1  answer y: 1  prediction: [ 0.07924255 99.829094    0.0815077   0.11972288  0.16910045  0.02871598\n",
            "  0.04824966  0.05527731]\n",
            "index: 190  actual y: 4  answer y: 3  prediction: [ 0.12146895  0.0488374   1.435419   47.022884   38.38234     1.3662455\n",
            "  0.25637993  0.0560846 ]\n",
            "index: 191  actual y: 3  answer y: 7  prediction: [ 0.02164179  0.43387142  0.05127587 14.666757    0.6459144   4.8570147\n",
            "  0.06196024 50.37328   ]\n",
            "index: 192  actual y: 4  answer y: 4  prediction: [ 0.1121747   0.2980619   0.50903773  1.1137006  83.74101     0.02547751\n",
            "  6.5188804   0.08468521]\n",
            "index: 193  actual y: 2  answer y: 2  prediction: [ 0.14425424  0.09386145 98.15814     1.3216472   0.22342144  0.44215113\n",
            "  0.12039979  0.2817012 ]\n",
            "index: 194  actual y: 0  answer y: 0  prediction: [99.01969     0.02788579  1.2420895   0.06783226  0.22979437  0.11840154\n",
            "  0.03903883  0.05848283]\n",
            "index: 195  actual y: 4  answer y: 4  prediction: [ 0.0448833   0.1993516   1.1510444   0.05554882 80.70425     0.08537486\n",
            "  5.446983    7.693117  ]\n",
            "index: 196  actual y: 4  answer y: 0  prediction: [12.696153    0.37738177  0.26564354  0.42502558  1.0643634   0.39647835\n",
            "  0.19233319 12.559698  ]\n",
            "index: 197  actual y: 5  answer y: 5  prediction: [ 0.02726125  0.06794717  1.4521903   0.2857428   0.43386525 96.880516\n",
            "  0.0299855   1.294991  ]\n",
            "index: 198  actual y: 1  answer y: 1  prediction: [ 0.0653175  99.864426    0.0802261   0.0110199   0.06381016  0.01558159\n",
            "  0.07347262  0.3123334 ]\n",
            "index: 199  actual y: 5  answer y: 5  prediction: [ 0.00278943  0.00777164  0.03517514  0.00997986  0.01250084 99.99114\n",
            "  0.01399892  0.00777487]\n",
            "index: 200  actual y: 3  answer y: 3  prediction: [ 0.73184085  1.1146301   0.16352454 94.57672     0.3577565   0.33795372\n",
            "  0.04409588  0.02855928]\n",
            "index: 201  actual y: 0  answer y: 0  prediction: [97.03638     0.0179031   0.03448522  0.119326    6.5660253   0.01007474\n",
            "  0.2189123   0.1928186 ]\n",
            "index: 202  actual y: 0  answer y: 0  prediction: [99.3607      0.02174832  1.0244163   0.01997459  1.0988978   0.0241019\n",
            "  0.02082702  0.15577666]\n",
            "index: 203  actual y: 2  answer y: 2  prediction: [ 0.31384423  0.03596478 96.81613     0.13947144  1.3037739   3.0559525\n",
            "  0.04809146  0.3422717 ]\n",
            "index: 204  actual y: 2  answer y: 2  prediction: [ 0.03574775  0.05937472 99.65541     0.15890215  0.73458266  0.021239\n",
            "  0.02819134  0.06787307]\n",
            "index: 205  actual y: 0  answer y: 0  prediction: [99.657486    0.09056734  0.13559729  0.02801738  0.0312092   0.17855994\n",
            "  0.1504698   0.02962665]\n",
            "index: 206  actual y: 2  answer y: 5  prediction: [ 0.0816141   0.11242186  4.1693745   0.1299828   4.6845436  84.9713\n",
            "  1.9618909   0.03446839]\n",
            "index: 207  actual y: 2  answer y: 5  prediction: [ 0.09093387  0.9269266  10.210199    0.0907653   1.6531255  41.01587\n",
            "  0.09240567 11.588252  ]\n",
            "index: 208  actual y: 0  answer y: 0  prediction: [99.92409     0.01018254  0.1658587   0.02297357  0.00764008  0.00265371\n",
            "  0.03191117  0.02257971]\n",
            "index: 209  actual y: 7  answer y: 7  prediction: [ 0.00781405  0.00647186  0.02631322  0.03576243  0.01472261  0.01277775\n",
            "  0.01864868 99.96901   ]\n",
            "index: 210  actual y: 6  answer y: 6  prediction: [ 0.35427964  0.01572817  0.02894411  0.09398885  0.08939509  0.1620022\n",
            " 99.60986     0.07548476]\n",
            "index: 211  actual y: 2  answer y: 5  prediction: [19.773552    0.03881592  0.55012673  0.23950827  0.60129595 41.547886\n",
            "  2.3408792   0.1641393 ]\n",
            "index: 212  actual y: 6  answer y: 0  prediction: [38.523125    0.16451487  0.05833633  3.9650042   0.23275544  0.0448365\n",
            " 33.390553    0.03726326]\n",
            "index: 213  actual y: 5  answer y: 7  prediction: [ 0.07747462  1.251555    0.12550798  0.37149447  0.08130269  2.248032\n",
            "  0.01894638 91.22632   ]\n",
            "index: 214  actual y: 6  answer y: 6  prediction: [ 0.00721975  0.01039541  0.07735343  0.01574993  0.02749161  0.01065875\n",
            " 99.94294     0.04484045]\n",
            "index: 215  actual y: 1  answer y: 1  prediction: [ 0.0098085  99.97549     0.0232273   0.00967246  0.02969305  0.01691315\n",
            "  0.03228518  0.04145385]\n",
            "index: 216  actual y: 5  answer y: 5  prediction: [11.633686    0.53247523  0.18590842  0.8510603   0.44885314 94.018\n",
            "  0.0778932   0.05474091]\n",
            "index: 217  actual y: 3  answer y: 3  prediction: [ 0.06732868  7.8195014   0.05275237 64.57645     2.4609413   0.21582597\n",
            "  0.06046797  0.639107  ]\n",
            "index: 218  actual y: 2  answer y: 2  prediction: [ 0.04289868  0.06359714 99.66843     0.19700317  0.03648587  0.09151982\n",
            "  0.2517369   0.08307624]\n",
            "index: 219  actual y: 4  answer y: 4  prediction: [ 0.49884692  0.00852936  0.0478487   0.693556   98.93534     0.01445365\n",
            "  0.25063908  0.02782259]\n",
            "index: 220  actual y: 5  answer y: 5  prediction: [ 0.01730829  0.02542185  0.05160218  0.07414575  0.04192718 99.904434\n",
            "  0.00461035  0.23744512]\n",
            "index: 221  actual y: 2  answer y: 2  prediction: [ 0.09707364  0.00942284 99.94132     0.01024129  0.01514224  0.0103777\n",
            "  0.01791197  0.02274917]\n",
            "index: 222  actual y: 7  answer y: 7  prediction: [ 0.00558059  0.00405529  0.01413217  0.0125958   0.01143869  0.01860103\n",
            "  0.01406764 99.98192   ]\n",
            "index: 223  actual y: 6  answer y: 5  prediction: [ 1.5169176   0.06917564  0.10866274  2.4629493   8.676063   28.091583\n",
            "  1.5416509   0.03558996]\n",
            "index: 224  actual y: 5  answer y: 5  prediction: [ 0.01100763  0.04550408  0.322512    0.18584491  0.12854089 99.89881\n",
            "  0.00807155  0.04399075]\n",
            "index: 225  actual y: 3  answer y: 3  prediction: [ 0.13793954  0.04551516  0.08803697 99.74064     0.1720028   0.03043193\n",
            "  0.2566552   0.02166149]\n",
            "index: 226  actual y: 5  answer y: 2  prediction: [ 1.9040197   0.10271873 62.445004    0.51586056  1.7928969   8.999821\n",
            "  0.20766212  0.7277531 ]\n",
            "index: 227  actual y: 7  answer y: 7  prediction: [ 0.04259666  0.00983069  0.04400945  0.02411406  0.01226708  0.01190273\n",
            "  0.02369981 99.9656    ]\n",
            "index: 228  actual y: 0  answer y: 0  prediction: [99.95827     0.02595244  0.07662731  0.0135878   0.04526834  0.00805794\n",
            "  0.01171015  0.0850701 ]\n",
            "index: 229  actual y: 5  answer y: 5  prediction: [ 0.05684123 12.696503    0.02717128  0.87953323  0.18966873 49.03502\n",
            "  0.7681764   1.1378605 ]\n",
            "index: 230  actual y: 3  answer y: 3  prediction: [ 2.902446    0.4351847   0.3990415  82.99143     2.1887283   0.00842811\n",
            "  0.13292554  0.02013237]\n",
            "index: 231  actual y: 3  answer y: 3  prediction: [ 0.0844671   0.00997855  0.02043167 99.864395    0.01678311  0.02973041\n",
            "  0.0965644   0.08550765]\n",
            "index: 232  actual y: 1  answer y: 1  prediction: [ 0.20328599 97.3925      0.16541734  0.6361162   0.5075275   0.51784354\n",
            "  0.06138796  0.23719154]\n",
            "index: 233  actual y: 5  answer y: 5  prediction: [ 0.01475222  0.01613235  0.02993385  0.0191279   0.02127788 99.98578\n",
            "  0.01660229  0.00659538]\n",
            "index: 234  actual y: 3  answer y: 3  prediction: [ 5.8604717   0.43336245  0.17389114 35.70044     0.13232663  0.07380792\n",
            "  0.04618079  8.345996  ]\n",
            "index: 235  actual y: 7  answer y: 7  prediction: [ 0.01245822  0.00437349  0.03337599  0.05894821  0.00890148  0.00832886\n",
            "  0.01418385 99.968864  ]\n",
            "index: 236  actual y: 6  answer y: 6  prediction: [ 0.17936555  0.11489234  0.10457923  0.05186388 23.201984    2.6828465\n",
            " 68.94222     0.04856692]\n",
            "index: 237  actual y: 0  answer y: 0  prediction: [98.51104     0.01208597  0.04996556  0.06379554  0.94226253  0.3230964\n",
            "  1.032577    0.02632333]\n",
            "index: 238  actual y: 3  answer y: 3  prediction: [ 1.0403631   0.03924213  0.01670441 64.95396     0.07414448  0.21608788\n",
            "  0.49922174  5.8369365 ]\n",
            "index: 239  actual y: 3  answer y: 3  prediction: [ 0.12053462  0.04058319  0.01677337 99.26874     0.18527216  0.02468379\n",
            "  0.05210192  0.2222582 ]\n",
            "index: 240  actual y: 6  answer y: 6  prediction: [ 0.7035037   0.02977869  0.23758622  0.6074294   0.094372    0.09732419\n",
            " 97.91088     0.10797998]\n",
            "index: 241  actual y: 3  answer y: 3  prediction: [ 0.03892671  0.35729155 51.595108   56.75305     0.12006904  0.5318047\n",
            "  0.11685883  0.87271404]\n",
            "index: 242  actual y: 1  answer y: 1  prediction: [ 0.03420775 66.336525    2.2248056   4.1691484   1.523913    2.2718225\n",
            "  0.01878374  0.14019175]\n",
            "index: 243  actual y: 2  answer y: 1  prediction: [ 0.56185365 80.772835   22.60872     0.20245826  0.11139903  0.18627323\n",
            "  0.2331098   0.00819605]\n",
            "8/8 [==============================] - 0s 51ms/step - loss: 0.1415 - accuracy: 0.8033 - precision: 0.8725 - recall: 0.7719 - f1score: 0.8186\n",
            "loss: 0.142, accuracy: 0.803, precision: 0.873, recall: 0.772, f1score: 0.819\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGR0eJBKxiG_"
      },
      "source": [
        ""
      ],
      "execution_count": 68,
      "outputs": []
    }
  ]
}