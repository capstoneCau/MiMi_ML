{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batchSIZE is 512, Learning Rate is 0.001\n",
      "Found 806 images belonging to 6 classes.\n",
      "Found 104 images belonging to 6 classes.\n",
      "Found 288 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "import os # miscellaneous operating system interfaces\n",
    "import shutil # high-level file operations\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import decimal\n",
    "import random\n",
    "from itertools import product\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Conv2D \n",
    "from keras.layers import MaxPooling2D \n",
    "from keras.layers import Flatten \n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.applications import MobileNetV2, Xception, DenseNet121,ResNet50V2,NASNetMobile\n",
    "from keras.applications.mobilenet_v2 import preprocess_input\n",
    "from keras.layers import  Input, Conv2D, Conv2DTranspose, ReLU,AveragePooling2D, MaxPooling2D, Dense, BatchNormalization, GlobalAveragePooling2D, Concatenate\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "base_dir = r'C:\\Users\\1217s\\Desktop\\capstone deeplearning\\images\\woman'\n",
    "\n",
    "train_img_dir = r'C:\\Users\\1217s\\Desktop\\capstone deeplearning\\images\\woman\\train'\n",
    "\n",
    "test_img_dir = r'C:\\Users\\1217s\\Desktop\\capstone deeplearning\\images\\woman\\test'\n",
    "\n",
    "val_img_dir = r'C:\\Users\\1217s\\Desktop\\capstone deeplearning\\images\\woman\\val'\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "training_epochs = 20\n",
    "batch_size = 512\n",
    "\n",
    "\n",
    "\n",
    "print(f'batchSIZE is {batch_size}, Learning Rate is {learning_rate}')\n",
    "train_datagen = ImageDataGenerator(\n",
    "                                    rotation_range=40,\n",
    "                                    width_shift_range=0.2,\n",
    "                                    height_shift_range=0.2,\n",
    "                                    rescale=1./255,\n",
    "                                    shear_range=0.2,\n",
    "                                    zoom_range=0.2,\n",
    "                                    horizontal_flip=True,\n",
    "                                    fill_mode = \"nearest\")\n",
    "val_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "categories = ['dog','cat','rabbit','squirrel','deer','fox']\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(train_img_dir, target_size=(128,128), \n",
    "                                             classes=categories, \n",
    "                                             batch_size=batch_size)\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(test_img_dir,\n",
    "                                        target_size=(128,128), \n",
    "                                        classes=categories, \n",
    "                                        batch_size=batch_size)\n",
    "\n",
    "val_set = test_datagen.flow_from_directory(val_img_dir,\n",
    "                                        target_size=(128,128), \n",
    "                                        classes=categories, \n",
    "                                        batch_size=batch_size)\n",
    "\n",
    "x_train, y_train = next(training_set)\n",
    "x_test, y_test = next(test_set)\n",
    "x_val,y_val = next(val_set)   \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "    \n",
    "\n",
    "    conv2_1 = Conv2D(32, 3, 1, 'SAME')(inputs)\n",
    "    conv2_2 = Conv2D(32, 3, 1, 'SAME')(conv2_1)\n",
    "    pool2_3 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv2_2)\n",
    "    br1 = BatchNormalization()(pool2_3)\n",
    "    \n",
    "    conv2_1 = Conv2D(32, 3, 1, 'SAME')(br1)\n",
    "    conv2_2 = Conv2D(32, 3, 1, 'SAME')(conv2_1)\n",
    "    pool2_3 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv2_2)\n",
    "    br1 = BatchNormalization()(pool2_3)\n",
    "    \n",
    "    \n",
    "    conv3_1 = Conv2D(64, 3, 1, 'SAME')(br1)\n",
    "    conv3_2 = Conv2D(64, 3, 1, 'SAME')(conv3_1)\n",
    "    pool3_2 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv3_2)\n",
    "    br2 = BatchNormalization()(pool3_2)\n",
    "    \n",
    "    conv3_1 = Conv2D(64, 3, 1, 'SAME')(br2)\n",
    "    conv3_2 = Conv2D(64, 3, 1, 'SAME')(conv3_1)\n",
    "    pool3_2 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv3_2)\n",
    "    br2 = BatchNormalization()(pool3_2)\n",
    "    \n",
    "    \n",
    "    conv4_1 = Conv2D(128, 3, 1, 'SAME')(br2)\n",
    "    conv4_2 = Conv2D(128, 3, 1, 'SAME')(conv4_1)\n",
    "    pool4_2 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv4_2)\n",
    "    br3 = BatchNormalization()(pool4_2)\n",
    "    \n",
    "    conv4_1 = Conv2D(128, 3, 1, 'SAME')(br3)\n",
    "    conv4_2 = Conv2D(128, 3, 1, 'SAME')(conv4_1)\n",
    "    pool4_2 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv4_2)\n",
    "    br3 = BatchNormalization()(pool4_2)\n",
    "    \n",
    "    flatten1 = Flatten()(pool4_2)\n",
    "    dense1 = Dense(units = 512, activation = 'relu')(flatten1)\n",
    "    dense2 = Dense(units = 1024, activation = 'relu')(dense1)\n",
    "    dr1 = Dropout(0.7)(dense2)\n",
    "    dense3 = Dense(units = 5, activation = 'sigmoid')(dr1)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=dense3) \n",
    "\n",
    "def vgg16():\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "    \n",
    "\n",
    "    conv1_1 = Conv2D(3, 3, 1, 'SAME',activation = 'relu')(inputs)\n",
    "    conv1_2 = Conv2D(64, 3, 1, 'SAME',activation = 'relu')(conv1_1)\n",
    "    conv1_3 = Conv2D(64, 3, 1, 'SAME',activation = 'relu')(conv1_2)\n",
    "    pool1 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv1_3)\n",
    "    \n",
    "    conv2_1 = Conv2D(128, 3, 1, 'SAME',activation = 'relu')(pool1)\n",
    "    conv2_2 = Conv2D(128, 3, 1, 'SAME',activation = 'relu')(conv2_1)\n",
    "    pool2 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv2_2)\n",
    "    \n",
    "    \n",
    "    conv3_1 = Conv2D(256, 3, 1, 'SAME',activation = 'relu')(pool2)\n",
    "    conv3_2 = Conv2D(256, 3, 1, 'SAME',activation = 'relu')(conv3_1)\n",
    "    conv3_3 = Conv2D(256, 3, 1, 'SAME',activation = 'relu')(conv3_2)\n",
    "    pool3 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv3_3)\n",
    "\n",
    "    \n",
    "    conv4_1 = Conv2D(512, 3, 1, 'SAME',activation = 'relu')(pool3)\n",
    "    conv4_2 = Conv2D(512, 3, 1, 'SAME',activation = 'relu')(conv4_1)\n",
    "    conv4_3 = Conv2D(512, 3, 1, 'SAME',activation = 'relu')(conv4_2)\n",
    "    pool4 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv4_3)\n",
    "    \n",
    "    conv5_1 = Conv2D(512, 3, 1, 'SAME',activation = 'relu')(pool4)\n",
    "    conv5_2 = Conv2D(512, 3, 1, 'SAME',activation = 'relu')(conv5_1)\n",
    "    conv5_3 = Conv2D(512, 3, 1, 'SAME',activation = 'relu')(conv5_2)\n",
    "    pool5 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv5_3)\n",
    "    \n",
    "    flatten1 = Flatten()(pool5)\n",
    "    dense1 = Dense(units = 4096)(flatten1)\n",
    "    dense2 = Dense(units = 4096)(dense1)\n",
    "    dense3 = Dense(units = 5, activation = 'sigmoid')(dense2)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=dense3) \n",
    "\n",
    "def mobile_net():\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "    mobileNet = MobileNetV2(weights = \"imagenet\", include_top=False,input_shape = (128,128,3)\n",
    "                            ,input_tensor = inputs)\n",
    "    for layer in mobileNet.layers:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    output = mobileNet.output\n",
    "    pooling = AveragePooling2D(pool_size=(4,4),padding='SAME')(output)\n",
    "    \n",
    "    flatten1 = Flatten()(pooling)\n",
    "    dense1 = Dense(units = 512, activation = 'relu')(flatten1)\n",
    "    dense2 = Dense(units = 1024, activation = 'relu')(dense1)\n",
    "    dr1 = Dropout(0.7)(dense2)\n",
    "    dense3 = Dense(units = 5, activation = 'sigmoid')(dr1)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=dense3)\n",
    "\n",
    "def xception():\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "    xception = Xception(weights = \"imagenet\", include_top=False,input_shape = (128,128,3)\n",
    "                            ,input_tensor = inputs)\n",
    "    for layer in xception.layers:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    output = xception.output\n",
    "    pooling = AveragePooling2D(pool_size=(4,4),padding='SAME')(output)\n",
    "    \n",
    "    flatten1 = Flatten()(pooling)\n",
    "    dense1 = Dense(units = 512, activation = 'relu')(flatten1)\n",
    "    dense2 = Dense(units = 1024, activation = 'relu')(dense1)\n",
    "    dr1 = Dropout(0.7)(dense2)\n",
    "    dense3 = Dense(units = 5, activation = 'sigmoid')(dr1)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=dense3)\n",
    "\n",
    "def resnet():\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "    resnet = ResNet50V2 (weights = \"imagenet\", include_top=False,input_shape = (128,128,3)\n",
    "                            ,input_tensor = inputs)\n",
    "    for layer in resnet.layers:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    output = resnet.output\n",
    "    pooling = AveragePooling2D(pool_size=(4,4),padding='SAME')(output)\n",
    "    \n",
    "    flatten1 = Flatten()(pooling)\n",
    "    dense1 = Dense(units = 512, activation = 'relu')(flatten1)\n",
    "    dense2 = Dense(units = 1024, activation = 'relu')(dense1)\n",
    "    dr1 = Dropout(0.7)(dense2)\n",
    "    dense3 = Dense(units = 5, activation = 'sigmoid')(dr1)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=dense3)\n",
    "\n",
    "def densenet():\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "    densenet = DenseNet121(weights = \"imagenet\", include_top=False,input_shape = (128,128,3)\n",
    "                            ,input_tensor = inputs)\n",
    "    for layer in densenet.layers:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    output = densenet.output\n",
    "    pooling = AveragePooling2D(pool_size=(4,4),padding='SAME')(output)\n",
    "    \n",
    "    flatten1 = Flatten()(pooling)\n",
    "    dense1 = Dense(units = 512, activation = 'relu')(flatten1)\n",
    "    dense2 = Dense(units = 1024, activation = 'relu')(dense1)\n",
    "    dr1 = Dropout(0.7)(dense2)\n",
    "    dense3 = Dense(units = 6, activation = 'sigmoid')(dr1)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=dense3)\n",
    "\n",
    "class ResidualUnit(tf.keras.Model):\n",
    "    def __init__(self, filter_in, filter_out, kernel_size):\n",
    "        super(ResidualUnit, self).__init__()\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filter_out, kernel_size, padding='same')\n",
    "        \n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filter_out, kernel_size, padding='same')\n",
    "        \n",
    "        if filter_in == filter_out:\n",
    "            self.identity = lambda x: x\n",
    "        else:\n",
    "            self.identity = tf.keras.layers.Conv2D(filter_out, (1,1), padding='same')\n",
    "\n",
    "    def call(self, x, training=False, mask=None):\n",
    "        h = self.bn1(x, training=training)\n",
    "        h = tf.nn.relu(h)\n",
    "        h = self.conv1(h)\n",
    "        \n",
    "        h = self.bn2(h, training=training)\n",
    "        h = tf.nn.relu(h)\n",
    "        h = self.conv2(h)\n",
    "        return self.identity(x) + h\n",
    "    \n",
    "class ResnetLayer(tf.keras.Model):\n",
    "    def __init__(self, filter_in, filters, kernel_size):\n",
    "        super(ResnetLayer, self).__init__()\n",
    "        self.sequence = list()\n",
    "        for f_in, f_out in zip([filter_in] + list(filters), filters):\n",
    "            self.sequence.append(ResidualUnit(f_in, f_out, kernel_size))\n",
    "\n",
    "    def call(self, x, training=False, mask=None):\n",
    "        for unit in self.sequence:\n",
    "            x = unit(x, training=training)\n",
    "        return x\n",
    "    \n",
    "class ResNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu') # 28x28x8\n",
    "        \n",
    "        self.res1 = ResnetLayer(64, (16, 16), (3, 3)) # 28x28x16\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D((2, 2)) # 14x14x16\n",
    "        \n",
    "        self.res2 = ResnetLayer(128, (32, 32), (3, 3)) # 14x14x32\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D((2, 2)) # 7x7x32\n",
    "        \n",
    "        \n",
    "        self.res3 = ResnetLayer(256, (64, 64), (3, 3)) # 7x7x64\n",
    "        self.pool = tf.keras.layers.MaxPool2D((2, 2)) # 7x7x32\n",
    "\n",
    "        \n",
    "        self.res4 = ResnetLayer(512, (64, 64), (3, 3)) # 7x7x64\n",
    "        self.pool = tf.keras.layers.MaxPool2D((2, 2)) # 7x7x32\n",
    "\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = Dense(units = 1024, activation = 'relu')\n",
    "        self.dense2 = Dense(units = 1024, activation = 'relu')\n",
    "        self.dense3 = tf.keras.layers.Dense(5, activation='softmax')\n",
    "        \n",
    "    def call(self, x, training=False, mask=None):\n",
    "        x = self.conv1(x)\n",
    "        x = self.res1(x, training=training)\n",
    "        x = self.pool1(x)\n",
    "        x = self.res2(x, training=training)\n",
    "        x = self.pool2(x)\n",
    "        x = self.res3(x, training=training)\n",
    "        x = self.pool2(x)\n",
    "        x = self.res4(x, training=training)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return self.dense3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "20/20 [==============================] - 58s 3s/step - loss: 0.5520 - accuracy: 0.1660 - precision: 0.1780 - recall: 0.0712 - f1score: 0.0733 - val_loss: 1.7503 - val_accuracy: 0.1493 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1score: 0.0000e+00\n",
      "Epoch 2/60\n",
      "20/20 [==============================] - 57s 3s/step - loss: 0.4678 - accuracy: 0.2129 - precision: 0.2300 - recall: 0.0212 - f1score: 0.0380 - val_loss: 1.1165 - val_accuracy: 0.1806 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1score: 0.0000e+00\n",
      "Epoch 3/60\n",
      "20/20 [==============================] - 57s 3s/step - loss: 0.4395 - accuracy: 0.2832 - precision: 0.4068 - recall: 0.0825 - f1score: 0.1345 - val_loss: 0.5223 - val_accuracy: 0.2083 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1score: 0.0000e+00\n",
      "Epoch 4/60\n",
      "20/20 [==============================] - 57s 3s/step - loss: 0.3862 - accuracy: 0.4102 - precision: 0.5712 - recall: 0.1671 - f1score: 0.2536 - val_loss: 0.6274 - val_accuracy: 0.1944 - val_precision: 0.1481 - val_recall: 0.0139 - val_f1score: 0.0252\n",
      "Epoch 5/60\n",
      "20/20 [==============================] - 57s 3s/step - loss: 0.3307 - accuracy: 0.5469 - precision: 0.6836 - recall: 0.3571 - f1score: 0.4641 - val_loss: 0.6714 - val_accuracy: 0.2257 - val_precision: 0.2691 - val_recall: 0.0903 - val_f1score: 0.1334\n",
      "Epoch 6/60\n",
      "20/20 [==============================] - 57s 3s/step - loss: 0.2896 - accuracy: 0.6113 - precision: 0.7337 - recall: 0.4733 - f1score: 0.5726 - val_loss: 0.7482 - val_accuracy: 0.2535 - val_precision: 0.2516 - val_recall: 0.1354 - val_f1score: 0.1745\n",
      "Epoch 7/60\n",
      "20/20 [==============================] - 57s 3s/step - loss: 0.2592 - accuracy: 0.6758 - precision: 0.7637 - recall: 0.5692 - f1score: 0.6492 - val_loss: 0.6491 - val_accuracy: 0.2396 - val_precision: 0.2449 - val_recall: 0.2118 - val_f1score: 0.2270\n",
      "Epoch 8/60\n",
      "20/20 [==============================] - 57s 3s/step - loss: 0.2316 - accuracy: 0.7383 - precision: 0.7889 - recall: 0.6611 - f1score: 0.7179 - val_loss: 1.1557 - val_accuracy: 0.2569 - val_precision: 0.2638 - val_recall: 0.2222 - val_f1score: 0.2408\n",
      "Epoch 9/60\n",
      "20/20 [==============================] - 57s 3s/step - loss: 0.1824 - accuracy: 0.7754 - precision: 0.8102 - recall: 0.7246 - f1score: 0.7637 - val_loss: 2.3044 - val_accuracy: 0.1944 - val_precision: 0.1888 - val_recall: 0.2014 - val_f1score: 0.1948\n",
      "Epoch 10/60\n",
      "20/20 [==============================] - 57s 3s/step - loss: 0.2067 - accuracy: 0.7637 - precision: 0.8105 - recall: 0.7053 - f1score: 0.7527 - val_loss: 0.8399 - val_accuracy: 0.3194 - val_precision: 0.3302 - val_recall: 0.2674 - val_f1score: 0.2951\n",
      "Epoch 11/60\n",
      "20/20 [==============================] - 57s 3s/step - loss: 0.1030 - accuracy: 0.8906 - precision: 0.9220 - recall: 0.8675 - f1score: 0.8933 - val_loss: 1.4822 - val_accuracy: 0.2986 - val_precision: 0.3123 - val_recall: 0.2882 - val_f1score: 0.2996\n",
      "Epoch 12/60\n",
      "20/20 [==============================] - 56s 3s/step - loss: 0.0459 - accuracy: 0.9668 - precision: 0.9691 - recall: 0.9541 - f1score: 0.9613 - val_loss: 1.7324 - val_accuracy: 0.3125 - val_precision: 0.3111 - val_recall: 0.3125 - val_f1score: 0.3118\n",
      "Epoch 13/60\n",
      "20/20 [==============================] - 56s 3s/step - loss: 0.0192 - accuracy: 0.9902 - precision: 0.9913 - recall: 0.9857 - f1score: 0.9884 - val_loss: 1.2898 - val_accuracy: 0.3889 - val_precision: 0.3922 - val_recall: 0.3889 - val_f1score: 0.3903\n",
      "Epoch 14/60\n",
      "20/20 [==============================] - 56s 3s/step - loss: 0.0142 - accuracy: 0.9902 - precision: 0.9895 - recall: 0.9857 - f1score: 0.9876 - val_loss: 1.2538 - val_accuracy: 0.3681 - val_precision: 0.3856 - val_recall: 0.3646 - val_f1score: 0.3747\n",
      "Epoch 15/60\n",
      "20/20 [==============================] - 56s 3s/step - loss: 0.0142 - accuracy: 0.9902 - precision: 0.9906 - recall: 0.9846 - f1score: 0.9875 - val_loss: 1.3282 - val_accuracy: 0.3681 - val_precision: 0.3736 - val_recall: 0.3542 - val_f1score: 0.3633\n",
      "Epoch 16/60\n",
      "20/20 [==============================] - 56s 3s/step - loss: 0.0115 - accuracy: 0.9922 - precision: 0.9887 - recall: 0.9923 - f1score: 0.9905 - val_loss: 1.5581 - val_accuracy: 0.3194 - val_precision: 0.3175 - val_recall: 0.3194 - val_f1score: 0.3182\n",
      "Epoch 17/60\n",
      "20/20 [==============================] - 55s 3s/step - loss: 0.0193 - accuracy: 0.9766 - precision: 0.9751 - recall: 0.9731 - f1score: 0.9740 - val_loss: 1.0379 - val_accuracy: 0.4375 - val_precision: 0.4348 - val_recall: 0.3993 - val_f1score: 0.4160\n",
      "Epoch 18/60\n",
      "20/20 [==============================] - 56s 3s/step - loss: 0.0238 - accuracy: 0.9766 - precision: 0.9736 - recall: 0.9788 - f1score: 0.9761 - val_loss: 1.4314 - val_accuracy: 0.3646 - val_precision: 0.3763 - val_recall: 0.3681 - val_f1score: 0.3719\n",
      "Epoch 19/60\n",
      "20/20 [==============================] - 56s 3s/step - loss: 0.0268 - accuracy: 0.9707 - precision: 0.9721 - recall: 0.9645 - f1score: 0.9680 - val_loss: 0.8184 - val_accuracy: 0.4896 - val_precision: 0.4999 - val_recall: 0.4722 - val_f1score: 0.4851\n",
      "Epoch 20/60\n",
      "20/20 [==============================] - 56s 3s/step - loss: 0.0518 - accuracy: 0.9531 - precision: 0.9481 - recall: 0.9491 - f1score: 0.9483 - val_loss: 0.8831 - val_accuracy: 0.3924 - val_precision: 0.4225 - val_recall: 0.3854 - val_f1score: 0.4029\n",
      "Epoch 21/60\n",
      "20/20 [==============================] - 56s 3s/step - loss: 0.0499 - accuracy: 0.9453 - precision: 0.9589 - recall: 0.9423 - f1score: 0.9500 - val_loss: 0.7675 - val_accuracy: 0.4826 - val_precision: 0.5014 - val_recall: 0.4792 - val_f1score: 0.4899\n",
      "Epoch 22/60\n",
      "20/20 [==============================] - 55s 3s/step - loss: 0.0084 - accuracy: 0.9961 - precision: 0.9934 - recall: 0.9915 - f1score: 0.9924 - val_loss: 0.7126 - val_accuracy: 0.4757 - val_precision: 0.4922 - val_recall: 0.4583 - val_f1score: 0.4745\n",
      "Epoch 23/60\n",
      "20/20 [==============================] - 55s 3s/step - loss: 0.0075 - accuracy: 0.9961 - precision: 0.9925 - recall: 0.9887 - f1score: 0.9903 - val_loss: 0.6348 - val_accuracy: 0.5278 - val_precision: 0.5387 - val_recall: 0.5035 - val_f1score: 0.5202\n",
      "Epoch 24/60\n",
      "20/20 [==============================] - 55s 3s/step - loss: 0.0116 - accuracy: 0.9883 - precision: 0.9903 - recall: 0.9904 - f1score: 0.9903 - val_loss: 0.5131 - val_accuracy: 0.5729 - val_precision: 0.6018 - val_recall: 0.5417 - val_f1score: 0.5693\n",
      "Epoch 25/60\n",
      "20/20 [==============================] - 56s 3s/step - loss: 0.0063 - accuracy: 0.9941 - precision: 0.9980 - recall: 0.9942 - f1score: 0.9961 - val_loss: 0.5098 - val_accuracy: 0.5556 - val_precision: 0.5558 - val_recall: 0.5139 - val_f1score: 0.5338\n",
      "Epoch 26/60\n",
      "20/20 [==============================] - 56s 3s/step - loss: 0.0104 - accuracy: 0.9902 - precision: 0.9922 - recall: 0.9885 - f1score: 0.9903 - val_loss: 0.5535 - val_accuracy: 0.5347 - val_precision: 0.5576 - val_recall: 0.5139 - val_f1score: 0.5347\n",
      "Epoch 27/60\n",
      "20/20 [==============================] - 56s 3s/step - loss: 0.0035 - accuracy: 1.0000 - precision: 1.0000 - recall: 0.9962 - f1score: 0.9980 - val_loss: 0.4823 - val_accuracy: 0.5764 - val_precision: 0.5913 - val_recall: 0.5486 - val_f1score: 0.5690\n",
      "Epoch 28/60\n",
      "20/20 [==============================] - 56s 3s/step - loss: 0.0025 - accuracy: 1.0000 - precision: 0.9981 - recall: 1.0000 - f1score: 0.9991 - val_loss: 0.4630 - val_accuracy: 0.5868 - val_precision: 0.6224 - val_recall: 0.5694 - val_f1score: 0.5944\n",
      "Epoch 29/60\n",
      "20/20 [==============================] - 56s 3s/step - loss: 0.0025 - accuracy: 0.9980 - precision: 1.0000 - recall: 0.9981 - f1score: 0.9990 - val_loss: 0.4680 - val_accuracy: 0.6111 - val_precision: 0.6564 - val_recall: 0.5903 - val_f1score: 0.6209\n",
      "Epoch 30/60\n",
      "20/20 [==============================] - 56s 3s/step - loss: 0.0023 - accuracy: 0.9980 - precision: 0.9981 - recall: 0.9981 - f1score: 0.9981 - val_loss: 0.4749 - val_accuracy: 0.6146 - val_precision: 0.6467 - val_recall: 0.5868 - val_f1score: 0.6147\n",
      "Epoch 31/60\n",
      "20/20 [==============================] - 56s 3s/step - loss: 0.0018 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.4714 - val_accuracy: 0.6111 - val_precision: 0.6517 - val_recall: 0.5868 - val_f1score: 0.6171\n",
      "Epoch 32/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 55s 3s/step - loss: 0.0015 - accuracy: 0.9980 - precision: 1.0000 - recall: 0.9972 - f1score: 0.9986 - val_loss: 0.4694 - val_accuracy: 0.6146 - val_precision: 0.6450 - val_recall: 0.6007 - val_f1score: 0.6216\n",
      "Epoch 33/60\n",
      "20/20 [==============================] - 56s 3s/step - loss: 0.0036 - accuracy: 0.9980 - precision: 0.9981 - recall: 0.9981 - f1score: 0.9981 - val_loss: 0.5113 - val_accuracy: 0.5972 - val_precision: 0.6219 - val_recall: 0.5764 - val_f1score: 0.5977\n",
      "Epoch 34/60\n",
      "20/20 [==============================] - 55s 3s/step - loss: 9.8516e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.5197 - val_accuracy: 0.5938 - val_precision: 0.6272 - val_recall: 0.5764 - val_f1score: 0.6006\n",
      "Epoch 35/60\n",
      "20/20 [==============================] - 55s 3s/step - loss: 0.0012 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.5186 - val_accuracy: 0.5972 - val_precision: 0.6259 - val_recall: 0.5764 - val_f1score: 0.5997\n",
      "Epoch 36/60\n",
      "20/20 [==============================] - 55s 3s/step - loss: 0.0015 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.5060 - val_accuracy: 0.6076 - val_precision: 0.6310 - val_recall: 0.5833 - val_f1score: 0.6056\n",
      "Epoch 37/60\n",
      "20/20 [==============================] - 55s 3s/step - loss: 0.0036 - accuracy: 0.9980 - precision: 0.9981 - recall: 0.9981 - f1score: 0.9981 - val_loss: 0.4866 - val_accuracy: 0.6042 - val_precision: 0.6349 - val_recall: 0.5938 - val_f1score: 0.6132\n",
      "Epoch 38/60\n",
      "20/20 [==============================] - 55s 3s/step - loss: 9.4268e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.4847 - val_accuracy: 0.6181 - val_precision: 0.6421 - val_recall: 0.5903 - val_f1score: 0.6145\n",
      "Epoch 39/60\n",
      "20/20 [==============================] - 55s 3s/step - loss: 7.1517e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.4845 - val_accuracy: 0.6146 - val_precision: 0.6557 - val_recall: 0.5972 - val_f1score: 0.6246\n",
      "Epoch 40/60\n",
      "20/20 [==============================] - 55s 3s/step - loss: 8.6531e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.4822 - val_accuracy: 0.6111 - val_precision: 0.6604 - val_recall: 0.6042 - val_f1score: 0.6305\n",
      "Epoch 41/60\n",
      "20/20 [==============================] - 55s 3s/step - loss: 9.0358e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.4832 - val_accuracy: 0.6111 - val_precision: 0.6570 - val_recall: 0.6076 - val_f1score: 0.6308\n",
      "Epoch 42/60\n",
      "20/20 [==============================] - 55s 3s/step - loss: 7.7359e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.4824 - val_accuracy: 0.6181 - val_precision: 0.6570 - val_recall: 0.6076 - val_f1score: 0.6308\n",
      "Epoch 43/60\n",
      "20/20 [==============================] - 55s 3s/step - loss: 0.0011 - accuracy: 1.0000 - precision: 1.0000 - recall: 0.9981 - f1score: 0.9990 - val_loss: 0.4813 - val_accuracy: 0.6146 - val_precision: 0.6584 - val_recall: 0.6042 - val_f1score: 0.6294\n",
      "Epoch 44/60\n",
      "20/20 [==============================] - 56s 3s/step - loss: 0.0026 - accuracy: 0.9980 - precision: 0.9981 - recall: 0.9981 - f1score: 0.9981 - val_loss: 0.4607 - val_accuracy: 0.6215 - val_precision: 0.6597 - val_recall: 0.5938 - val_f1score: 0.6242\n",
      "Epoch 45/60\n",
      "20/20 [==============================] - 57s 3s/step - loss: 7.8610e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.4599 - val_accuracy: 0.6250 - val_precision: 0.6641 - val_recall: 0.6007 - val_f1score: 0.6299\n",
      "Epoch 46/60\n",
      "20/20 [==============================] - 60s 3s/step - loss: 8.9539e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.4623 - val_accuracy: 0.6215 - val_precision: 0.6618 - val_recall: 0.6042 - val_f1score: 0.6309\n",
      "Epoch 47/60\n",
      "18/20 [==========================>...] - ETA: 5s - loss: 8.0993e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000"
     ]
    }
   ],
   "source": [
    "def recall(y_target, y_pred):\n",
    "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
    "    # round : 반올림한다\n",
    "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "\n",
    "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
    "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
    "\n",
    "    # (True Positive + False Negative) = 실제 값이 1(Positive) 전체\n",
    "    count_true_positive_false_negative = K.sum(y_target_yn)\n",
    "\n",
    "    # Recall =  (True Positive) / (True Positive + False Negative)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    recall = count_true_positive / (count_true_positive_false_negative + K.epsilon())\n",
    "\n",
    "    # return a single tensor value\n",
    "    return recall\n",
    "\n",
    "\n",
    "def precision(y_target, y_pred):\n",
    "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
    "    # round : 반올림한다\n",
    "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "\n",
    "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
    "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
    "\n",
    "    # (True Positive + False Positive) = 예측 값이 1(Positive) 전체\n",
    "    count_true_positive_false_positive = K.sum(y_pred_yn)\n",
    "\n",
    "    # Precision = (True Positive) / (True Positive + False Positive)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    precision = count_true_positive / (count_true_positive_false_positive + K.epsilon())\n",
    "\n",
    "    # return a single tensor value\n",
    "    return precision\n",
    "\n",
    "\n",
    "def f1score(y_target, y_pred):\n",
    "    _recall = recall(y_target, y_pred)\n",
    "    _precision = precision(y_target, y_pred)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    _f1score = ( 2 * _recall * _precision) / (_recall + _precision+ K.epsilon())\n",
    "    \n",
    "    # return a single tensor value\n",
    "    return _f1score\n",
    "\n",
    "\n",
    "#model = create_model()\n",
    "#model = ResNet()\n",
    "#model = mobile_net()\n",
    "#model = xception()\n",
    "model = densenet()\n",
    "#model = resnet()\n",
    "\n",
    "## learning rate scheduing\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=learning_rate,\n",
    "                                                          decay_steps=training_epochs*10,\n",
    "                                                          decay_rate=0.4,\n",
    "                                                          staircase=True)\n",
    "## optimizer는 Adam, loss는 sparse categorical crossentropy 사용\n",
    "## label이 ont-hot으로 encoding 안 된 경우에 sparse categorical corssentropy 및 sparse categorical accuracy 사용\n",
    "model.compile(keras.optimizers.Adam(lr_schedule), loss = 'binary_crossentropy', metrics=['accuracy', precision, recall, f1score])\n",
    "\n",
    "## Train!\n",
    "## Train!\n",
    "history = model.fit(x_train, y_train, steps_per_epoch=training_epochs,\n",
    "         epochs=60, validation_data = (x_val,y_val))\n",
    "\n",
    "model.save('animal_model_woman.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = classifier.predict_generator(test_set, steps=1)\n",
    "# print(test_set.class_indices)\n",
    "# print(output)\n",
    "size = y_test[:,-1]\n",
    "print(size.size)\n",
    "\n",
    "\n",
    "# predict 10 random hand-writing data\n",
    "y_predicted = model.predict(x_test)\n",
    "for x in range(0,size.size):\n",
    "    \n",
    "    print(\"index:\", x,\n",
    "          \" actual y:\", np.argmax(y_test[x]),\n",
    "          \" answer y:\", np.argmax(y_predicted[x]),\n",
    "            \" prediction:\", np.array(y_predicted[x]))\n",
    "\n",
    "_loss, _acc, _precision, _recall, _f1score = model.evaluate(x_test, y_test)\n",
    "print('loss: {:.3f}, accuracy: {:.3f}, precision: {:.3f}, recall: {:.3f}, f1score: {:.3f}'.format(_loss, _acc, _precision, _recall, _f1score))\n",
    "# print('loss: ', evaluation[0])\n",
    "# print('accuracy', evaluation[1])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 손실 그래프\n",
    "def plot_loss(history):\n",
    "   # 선 그리기\n",
    "    plt.plot(history.history['loss'], 'y', label='train loss')\n",
    "    plt.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "   # 그래프 제목\n",
    "    plt.title('Model Loss')\n",
    "   # x,y축 이름 표시\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "   # 각 라인 표식 표시\n",
    "    plt.legend(['Train','Validation'],loc=0)\n",
    "\n",
    "# 정확도 그래프\n",
    "def plot_acc(history):\n",
    "  # dir(history.history)\n",
    "    plt.plot(history.history['accuracy'], 'b', label='train accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], 'g', label='val accuracy')\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc=0)\n",
    "\n",
    "plot_loss(history)\n",
    "plt.show()\n",
    "plot_acc(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = [32,64,128,256,512,1024]\n",
    "# learning_rate = [0.00001,0.0001,0.001]\n",
    "# for  batch_size, learning_rate in product(batch_size,learning_rate):\n",
    "#     print(f'batchSIZE is {batch_size}, Learning Rate is {learning_rate}')\n",
    "#     train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "#                                    shear_range = 0.2,\n",
    "#                                    zoom_range = 0.2,\n",
    "#                                    horizontal_flip = True)\n",
    "\n",
    "#     test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "#     categories = ['dog','cat','rabbit','squirrel','deer']\n",
    "\n",
    "#     training_set = train_datagen.flow_from_directory(train_img_dir, target_size=(128,128), \n",
    "#                                                  classes=categories, \n",
    "#                                                  batch_size=batch_size)\n",
    "\n",
    "#     test_set = test_datagen.flow_from_directory(test_img_dir,\n",
    "#                                             target_size=(128,128), \n",
    "#                                             classes=categories, \n",
    "#                                             batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "#     x_train, y_train = next(training_set)\n",
    "#     x_test, y_test = next(test_set)\n",
    "#     model = create_model()\n",
    "#     ## learning rate scheduing\n",
    "#     lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=learning_rate,\n",
    "#                                                               decay_steps=training_epochs*10,\n",
    "#                                                               decay_rate=0.4,\n",
    "#                                                               staircase=True)\n",
    "#     ## optimizer는 Adam, loss는 sparse categorical crossentropy 사용\n",
    "#     ## label이 ont-hot으로 encoding 안 된 경우에 sparse categorical corssentropy 및 sparse categorical accuracy 사용\n",
    "#     model.compile(keras.optimizers.Adam(lr_schedule), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "#     ## Train!\n",
    "#     history = model.fit(x_train, y_train, #steps_per_epoch=training_epochs,\n",
    "#          epochs=40, validation_data = (x_val,y_val))\n",
    "#     # output = classifier.predict_generator(test_set, steps=1)\n",
    "#     # print(test_set.class_indices)\n",
    "#     # print(output)\n",
    "#     size = y_test[:,-1]\n",
    "#     print(size.size)\n",
    "\n",
    "\n",
    "#     # predict 10 random hand-writing data\n",
    "#     y_predicted = model.predict(x_test)\n",
    "#     for x in range(0,size.size):\n",
    "\n",
    "#         print(\"index:\", x,\n",
    "#               \" actual y:\", np.argmax(y_test[x]),\n",
    "#               \" answer y:\", np.argmax(y_predicted[x]),\n",
    "#                 \" prediction:\", np.array(y_predicted[x]))\n",
    "\n",
    "#     evaluation = model.evaluate(x_test, y_test)\n",
    "#     print('loss: ', evaluation[0])\n",
    "#     print('accuracy', evaluation[1])\n",
    "\n",
    "#     plot_loss(history)\n",
    "#     plt.show()\n",
    "#     plot_acc(history)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
