{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6AuZP58d8644",
    "outputId": "42b3a65c-ffa0-4656-834e-db9d81293d92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batchSIZE is 512, Learning Rate is 0.001\n",
      "Found 949 images belonging to 7 classes.\n",
      "Found 116 images belonging to 7 classes.\n",
      "Found 330 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os # miscellaneous operating system interfaces\n",
    "import shutil # high-level file operations\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import random\n",
    "from itertools import product\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Conv2D \n",
    "from keras.layers import MaxPooling2D \n",
    "from keras.layers import Flatten \n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.applications import MobileNetV2, Xception, DenseNet121,ResNet50V2,NASNetMobile,InceptionV3, InceptionResNetV2,VGG19\n",
    "from keras.applications.mobilenet_v2 import preprocess_input\n",
    "from keras.layers import  Input, Conv2D, Conv2DTranspose, ReLU,AveragePooling2D, MaxPooling2D, Dense, BatchNormalization, GlobalAveragePooling2D, Concatenate,Softmax\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "\n",
    "base_dir = \"/content/drive/My Drive/Colab Notebooks/datasets/woman\"\n",
    "\n",
    "train_img_dir = \"/content/drive/My Drive/Colab Notebooks/datasets/woman/train\"\n",
    "\n",
    "test_img_dir = \"/content/drive/My Drive/Colab Notebooks/datasets/woman/test\"\n",
    "\n",
    "val_img_dir = \"/content/drive/My Drive/Colab Notebooks/datasets/woman/val\"\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "training_epochs = 20 # traindata개수/batchsize\n",
    "batch_size = 512\n",
    "validation_steps = 20 # valdata개수/batchsize\n",
    "\n",
    "\n",
    "\n",
    "print(f'batchSIZE is {batch_size}, Learning Rate is {learning_rate}')\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "val_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "categories = ['dog','cat','rabbit','squirrel','deer','fox','penguin']\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(train_img_dir, target_size=(128,128), \n",
    "                                             classes=categories, \n",
    "                                             batch_size=batch_size)\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(test_img_dir,\n",
    "                                        target_size=(128,128), \n",
    "                                        classes=categories, \n",
    "                                        batch_size=batch_size)\n",
    "\n",
    "val_set = test_datagen.flow_from_directory(val_img_dir,\n",
    "                                        target_size=(128,128), \n",
    "                                        classes=categories, \n",
    "                                        batch_size=batch_size)\n",
    "\n",
    "x_train, y_train = next(training_set)\n",
    "x_test, y_test = next(test_set)\n",
    "x_val,y_val = next(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "aDS8HPAA9CrJ"
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "    \n",
    "\n",
    "    conv2_1 = Conv2D(32, 3, 1, 'SAME')(inputs)\n",
    "    conv2_2 = Conv2D(32, 3, 1, 'SAME')(conv2_1)\n",
    "    pool2_3 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv2_2)\n",
    "    br1 = BatchNormalization()(pool2_3)\n",
    "    \n",
    "    conv2_1 = Conv2D(32, 3, 1, 'SAME')(br1)\n",
    "    conv2_2 = Conv2D(32, 3, 1, 'SAME')(conv2_1)\n",
    "    pool2_3 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv2_2)\n",
    "    br1 = BatchNormalization()(pool2_3)\n",
    "    \n",
    "    \n",
    "    conv3_1 = Conv2D(64, 3, 1, 'SAME')(br1)\n",
    "    conv3_2 = Conv2D(64, 3, 1, 'SAME')(conv3_1)\n",
    "    pool3_2 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv3_2)\n",
    "    br2 = BatchNormalization()(pool3_2)\n",
    "    \n",
    "    conv3_1 = Conv2D(64, 3, 1, 'SAME')(br2)\n",
    "    conv3_2 = Conv2D(64, 3, 1, 'SAME')(conv3_1)\n",
    "    pool3_2 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv3_2)\n",
    "    br2 = BatchNormalization()(pool3_2)\n",
    "    \n",
    "    \n",
    "    conv4_1 = Conv2D(128, 3, 1, 'SAME')(br2)\n",
    "    conv4_2 = Conv2D(128, 3, 1, 'SAME')(conv4_1)\n",
    "    pool4_2 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv4_2)\n",
    "    br3 = BatchNormalization()(pool4_2)\n",
    "    \n",
    "    conv4_1 = Conv2D(128, 3, 1, 'SAME')(br3)\n",
    "    conv4_2 = Conv2D(128, 3, 1, 'SAME')(conv4_1)\n",
    "    pool4_2 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv4_2)\n",
    "    br3 = BatchNormalization()(pool4_2)\n",
    "    \n",
    "    flatten1 = Flatten()(pool4_2)\n",
    "    dense1 = Dense(units = 512, activation = 'relu')(flatten1)\n",
    "    dense2 = Dense(units = 1024, activation = 'relu')(dense1)\n",
    "    dr1 = Dropout(0.7)(dense2)\n",
    "    dense3 = Dense(units = 5, activation = 'sigmoid')(dr1)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=dense3) \n",
    "def mobile_net():\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "    mobileNet = MobileNetV2(weights = \"imagenet\", include_top=False,input_shape = (128,128,3)\n",
    "                            ,input_tensor = inputs)\n",
    "    for layer in mobileNet.layers:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    output = mobileNet.output\n",
    "    pooling = AveragePooling2D(pool_size=(16,16),padding='SAME')(output)\n",
    "    flatten1 = Flatten()(pooling)\n",
    "    dense1 = Dense(units = 256)(flatten1)\n",
    "    batch1 = BatchNormalization()(dense1)\n",
    "    relu1 = ReLU()(batch1)\n",
    "    dense2 = Dense(units = 64)(relu1)\n",
    "    batch2 = BatchNormalization()(dense2)\n",
    "    relu2 = ReLU()(batch2)\n",
    "    dense3 = Dense(units = 6, activation = 'sigmoid')(relu2)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=dense3)\n",
    "\n",
    "def xception():\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "    xception = Xception(weights = \"imagenet\", include_top=False,input_shape = (128,128,3)\n",
    "                            ,input_tensor = inputs)\n",
    "    for layer in xception.layers:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    output = xception.output\n",
    "    pooling = AveragePooling2D(pool_size=(8,8),padding='SAME')(output)\n",
    "    flatten1 = Flatten()(pooling)\n",
    "    dense1 = Dense(units = 256)(flatten1)\n",
    "    batch1 = BatchNormalization()(dense1)\n",
    "    relu1 = ReLU()(batch1)\n",
    "    dense2 = Dense(units = 64)(relu1)\n",
    "    batch2 = BatchNormalization()(dense2)\n",
    "    relu2 = ReLU()(batch2)\n",
    "    dense3 = Dense(units = 6, activation = 'sigmoid')(relu2)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=dense3)\n",
    "    # pooling = AveragePooling2D(pool_size=(4,4),padding='SAME')(output)\n",
    "    \n",
    "    # flatten1 = Flatten()(pooling)\n",
    "    # dense1 = Dense(units = 512, activation = 'relu')(flatten1)\n",
    "    # dense2 = Dense(units = 1024, activation = 'relu')(dense1)\n",
    "    # dr1 = Dropout(0.7)(dense2)\n",
    "    # dense3 = Dense(units = 5, activation = 'sigmoid')(dr1)\n",
    "    \n",
    "    # return keras.Model(inputs=inputs, outputs=dense3)\n",
    "\n",
    "def resnet():\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "    resnet = ResNet50V2 (weights = \"imagenet\", include_top=False,input_shape = (128,128,3)\n",
    "                            ,input_tensor = inputs)\n",
    "    for layer in resnet.layers:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    output = resnet.output\n",
    "    pooling = AveragePooling2D(pool_size=(8,8),padding='SAME')(output)\n",
    "    flatten1 = Flatten()(pooling)\n",
    "    dense1 = Dense(units = 128)(flatten1)\n",
    "    batch1 = BatchNormalization()(dense1)\n",
    "    relu1 = ReLU()(batch1)\n",
    "    dense2 = Dense(units = 32)(relu1)\n",
    "    batch2 = BatchNormalization()(dense2)\n",
    "    relu2 = ReLU()(batch2)\n",
    "    dense3 = Dense(units = 7, activation = 'sigmoid')(relu2)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=dense3)\n",
    "\n",
    "def densenet():\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "    densenet = DenseNet121(weights = \"imagenet\", include_top=False,input_shape = (128,128,3)\n",
    "                            ,input_tensor = inputs)\n",
    "    for layer in densenet.layers:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    output = densenet.output\n",
    "    pooling = MaxPooling2D(pool_size=(4,4),padding='SAME')(output)\n",
    "    flatten1 = Flatten()(pooling)\n",
    "    dense1 = Dense(units = 1024)(flatten1)\n",
    "    batch1 = BatchNormalization()(dense1)\n",
    "    relu1 = ReLU()(batch1)\n",
    "    dense2 = Dense(units = 512)(relu1)\n",
    "    batch2 = BatchNormalization()(dense2)\n",
    "    relu2 = ReLU()(batch2)\n",
    "    dense3 = Dense(units = 7, activation = 'sigmoid')(relu2)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=dense3)\n",
    "\n",
    "def inception():\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "    inception = InceptionV3(weights = \"imagenet\", include_top=False,input_shape = (128,128,3)\n",
    "                            ,input_tensor = inputs)\n",
    "    for layer in inception.layers:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    output = inception.output\n",
    "    pooling = AveragePooling2D(pool_size=(32,32),padding='SAME')(output)\n",
    "    flatten1 = Flatten()(pooling)\n",
    "    dense1 = Dense(units = 96)(flatten1)\n",
    "    batch1 = BatchNormalization()(dense1)\n",
    "    relu1 = ReLU()(batch1)\n",
    "    dense2 = Dense(units = 24)(relu1)\n",
    "    batch2 = BatchNormalization()(dense2)\n",
    "    relu2 = ReLU()(batch2)\n",
    "    dense3 = Dense(units = 6, activation = 'sigmoid')(relu2)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=dense3)\n",
    "\n",
    "def inceptionresnet():\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "    inceptionresnet = InceptionResNetV2(weights = \"imagenet\", include_top=False,input_shape = (128,128,3)\n",
    "                            ,input_tensor = inputs)\n",
    "    for layer in inceptionresnet.layers:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    output = inceptionresnet.output\n",
    "    pooling = MaxPooling2D(pool_size=(4,4),padding='SAME')(output)\n",
    "    flatten1 = Flatten()(pooling)\n",
    "    dense1 = Dense(units = 1024)(flatten1)\n",
    "    batch1 = BatchNormalization()(dense1)\n",
    "    relu1 = ReLU()(batch1)\n",
    "    dense2 = Dense(units = 512)(relu1)\n",
    "    batch2 = BatchNormalization()(dense2)\n",
    "    relu2 = ReLU()(batch2)\n",
    "    dense3 = Dense(units = 7, activation = 'sigmoid')(relu2)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=dense3)\n",
    "\n",
    "def vgg():\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "    vgg = VGG19(weights = \"imagenet\", include_top=False,input_shape = (128,128,3)\n",
    "                            ,input_tensor = inputs)\n",
    "    for layer in vgg.layers:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    output = vgg.output\n",
    "    pooling = AveragePooling2D(pool_size=(32,32),padding='SAME')(output)\n",
    "    flatten1 = Flatten()(pooling)\n",
    "    dense1 = Dense(units = 96)(flatten1)\n",
    "    batch1 = BatchNormalization()(dense1)\n",
    "    relu1 = ReLU()(batch1)\n",
    "    dense2 = Dense(units = 24)(relu1)\n",
    "    batch2 = BatchNormalization()(dense2)\n",
    "    relu2 = ReLU()(batch2)\n",
    "    dense3 = Dense(units = 6, activation = 'sigmoid')(relu2)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=dense3)\n",
    "\n",
    "\n",
    "class ResidualUnit(tf.keras.Model):\n",
    "    def __init__(self, filter_in, filter_out, kernel_size):\n",
    "        super(ResidualUnit, self).__init__()\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filter_out, kernel_size, padding='same')\n",
    "        \n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filter_out, kernel_size, padding='same')\n",
    "        \n",
    "        if filter_in == filter_out:\n",
    "            self.identity = lambda x: x\n",
    "        else:\n",
    "            self.identity = tf.keras.layers.Conv2D(filter_out, (1,1), padding='same')\n",
    "\n",
    "    def call(self, x, training=False, mask=None):\n",
    "        h = self.bn1(x, training=training)\n",
    "        h = tf.nn.relu(h)\n",
    "        h = self.conv1(h)\n",
    "        \n",
    "        h = self.bn2(h, training=training)\n",
    "        h = tf.nn.relu(h)\n",
    "        h = self.conv2(h)\n",
    "        return self.identity(x) + h\n",
    "    \n",
    "class ResnetLayer(tf.keras.Model):\n",
    "    def __init__(self, filter_in, filters, kernel_size):\n",
    "        super(ResnetLayer, self).__init__()\n",
    "        self.sequence = list()\n",
    "        for f_in, f_out in zip([filter_in] + list(filters), filters):\n",
    "            self.sequence.append(ResidualUnit(f_in, f_out, kernel_size))\n",
    "\n",
    "    def call(self, x, training=False, mask=None):\n",
    "        for unit in self.sequence:\n",
    "            x = unit(x, training=training)\n",
    "        return x\n",
    "    \n",
    "class ResNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu') # 28x28x8\n",
    "        \n",
    "        self.res1 = ResnetLayer(64, (16, 16), (3, 3)) # 28x28x16\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D((2, 2)) # 14x14x16\n",
    "        \n",
    "        \n",
    "        self.res2 = ResnetLayer(128, (32, 32), (3, 3)) # 14x14x32\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D((2, 2)) # 7x7x32\n",
    "    \n",
    "        \n",
    "        self.res3 = ResnetLayer(256, (64, 64), (3, 3)) # 7x7x64\n",
    "        self.pool = tf.keras.layers.MaxPool2D((2, 2)) # 7x7x32\n",
    "        \n",
    "        \n",
    "        self.res4 = ResnetLayer(512, (64, 64), (3, 3)) # 7x7x64\n",
    "        self.pool = tf.keras.layers.MaxPool2D((2, 2)) # 7x7x32\n",
    "        \n",
    "        \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(1024, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(5, activation='softmax')\n",
    "        \n",
    "    def call(self, x, training=False, mask=None):\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        x = self.res1(x, training=training)\n",
    "        x = self.pool1(x)\n",
    "        x = self.res2(x, training=training)\n",
    "        x = self.pool2(x)\n",
    "        x = self.res3(x, training=training)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        return self.dense2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KVIEcU_U9C-h",
    "outputId": "f2f031d4-4889-4240-dcfa-e3e0a6a30649"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "20/20 [==============================] - 5s 240ms/step - loss: 0.4744 - accuracy: 0.3145 - precision: 0.3197 - recall: 0.2748 - f1score: 0.2721 - val_loss: 1.1075 - val_accuracy: 0.1364 - val_precision: 0.0250 - val_recall: 0.0029 - val_f1score: 0.0053\n",
      "Epoch 2/120\n",
      "20/20 [==============================] - 3s 161ms/step - loss: 0.2879 - accuracy: 0.5820 - precision: 0.6793 - recall: 0.4194 - f1score: 0.5128 - val_loss: 1.1510 - val_accuracy: 0.1576 - val_precision: 0.1749 - val_recall: 0.0912 - val_f1score: 0.1182\n",
      "Epoch 3/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 0.1961 - accuracy: 0.7285 - precision: 0.7877 - recall: 0.6622 - f1score: 0.7181 - val_loss: 1.2765 - val_accuracy: 0.2788 - val_precision: 0.4408 - val_recall: 0.0660 - val_f1score: 0.1124\n",
      "Epoch 4/120\n",
      "20/20 [==============================] - 3s 161ms/step - loss: 0.1046 - accuracy: 0.8594 - precision: 0.8981 - recall: 0.8216 - f1score: 0.8560 - val_loss: 1.3010 - val_accuracy: 0.1394 - val_precision: 0.1432 - val_recall: 0.1412 - val_f1score: 0.1421\n",
      "Epoch 5/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 0.0585 - accuracy: 0.9473 - precision: 0.9402 - recall: 0.9357 - f1score: 0.9374 - val_loss: 1.2073 - val_accuracy: 0.2424 - val_precision: 0.2661 - val_recall: 0.1866 - val_f1score: 0.2159\n",
      "Epoch 6/120\n",
      "20/20 [==============================] - 3s 161ms/step - loss: 0.0669 - accuracy: 0.9180 - precision: 0.9394 - recall: 0.8962 - f1score: 0.9163 - val_loss: 0.7531 - val_accuracy: 0.3394 - val_precision: 0.4341 - val_recall: 0.2744 - val_f1score: 0.3350\n",
      "Epoch 7/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 0.0551 - accuracy: 0.9336 - precision: 0.9273 - recall: 0.9250 - f1score: 0.9257 - val_loss: 0.6823 - val_accuracy: 0.4545 - val_precision: 0.5003 - val_recall: 0.4038 - val_f1score: 0.4448\n",
      "Epoch 8/120\n",
      "20/20 [==============================] - 3s 161ms/step - loss: 0.0582 - accuracy: 0.9336 - precision: 0.9294 - recall: 0.9197 - f1score: 0.9240 - val_loss: 1.5386 - val_accuracy: 0.2485 - val_precision: 0.2543 - val_recall: 0.2790 - val_f1score: 0.2652\n",
      "Epoch 9/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 0.0639 - accuracy: 0.9141 - precision: 0.9290 - recall: 0.8944 - f1score: 0.9108 - val_loss: 1.0012 - val_accuracy: 0.3939 - val_precision: 0.3924 - val_recall: 0.3584 - val_f1score: 0.3724\n",
      "Epoch 10/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 0.0587 - accuracy: 0.9258 - precision: 0.9303 - recall: 0.9165 - f1score: 0.9228 - val_loss: 0.7270 - val_accuracy: 0.4091 - val_precision: 0.4608 - val_recall: 0.3697 - val_f1score: 0.4084\n",
      "Epoch 11/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 0.0314 - accuracy: 0.9668 - precision: 0.9685 - recall: 0.9635 - f1score: 0.9656 - val_loss: 0.4465 - val_accuracy: 0.5485 - val_precision: 0.6101 - val_recall: 0.4744 - val_f1score: 0.5304\n",
      "Epoch 12/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 0.0116 - accuracy: 0.9941 - precision: 0.9880 - recall: 0.9962 - f1score: 0.9920 - val_loss: 0.2374 - val_accuracy: 0.6909 - val_precision: 0.7601 - val_recall: 0.6311 - val_f1score: 0.6878\n",
      "Epoch 13/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 0.0086 - accuracy: 0.9941 - precision: 0.9927 - recall: 0.9962 - f1score: 0.9943 - val_loss: 0.2704 - val_accuracy: 0.6848 - val_precision: 0.7293 - val_recall: 0.6210 - val_f1score: 0.6686\n",
      "Epoch 14/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 0.0066 - accuracy: 0.9961 - precision: 0.9962 - recall: 0.9981 - f1score: 0.9971 - val_loss: 0.2250 - val_accuracy: 0.7303 - val_precision: 0.7641 - val_recall: 0.6592 - val_f1score: 0.7057\n",
      "Epoch 15/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 0.0040 - accuracy: 1.0000 - precision: 0.9981 - recall: 0.9981 - f1score: 0.9981 - val_loss: 0.2008 - val_accuracy: 0.7485 - val_precision: 0.7887 - val_recall: 0.6958 - val_f1score: 0.7367\n",
      "Epoch 16/120\n",
      "20/20 [==============================] - 3s 163ms/step - loss: 0.0042 - accuracy: 1.0000 - precision: 1.0000 - recall: 0.9981 - f1score: 0.9990 - val_loss: 0.2099 - val_accuracy: 0.7182 - val_precision: 0.7668 - val_recall: 0.7034 - val_f1score: 0.7315\n",
      "Epoch 17/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 0.0031 - accuracy: 0.9980 - precision: 0.9962 - recall: 0.9981 - f1score: 0.9971 - val_loss: 0.2018 - val_accuracy: 0.7455 - val_precision: 0.7797 - val_recall: 0.7412 - val_f1score: 0.7589\n",
      "Epoch 18/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 0.0021 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1848 - val_accuracy: 0.7727 - val_precision: 0.8017 - val_recall: 0.7429 - val_f1score: 0.7694\n",
      "Epoch 19/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 0.0028 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1806 - val_accuracy: 0.7788 - val_precision: 0.8130 - val_recall: 0.7340 - val_f1score: 0.7697\n",
      "Epoch 20/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 0.0020 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1779 - val_accuracy: 0.7727 - val_precision: 0.8160 - val_recall: 0.7399 - val_f1score: 0.7746\n",
      "Epoch 21/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 0.0018 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1745 - val_accuracy: 0.7727 - val_precision: 0.8132 - val_recall: 0.7370 - val_f1score: 0.7719\n",
      "Epoch 22/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 0.0021 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1672 - val_accuracy: 0.7758 - val_precision: 0.8127 - val_recall: 0.7370 - val_f1score: 0.7719\n",
      "Epoch 23/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 0.0023 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1599 - val_accuracy: 0.7939 - val_precision: 0.8326 - val_recall: 0.7634 - val_f1score: 0.7956\n",
      "Epoch 24/120\n",
      "20/20 [==============================] - 3s 161ms/step - loss: 0.0013 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1525 - val_accuracy: 0.8091 - val_precision: 0.8341 - val_recall: 0.7664 - val_f1score: 0.7979\n",
      "Epoch 25/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 0.0019 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1513 - val_accuracy: 0.8000 - val_precision: 0.8347 - val_recall: 0.7664 - val_f1score: 0.7983\n",
      "Epoch 26/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 0.0020 - accuracy: 1.0000 - precision: 0.9981 - recall: 1.0000 - f1score: 0.9991 - val_loss: 0.1729 - val_accuracy: 0.7788 - val_precision: 0.8228 - val_recall: 0.7546 - val_f1score: 0.7859\n",
      "Epoch 27/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 0.0015 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1660 - val_accuracy: 0.7970 - val_precision: 0.8244 - val_recall: 0.7605 - val_f1score: 0.7899\n",
      "Epoch 28/120\n",
      "20/20 [==============================] - 3s 163ms/step - loss: 0.0012 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1577 - val_accuracy: 0.8091 - val_precision: 0.8393 - val_recall: 0.7693 - val_f1score: 0.8018\n",
      "Epoch 29/120\n",
      "20/20 [==============================] - 3s 163ms/step - loss: 0.0010 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1547 - val_accuracy: 0.8061 - val_precision: 0.8399 - val_recall: 0.7782 - val_f1score: 0.8068\n",
      "Epoch 30/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 0.0015 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1530 - val_accuracy: 0.8030 - val_precision: 0.8430 - val_recall: 0.7840 - val_f1score: 0.8115\n",
      "Epoch 31/120\n",
      "20/20 [==============================] - 3s 161ms/step - loss: 0.0012 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1563 - val_accuracy: 0.8000 - val_precision: 0.8416 - val_recall: 0.7693 - val_f1score: 0.8030\n",
      "Epoch 32/120\n",
      "20/20 [==============================] - 3s 161ms/step - loss: 0.0012 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1546 - val_accuracy: 0.8030 - val_precision: 0.8341 - val_recall: 0.7693 - val_f1score: 0.7996\n",
      "Epoch 33/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 0.0016 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1545 - val_accuracy: 0.8030 - val_precision: 0.8306 - val_recall: 0.7723 - val_f1score: 0.7997\n",
      "Epoch 34/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 0.0012 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1538 - val_accuracy: 0.8121 - val_precision: 0.8304 - val_recall: 0.7723 - val_f1score: 0.7997\n",
      "Epoch 35/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 0.0010 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1575 - val_accuracy: 0.8030 - val_precision: 0.8308 - val_recall: 0.7517 - val_f1score: 0.7884\n",
      "Epoch 36/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 0.0011 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1568 - val_accuracy: 0.7970 - val_precision: 0.8288 - val_recall: 0.7576 - val_f1score: 0.7908\n",
      "Epoch 37/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 0.0013 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1519 - val_accuracy: 0.8121 - val_precision: 0.8398 - val_recall: 0.7723 - val_f1score: 0.8041\n",
      "Epoch 38/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 0.0011 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1512 - val_accuracy: 0.8091 - val_precision: 0.8397 - val_recall: 0.7782 - val_f1score: 0.8071\n",
      "Epoch 39/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 0.0015 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1509 - val_accuracy: 0.8061 - val_precision: 0.8435 - val_recall: 0.7811 - val_f1score: 0.8104\n",
      "Epoch 40/120\n",
      "20/20 [==============================] - 3s 161ms/step - loss: 8.7022e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1518 - val_accuracy: 0.8030 - val_precision: 0.8442 - val_recall: 0.7840 - val_f1score: 0.8120\n",
      "Epoch 41/120\n",
      "20/20 [==============================] - 3s 163ms/step - loss: 9.0958e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1515 - val_accuracy: 0.8061 - val_precision: 0.8467 - val_recall: 0.7811 - val_f1score: 0.8117\n",
      "Epoch 42/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 9.1363e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1509 - val_accuracy: 0.8091 - val_precision: 0.8467 - val_recall: 0.7811 - val_f1score: 0.8117\n",
      "Epoch 43/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 8.1819e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1507 - val_accuracy: 0.8121 - val_precision: 0.8444 - val_recall: 0.7840 - val_f1score: 0.8122\n",
      "Epoch 44/120\n",
      "20/20 [==============================] - 3s 163ms/step - loss: 8.5418e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1510 - val_accuracy: 0.8121 - val_precision: 0.8445 - val_recall: 0.7811 - val_f1score: 0.8107\n",
      "Epoch 45/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 9.2579e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1509 - val_accuracy: 0.8091 - val_precision: 0.8445 - val_recall: 0.7811 - val_f1score: 0.8107\n",
      "Epoch 46/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 8.8624e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1512 - val_accuracy: 0.8091 - val_precision: 0.8409 - val_recall: 0.7782 - val_f1score: 0.8075\n",
      "Epoch 47/120\n",
      "20/20 [==============================] - 3s 163ms/step - loss: 8.0374e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1508 - val_accuracy: 0.8091 - val_precision: 0.8409 - val_recall: 0.7782 - val_f1score: 0.8076\n",
      "Epoch 48/120\n",
      "20/20 [==============================] - 3s 164ms/step - loss: 0.0011 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1506 - val_accuracy: 0.8121 - val_precision: 0.8417 - val_recall: 0.7840 - val_f1score: 0.8112\n",
      "Epoch 49/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 8.3623e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1505 - val_accuracy: 0.8061 - val_precision: 0.8446 - val_recall: 0.7840 - val_f1score: 0.8127\n",
      "Epoch 50/120\n",
      "20/20 [==============================] - 3s 163ms/step - loss: 8.1034e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1504 - val_accuracy: 0.8061 - val_precision: 0.8446 - val_recall: 0.7840 - val_f1score: 0.8127\n",
      "Epoch 51/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 7.9648e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1506 - val_accuracy: 0.8091 - val_precision: 0.8425 - val_recall: 0.7840 - val_f1score: 0.8118\n",
      "Epoch 52/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 7.3957e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1499 - val_accuracy: 0.8121 - val_precision: 0.8446 - val_recall: 0.7840 - val_f1score: 0.8127\n",
      "Epoch 53/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 7.1844e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1497 - val_accuracy: 0.8121 - val_precision: 0.8446 - val_recall: 0.7840 - val_f1score: 0.8127\n",
      "Epoch 54/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 7.1409e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1493 - val_accuracy: 0.8152 - val_precision: 0.8522 - val_recall: 0.7840 - val_f1score: 0.8159\n",
      "Epoch 55/120\n",
      "20/20 [==============================] - 3s 163ms/step - loss: 8.3473e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1493 - val_accuracy: 0.8182 - val_precision: 0.8497 - val_recall: 0.7840 - val_f1score: 0.8149\n",
      "Epoch 56/120\n",
      "20/20 [==============================] - 3s 163ms/step - loss: 0.0012 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1487 - val_accuracy: 0.8152 - val_precision: 0.8548 - val_recall: 0.7840 - val_f1score: 0.8171\n",
      "Epoch 57/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 9.3250e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1487 - val_accuracy: 0.8152 - val_precision: 0.8548 - val_recall: 0.7840 - val_f1score: 0.8171\n",
      "Epoch 58/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 7.6564e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1485 - val_accuracy: 0.8152 - val_precision: 0.8497 - val_recall: 0.7840 - val_f1score: 0.8147\n",
      "Epoch 59/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 8.5193e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1486 - val_accuracy: 0.8152 - val_precision: 0.8522 - val_recall: 0.7840 - val_f1score: 0.8159\n",
      "Epoch 60/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 7.4876e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1488 - val_accuracy: 0.8152 - val_precision: 0.8521 - val_recall: 0.7840 - val_f1score: 0.8160\n",
      "Epoch 61/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 8.3901e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1492 - val_accuracy: 0.8152 - val_precision: 0.8499 - val_recall: 0.7840 - val_f1score: 0.8148\n",
      "Epoch 62/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 6.4205e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1491 - val_accuracy: 0.8152 - val_precision: 0.8499 - val_recall: 0.7840 - val_f1score: 0.8148\n",
      "Epoch 63/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 7.2151e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1493 - val_accuracy: 0.8152 - val_precision: 0.8473 - val_recall: 0.7840 - val_f1score: 0.8137\n",
      "Epoch 64/120\n",
      "20/20 [==============================] - 3s 163ms/step - loss: 7.2760e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1495 - val_accuracy: 0.8152 - val_precision: 0.8473 - val_recall: 0.7840 - val_f1score: 0.8137\n",
      "Epoch 65/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 8.6652e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1495 - val_accuracy: 0.8152 - val_precision: 0.8499 - val_recall: 0.7840 - val_f1score: 0.8148\n",
      "Epoch 66/120\n",
      "20/20 [==============================] - 3s 163ms/step - loss: 7.8012e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1494 - val_accuracy: 0.8152 - val_precision: 0.8499 - val_recall: 0.7840 - val_f1score: 0.8148\n",
      "Epoch 67/120\n",
      "20/20 [==============================] - 3s 163ms/step - loss: 8.0750e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1492 - val_accuracy: 0.8152 - val_precision: 0.8499 - val_recall: 0.7840 - val_f1score: 0.8148\n",
      "Epoch 68/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 9.0071e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1492 - val_accuracy: 0.8182 - val_precision: 0.8473 - val_recall: 0.7840 - val_f1score: 0.8137\n",
      "Epoch 69/120\n",
      "20/20 [==============================] - 3s 163ms/step - loss: 8.0332e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1492 - val_accuracy: 0.8182 - val_precision: 0.8468 - val_recall: 0.7840 - val_f1score: 0.8134\n",
      "Epoch 70/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 7.6737e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1492 - val_accuracy: 0.8182 - val_precision: 0.8468 - val_recall: 0.7840 - val_f1score: 0.8134\n",
      "Epoch 71/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 7.7823e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1493 - val_accuracy: 0.8121 - val_precision: 0.8496 - val_recall: 0.7840 - val_f1score: 0.8148\n",
      "Epoch 72/120\n",
      "20/20 [==============================] - 3s 163ms/step - loss: 8.5609e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1491 - val_accuracy: 0.8121 - val_precision: 0.8468 - val_recall: 0.7840 - val_f1score: 0.8134\n",
      "Epoch 73/120\n",
      "20/20 [==============================] - 3s 164ms/step - loss: 7.1782e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1493 - val_accuracy: 0.8182 - val_precision: 0.8468 - val_recall: 0.7840 - val_f1score: 0.8134\n",
      "Epoch 74/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 7.5772e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1494 - val_accuracy: 0.8121 - val_precision: 0.8445 - val_recall: 0.7840 - val_f1score: 0.8124\n",
      "Epoch 75/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 0.0011 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1494 - val_accuracy: 0.8152 - val_precision: 0.8468 - val_recall: 0.7840 - val_f1score: 0.8134\n",
      "Epoch 76/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 8.9074e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1492 - val_accuracy: 0.8152 - val_precision: 0.8468 - val_recall: 0.7840 - val_f1score: 0.8134\n",
      "Epoch 77/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 7.0677e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1490 - val_accuracy: 0.8121 - val_precision: 0.8468 - val_recall: 0.7840 - val_f1score: 0.8134\n",
      "Epoch 78/120\n",
      "20/20 [==============================] - 3s 163ms/step - loss: 9.0057e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1492 - val_accuracy: 0.8121 - val_precision: 0.8445 - val_recall: 0.7840 - val_f1score: 0.8124\n",
      "Epoch 79/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 9.4960e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1491 - val_accuracy: 0.8121 - val_precision: 0.8443 - val_recall: 0.7840 - val_f1score: 0.8123\n",
      "Epoch 80/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 7.7131e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1489 - val_accuracy: 0.8121 - val_precision: 0.8443 - val_recall: 0.7840 - val_f1score: 0.8123\n",
      "Epoch 81/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 9.7503e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1491 - val_accuracy: 0.8121 - val_precision: 0.8468 - val_recall: 0.7840 - val_f1score: 0.8134\n",
      "Epoch 82/120\n",
      "20/20 [==============================] - 3s 163ms/step - loss: 0.0012 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1493 - val_accuracy: 0.8121 - val_precision: 0.8443 - val_recall: 0.7840 - val_f1score: 0.8123\n",
      "Epoch 83/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 7.7882e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1492 - val_accuracy: 0.8121 - val_precision: 0.8448 - val_recall: 0.7840 - val_f1score: 0.8126\n",
      "Epoch 84/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 9.4322e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1492 - val_accuracy: 0.8121 - val_precision: 0.8446 - val_recall: 0.7840 - val_f1score: 0.8127\n",
      "Epoch 85/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 8.1588e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1492 - val_accuracy: 0.8152 - val_precision: 0.8446 - val_recall: 0.7840 - val_f1score: 0.8127\n",
      "Epoch 86/120\n",
      "20/20 [==============================] - 3s 163ms/step - loss: 8.1031e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1491 - val_accuracy: 0.8121 - val_precision: 0.8496 - val_recall: 0.7840 - val_f1score: 0.8148\n",
      "Epoch 87/120\n",
      "20/20 [==============================] - 3s 163ms/step - loss: 7.9492e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1489 - val_accuracy: 0.8121 - val_precision: 0.8496 - val_recall: 0.7840 - val_f1score: 0.8148\n",
      "Epoch 88/120\n",
      "20/20 [==============================] - 3s 163ms/step - loss: 9.2805e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1490 - val_accuracy: 0.8121 - val_precision: 0.8468 - val_recall: 0.7840 - val_f1score: 0.8134\n",
      "Epoch 89/120\n",
      "20/20 [==============================] - 3s 163ms/step - loss: 7.6387e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1491 - val_accuracy: 0.8121 - val_precision: 0.8468 - val_recall: 0.7840 - val_f1score: 0.8134\n",
      "Epoch 90/120\n",
      "20/20 [==============================] - 3s 163ms/step - loss: 9.5588e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1490 - val_accuracy: 0.8152 - val_precision: 0.8468 - val_recall: 0.7840 - val_f1score: 0.8134\n",
      "Epoch 91/120\n",
      "20/20 [==============================] - 3s 164ms/step - loss: 0.0012 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1491 - val_accuracy: 0.8121 - val_precision: 0.8494 - val_recall: 0.7840 - val_f1score: 0.8145\n",
      "Epoch 92/120\n",
      "20/20 [==============================] - 3s 163ms/step - loss: 7.4256e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1491 - val_accuracy: 0.8121 - val_precision: 0.8468 - val_recall: 0.7840 - val_f1score: 0.8134\n",
      "Epoch 93/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 9.7799e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1491 - val_accuracy: 0.8121 - val_precision: 0.8468 - val_recall: 0.7840 - val_f1score: 0.8134\n",
      "Epoch 94/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 9.0199e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1493 - val_accuracy: 0.8152 - val_precision: 0.8443 - val_recall: 0.7840 - val_f1score: 0.8123\n",
      "Epoch 95/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 8.0792e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1493 - val_accuracy: 0.8152 - val_precision: 0.8469 - val_recall: 0.7840 - val_f1score: 0.8134\n",
      "Epoch 96/120\n",
      "20/20 [==============================] - 3s 163ms/step - loss: 0.0014 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1496 - val_accuracy: 0.8152 - val_precision: 0.8443 - val_recall: 0.7840 - val_f1score: 0.8123\n",
      "Epoch 97/120\n",
      "20/20 [==============================] - 3s 163ms/step - loss: 8.6695e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1493 - val_accuracy: 0.8121 - val_precision: 0.8443 - val_recall: 0.7840 - val_f1score: 0.8123\n",
      "Epoch 98/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 8.9163e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1493 - val_accuracy: 0.8121 - val_precision: 0.8443 - val_recall: 0.7840 - val_f1score: 0.8123\n",
      "Epoch 99/120\n",
      "20/20 [==============================] - 3s 163ms/step - loss: 6.5553e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1492 - val_accuracy: 0.8121 - val_precision: 0.8443 - val_recall: 0.7840 - val_f1score: 0.8123\n",
      "Epoch 100/120\n",
      "20/20 [==============================] - 3s 163ms/step - loss: 6.8503e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1493 - val_accuracy: 0.8121 - val_precision: 0.8443 - val_recall: 0.7840 - val_f1score: 0.8123\n",
      "Epoch 101/120\n",
      "20/20 [==============================] - 3s 163ms/step - loss: 8.8351e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1494 - val_accuracy: 0.8121 - val_precision: 0.8443 - val_recall: 0.7840 - val_f1score: 0.8123\n",
      "Epoch 102/120\n",
      "20/20 [==============================] - 3s 163ms/step - loss: 8.2627e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1494 - val_accuracy: 0.8121 - val_precision: 0.8443 - val_recall: 0.7840 - val_f1score: 0.8123\n",
      "Epoch 103/120\n",
      "20/20 [==============================] - 3s 163ms/step - loss: 7.2603e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1494 - val_accuracy: 0.8121 - val_precision: 0.8443 - val_recall: 0.7840 - val_f1score: 0.8123\n",
      "Epoch 104/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 7.2328e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1493 - val_accuracy: 0.8121 - val_precision: 0.8468 - val_recall: 0.7840 - val_f1score: 0.8134\n",
      "Epoch 105/120\n",
      "20/20 [==============================] - 3s 163ms/step - loss: 8.7529e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1492 - val_accuracy: 0.8121 - val_precision: 0.8468 - val_recall: 0.7840 - val_f1score: 0.8134\n",
      "Epoch 106/120\n",
      "20/20 [==============================] - 3s 163ms/step - loss: 7.9442e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1490 - val_accuracy: 0.8121 - val_precision: 0.8468 - val_recall: 0.7840 - val_f1score: 0.8134\n",
      "Epoch 107/120\n",
      "20/20 [==============================] - 3s 163ms/step - loss: 7.5291e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1492 - val_accuracy: 0.8121 - val_precision: 0.8468 - val_recall: 0.7840 - val_f1score: 0.8134\n",
      "Epoch 108/120\n",
      "20/20 [==============================] - 3s 163ms/step - loss: 6.8831e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1492 - val_accuracy: 0.8121 - val_precision: 0.8468 - val_recall: 0.7840 - val_f1score: 0.8134\n",
      "Epoch 109/120\n",
      "20/20 [==============================] - 3s 164ms/step - loss: 8.3034e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1492 - val_accuracy: 0.8121 - val_precision: 0.8468 - val_recall: 0.7840 - val_f1score: 0.8134\n",
      "Epoch 110/120\n",
      "20/20 [==============================] - 3s 163ms/step - loss: 8.8021e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1495 - val_accuracy: 0.8121 - val_precision: 0.8397 - val_recall: 0.7840 - val_f1score: 0.8102\n",
      "Epoch 111/120\n",
      "20/20 [==============================] - 3s 163ms/step - loss: 0.0012 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1495 - val_accuracy: 0.8121 - val_precision: 0.8443 - val_recall: 0.7840 - val_f1score: 0.8123\n",
      "Epoch 112/120\n",
      "20/20 [==============================] - 3s 163ms/step - loss: 7.1970e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1495 - val_accuracy: 0.8152 - val_precision: 0.8418 - val_recall: 0.7840 - val_f1score: 0.8113\n",
      "Epoch 113/120\n",
      "20/20 [==============================] - 3s 163ms/step - loss: 6.6449e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1496 - val_accuracy: 0.8152 - val_precision: 0.8418 - val_recall: 0.7840 - val_f1score: 0.8113\n",
      "Epoch 114/120\n",
      "20/20 [==============================] - 3s 163ms/step - loss: 7.9179e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1495 - val_accuracy: 0.8152 - val_precision: 0.8418 - val_recall: 0.7840 - val_f1score: 0.8113\n",
      "Epoch 115/120\n",
      "20/20 [==============================] - 3s 163ms/step - loss: 9.5619e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1495 - val_accuracy: 0.8152 - val_precision: 0.8443 - val_recall: 0.7840 - val_f1score: 0.8123\n",
      "Epoch 116/120\n",
      "20/20 [==============================] - 3s 163ms/step - loss: 8.4275e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1493 - val_accuracy: 0.8121 - val_precision: 0.8468 - val_recall: 0.7840 - val_f1score: 0.8134\n",
      "Epoch 117/120\n",
      "20/20 [==============================] - 3s 164ms/step - loss: 8.5851e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1493 - val_accuracy: 0.8121 - val_precision: 0.8496 - val_recall: 0.7840 - val_f1score: 0.8148\n",
      "Epoch 118/120\n",
      "20/20 [==============================] - 3s 162ms/step - loss: 8.4331e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1494 - val_accuracy: 0.8121 - val_precision: 0.8450 - val_recall: 0.7840 - val_f1score: 0.8127\n",
      "Epoch 119/120\n",
      "20/20 [==============================] - 3s 163ms/step - loss: 8.1499e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1493 - val_accuracy: 0.8121 - val_precision: 0.8496 - val_recall: 0.7840 - val_f1score: 0.8148\n",
      "Epoch 120/120\n",
      "20/20 [==============================] - 3s 163ms/step - loss: 0.0012 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1493 - val_accuracy: 0.8121 - val_precision: 0.8468 - val_recall: 0.7840 - val_f1score: 0.8134\n"
     ]
    }
   ],
   "source": [
    "def recall(y_target, y_pred):\n",
    "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
    "    # round : 반올림한다\n",
    "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "\n",
    "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
    "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
    "\n",
    "    # (True Positive + False Negative) = 실제 값이 1(Positive) 전체\n",
    "    count_true_positive_false_negative = K.sum(y_target_yn)\n",
    "\n",
    "    # Recall =  (True Positive) / (True Positive + False Negative)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    recall = count_true_positive / (count_true_positive_false_negative + K.epsilon())\n",
    "\n",
    "    # return a single tensor value\n",
    "    return recall\n",
    "\n",
    "\n",
    "def precision(y_target, y_pred):\n",
    "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
    "    # round : 반올림한다\n",
    "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "\n",
    "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
    "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
    "\n",
    "    # (True Positive + False Positive) = 예측 값이 1(Positive) 전체\n",
    "    count_true_positive_false_positive = K.sum(y_pred_yn)\n",
    "\n",
    "    # Precision = (True Positive) / (True Positive + False Positive)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    precision = count_true_positive / (count_true_positive_false_positive + K.epsilon())\n",
    "\n",
    "    # return a single tensor value\n",
    "    return precision\n",
    "\n",
    "\n",
    "def f1score(y_target, y_pred):\n",
    "    _recall = recall(y_target, y_pred)\n",
    "    _precision = precision(y_target, y_pred)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    _f1score = ( 2 * _recall * _precision) / (_recall + _precision+ K.epsilon())\n",
    "    \n",
    "    # return a single tensor value\n",
    "    return _f1score\n",
    "\n",
    "\n",
    "\n",
    "#model = create_model()\n",
    "#model = ResNet()\n",
    "#model = mobile_net()\n",
    "#model = xception()\n",
    "model = densenet()\n",
    "#model = resnet()\n",
    "#model = inception()\n",
    "#model =inceptionresnet()\n",
    "#model = vgg()\n",
    "\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=learning_rate,\n",
    "                                                          decay_steps=training_epochs * 10,\n",
    "                                                          decay_rate=0.5,\n",
    "                                                          staircase=True)\n",
    "\n",
    "\n",
    "\n",
    "# lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "#     initial_learning_rate=learning_rate,\n",
    "#     decay_steps=100000,\n",
    "#     decay_rate=0.96)\n",
    "\n",
    "# optimizer는 Adam, loss는 sparse categorical crossentropy 사용\n",
    "# label이 ont-hot으로 encoding 안 된 경우에 sparse categorical corssentropy 및 sparse categorical accuracy 사용\n",
    "model.compile(keras.optimizers.Adam(lr_schedule), loss = 'binary_crossentropy', metrics=['accuracy', precision, recall, f1score])\n",
    "\n",
    "# model.compile(\n",
    "#     optimizer='adam',\n",
    "#     loss='binary_crossentropy', \n",
    "#     metrics=['accuracy', precision, recall, f1score],\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Train!\n",
    "history = model.fit(x_train, y_train, steps_per_epoch=training_epochs,\n",
    "         epochs=120, validation_data = (x_val,y_val),validation_steps=validation_steps)\n",
    "model.save('animal_model_woman.h5')\n",
    "# epochs = 30\n",
    "# history = model.fit(\n",
    "#     training_set, \n",
    "#     epochs=epochs,\n",
    "#     steps_per_epoch=training_set.samples / epochs, \n",
    "#     validation_data=val_set,\n",
    "#     validation_steps=val_set.samples / epochs,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lkq1yTnE9CYr",
    "outputId": "d63e2184-0732-44fd-b87d-59c497461220"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116\n",
      "index: 0  actual y: 3  answer y: 3  prediction: [ 0.09520325  0.0024072   0.06906573 99.94711     0.00543547  0.02425941\n",
      "  0.01647186]\n",
      "index: 1  actual y: 5  answer y: 5  prediction: [ 0.03675899  0.26048112  0.02549109  0.02437138  0.23319167 99.46838\n",
      "  0.28905523]\n",
      "index: 2  actual y: 3  answer y: 5  prediction: [ 1.3502817   2.8748057   0.676362    9.32858     5.324941   23.119915\n",
      "  0.66591275]\n",
      "index: 3  actual y: 2  answer y: 2  prediction: [ 0.04556559  0.0452875  98.70219     0.26319394  1.0858666   0.672619\n",
      "  0.11347764]\n",
      "index: 4  actual y: 2  answer y: 2  prediction: [ 0.0169237   0.02968504 98.401535    1.1517233   1.434336    0.15458108\n",
      "  0.08638626]\n",
      "index: 5  actual y: 0  answer y: 0  prediction: [99.80103     0.03958112  0.06613318  0.15463534  0.06853865  0.18224065\n",
      "  0.07225082]\n",
      "index: 6  actual y: 4  answer y: 4  prediction: [ 0.04326061  0.24789882  0.05161195  0.03258335 99.603584    0.29929\n",
      "  0.65609443]\n",
      "index: 7  actual y: 2  answer y: 2  prediction: [ 0.12548734  8.879133   76.9621      0.12546685  5.6741033   0.3357275\n",
      "  0.17768963]\n",
      "index: 8  actual y: 4  answer y: 4  prediction: [ 1.2190899   0.26359317  0.85372937  0.86396563 95.76998     0.03287193\n",
      "  0.09527372]\n",
      "index: 9  actual y: 6  answer y: 0  prediction: [60.929848    0.01957089 18.401337    0.20740417  0.02673367  0.07919061\n",
      "  8.355776  ]\n",
      "index: 10  actual y: 2  answer y: 2  prediction: [ 0.03515482  1.2478971  91.34035    10.814615    3.2718453   0.00991752\n",
      "  0.02646804]\n",
      "index: 11  actual y: 0  answer y: 0  prediction: [99.66443     0.0534923   0.02074684  0.83587503  0.02419367  0.03887124\n",
      "  0.12453824]\n",
      "index: 12  actual y: 6  answer y: 6  prediction: [20.04641     0.15357433  1.1063554   0.13453175  0.75021404 13.282855\n",
      " 35.806652  ]\n",
      "index: 13  actual y: 5  answer y: 5  prediction: [ 0.09562142  0.03124353  1.7677364   0.0836327   0.02262462 99.57906\n",
      "  0.04173547]\n",
      "index: 14  actual y: 4  answer y: 4  prediction: [ 0.09224908  0.08784152  0.3615525   3.2368302  76.883736    0.41550323\n",
      " 12.896964  ]\n",
      "index: 15  actual y: 4  answer y: 3  prediction: [ 0.2452859   0.13296309  0.08290115 61.273342    1.5872548   3.3791964\n",
      " 13.445091  ]\n",
      "index: 16  actual y: 0  answer y: 4  prediction: [ 0.42543182  0.12181521  0.3818575   1.6519476  77.31488     2.379686\n",
      "  4.8205485 ]\n",
      "index: 17  actual y: 4  answer y: 4  prediction: [ 0.00200511  0.00769891  0.00744946  0.00663669 99.98726     0.0024637\n",
      "  0.00514706]\n",
      "index: 18  actual y: 1  answer y: 1  prediction: [ 0.04266229 99.717674    0.10501587  0.01556715  0.0146577   0.03441476\n",
      "  0.631593  ]\n",
      "index: 19  actual y: 4  answer y: 4  prediction: [ 0.04896382  0.01785101  0.0882078   0.03610897 99.84521     0.02218614\n",
      "  0.03582477]\n",
      "index: 20  actual y: 3  answer y: 3  prediction: [13.324483    0.20808697  0.05765524 90.2477      0.1386036   1.2683644\n",
      "  0.05898156]\n",
      "index: 21  actual y: 4  answer y: 4  prediction: [ 0.11977741  0.11475076  0.11698463  1.8117368  97.38648     0.9157795\n",
      "  0.31274372]\n",
      "index: 22  actual y: 3  answer y: 3  prediction: [ 0.0329494   0.03251691  0.01723939 99.469086    0.05217954  0.41449618\n",
      "  0.5131744 ]\n",
      "index: 23  actual y: 3  answer y: 3  prediction: [ 0.02180432  0.00255531  0.01689831 99.966515    0.00703619  0.04048468\n",
      "  0.00785596]\n",
      "index: 24  actual y: 6  answer y: 6  prediction: [ 0.41717458  0.09922179  0.14377238 30.56809     0.2648303   3.6582127\n",
      " 59.55109   ]\n",
      "index: 25  actual y: 0  answer y: 0  prediction: [92.81435     0.02166965 16.03618     0.27164122  0.05615642  0.5286825\n",
      "  0.05973453]\n",
      "index: 26  actual y: 3  answer y: 3  prediction: [ 0.58145875  0.07966692  0.07644752 81.42776     0.7518046   0.05303372\n",
      " 22.151985  ]\n",
      "index: 27  actual y: 5  answer y: 5  prediction: [ 0.07276828  0.00760026  0.03450045  0.05828823  0.46303445 99.80725\n",
      "  0.04711798]\n",
      "index: 28  actual y: 5  answer y: 6  prediction: [ 0.1735004   4.446382    0.13732806  3.9463003   1.9606715  16.521513\n",
      " 36.068768  ]\n",
      "index: 29  actual y: 0  answer y: 0  prediction: [99.94728     0.05653708  0.02474031  0.02612521  0.07573561  0.04777213\n",
      "  0.10051367]\n",
      "index: 30  actual y: 1  answer y: 1  prediction: [ 0.01243262 99.99132     0.001486    0.00620783  0.00318757  0.01558044\n",
      "  0.00223402]\n",
      "index: 31  actual y: 5  answer y: 5  prediction: [ 0.23090325  0.07419405  0.09887189  0.06883854  0.17784736 99.64155\n",
      "  0.14612302]\n",
      "index: 32  actual y: 0  answer y: 0  prediction: [99.81642     0.14588611  0.06915271  0.15387557  0.08832764  0.12042135\n",
      "  0.13928163]\n",
      "index: 33  actual y: 4  answer y: 4  prediction: [ 0.15562537  0.06237066  0.04204786  3.1746824  97.93242     0.7912212\n",
      "  0.11650444]\n",
      "index: 34  actual y: 3  answer y: 3  prediction: [ 0.06166817  0.21327598  0.22607678 98.88271     0.24647522  0.1438434\n",
      "  0.183273  ]\n",
      "index: 35  actual y: 5  answer y: 5  prediction: [ 0.20573284  0.10396459  0.1856816   0.04545334  0.10489664 99.685524\n",
      "  0.18090181]\n",
      "index: 36  actual y: 0  answer y: 0  prediction: [99.959496    0.05023436  0.01868887  0.01130431  0.0242155   0.00900371\n",
      "  0.03813232]\n",
      "index: 37  actual y: 0  answer y: 0  prediction: [99.97079     0.01190302  0.02281201  0.0055137   0.02932311  0.0183369\n",
      "  0.01089551]\n",
      "index: 38  actual y: 0  answer y: 0  prediction: [99.95022     0.02184452  0.00817512  0.02649023  0.05565979  0.02376368\n",
      "  0.02908572]\n",
      "index: 39  actual y: 0  answer y: 0  prediction: [89.97351     0.1838778   0.8653508   5.447065    0.40394548  0.04836439\n",
      "  0.0998676 ]\n",
      "index: 40  actual y: 5  answer y: 5  prediction: [ 0.21683201  0.06566887  0.09819374  0.5986459   0.29718193 98.37825\n",
      "  1.569346  ]\n",
      "index: 41  actual y: 3  answer y: 3  prediction: [ 0.01958642  0.03171291  0.02262999 99.9477      0.01368206  0.01402472\n",
      "  0.04606206]\n",
      "index: 42  actual y: 4  answer y: 1  prediction: [ 0.43062007 80.76785     0.32330194  1.493298   20.060972    1.9058859\n",
      "  0.8210345 ]\n",
      "index: 43  actual y: 0  answer y: 5  prediction: [13.460766    0.7322931   1.6954637   5.965108    0.66714716 52.6822\n",
      "  3.551664  ]\n",
      "index: 44  actual y: 4  answer y: 4  prediction: [ 0.05691154  0.06799974  0.1204435   0.1036484  99.74382     0.0400782\n",
      "  0.2740651 ]\n",
      "index: 45  actual y: 2  answer y: 2  prediction: [ 0.00285677  0.00242103 99.98659     0.00919655  0.00579967  0.0030113\n",
      "  0.00686811]\n",
      "index: 46  actual y: 5  answer y: 5  prediction: [ 0.04922577  0.00870269  0.01384371  0.4878993   0.02215253 99.880066\n",
      "  0.01650494]\n",
      "index: 47  actual y: 2  answer y: 2  prediction: [ 0.02427373  3.1421156  98.28018     0.01192542  0.15263051  0.07736013\n",
      "  0.05707127]\n",
      "index: 48  actual y: 2  answer y: 2  prediction: [25.271883    0.14550096 75.54287     4.2383265   0.06690858  0.06469773\n",
      "  0.20457599]\n",
      "index: 49  actual y: 6  answer y: 6  prediction: [ 0.072768    0.3132015   0.12844029  0.6232576   0.4753722  14.424093\n",
      " 84.425415  ]\n",
      "index: 50  actual y: 1  answer y: 5  prediction: [ 0.8969532   7.9894953   0.04286138 18.77311     0.29530242 26.96505\n",
      "  2.459827  ]\n",
      "index: 51  actual y: 3  answer y: 3  prediction: [ 0.45066598  0.0226369   0.67602575 60.006004    0.5974412  38.24257\n",
      "  0.06293389]\n",
      "index: 52  actual y: 2  answer y: 2  prediction: [ 0.01111576  0.02568923 99.86802     0.01326032  0.04147338  0.1240871\n",
      "  0.0099578 ]\n",
      "index: 53  actual y: 6  answer y: 6  prediction: [ 0.01956708  0.00910942  0.02081822  0.01248781  0.00257598  0.01286915\n",
      " 99.96698   ]\n",
      "index: 54  actual y: 5  answer y: 5  prediction: [ 0.5528282   0.01857409  0.28600007  0.10339775  0.08988613 99.3038\n",
      "  1.6783137 ]\n",
      "index: 55  actual y: 1  answer y: 1  prediction: [ 0.1831975  99.39843     0.02798714  0.0310025   0.04535098  0.06572711\n",
      "  1.2394322 ]\n",
      "index: 56  actual y: 4  answer y: 4  prediction: [ 0.0544828   0.15244794  0.01908694  0.04349181 99.81999     0.0133393\n",
      "  0.04604939]\n",
      "index: 57  actual y: 3  answer y: 3  prediction: [ 0.30124834  0.17976141  0.10621576 92.745155    1.328309    0.54277277\n",
      "  2.6116257 ]\n",
      "index: 58  actual y: 1  answer y: 1  prediction: [ 0.24386784 97.49108     0.12237937  1.4226892   0.28943342  0.28094625\n",
      "  0.7548884 ]\n",
      "index: 59  actual y: 3  answer y: 3  prediction: [ 0.03463781  0.01466876  0.03299035 99.71369     0.04143196  0.24256732\n",
      "  0.559788  ]\n",
      "index: 60  actual y: 1  answer y: 2  prediction: [ 0.6684332  26.441467   29.652605    0.1447663   8.507827    0.07726733\n",
      "  1.3861471 ]\n",
      "index: 61  actual y: 5  answer y: 5  prediction: [ 0.06054698  0.02670173  0.0332408   0.10541505  0.15680596 99.74957\n",
      "  0.19321364]\n",
      "index: 62  actual y: 4  answer y: 4  prediction: [ 0.04896382  0.01785101  0.0882078   0.03610897 99.84521     0.02218614\n",
      "  0.03582477]\n",
      "index: 63  actual y: 1  answer y: 5  prediction: [ 1.4761095  27.061245    0.24812485  0.02150326  0.18400002 57.824177\n",
      "  0.01184911]\n",
      "index: 64  actual y: 2  answer y: 5  prediction: [ 0.05830548  0.03755504  8.2149935   4.1407285   0.14304699 88.31186\n",
      "  0.09516998]\n",
      "index: 65  actual y: 1  answer y: 4  prediction: [ 0.02500911  0.31190267  2.1517305   0.55981    98.52556     0.01766467\n",
      "  0.20315257]\n",
      "index: 66  actual y: 1  answer y: 4  prediction: [ 0.08155929  2.4453354   0.17873377  0.44758403 76.4385     14.827004\n",
      "  0.08214535]\n",
      "index: 67  actual y: 5  answer y: 5  prediction: [ 0.0821747   0.0091111   0.07114205  0.00505115  0.10020761 99.84923\n",
      "  0.02359771]\n",
      "index: 68  actual y: 5  answer y: 5  prediction: [ 0.0789574   0.02958426  0.01540684  0.02683979  0.14470889 99.90011\n",
      "  0.02037469]\n",
      "index: 69  actual y: 4  answer y: 4  prediction: [ 0.10484239  0.0828921   1.5363818   0.14985414 99.003586    0.23846352\n",
      "  0.10920651]\n",
      "index: 70  actual y: 3  answer y: 3  prediction: [ 0.32384297  0.13063417  0.10677814 98.45602     0.29056725  0.73478884\n",
      "  0.7231062 ]\n",
      "index: 71  actual y: 6  answer y: 6  prediction: [ 0.09453146  0.2741471   0.5281983   1.8666446   4.959344    0.16749771\n",
      " 88.672035  ]\n",
      "index: 72  actual y: 1  answer y: 1  prediction: [ 0.757878   96.43695     0.06261658  0.06116372  1.4165632   1.1751271\n",
      "  0.08283404]\n",
      "index: 73  actual y: 3  answer y: 3  prediction: [ 0.02004239  5.115016    0.01735405 87.277565    0.21936347  1.6314027\n",
      "  0.69907784]\n",
      "index: 74  actual y: 3  answer y: 3  prediction: [ 0.02801362  0.02701957  0.08137457 99.50045     0.6055108   0.11439268\n",
      "  0.05725507]\n",
      "index: 75  actual y: 0  answer y: 0  prediction: [99.964355    0.0204976   0.02359889  0.02599365  0.0077086   0.03559877\n",
      "  0.02766296]\n",
      "index: 76  actual y: 3  answer y: 3  prediction: [ 2.0330136   0.0342503   0.04816771 98.89682     0.06529748  0.02041296\n",
      "  0.7981019 ]\n",
      "index: 77  actual y: 0  answer y: 0  prediction: [99.571396    0.03196137  0.02891736  0.03434342  0.12674697  0.23747435\n",
      "  1.9884104 ]\n",
      "index: 78  actual y: 2  answer y: 2  prediction: [ 0.02799795  0.09924912 87.3616      0.29563928  1.8609498   8.063458\n",
      "  0.02728946]\n",
      "index: 79  actual y: 3  answer y: 3  prediction: [ 0.72956747  0.02755577  0.02036943 73.614685    9.658382    0.0863884\n",
      " 10.451293  ]\n",
      "index: 80  actual y: 0  answer y: 0  prediction: [96.78533     0.01847513  9.181393    0.02202054  0.1594744   0.29536477\n",
      "  0.02585689]\n",
      "index: 81  actual y: 6  answer y: 2  prediction: [ 0.07087782  0.01706474 94.1752      0.00744747  0.07947099  0.29667908\n",
      "  8.446971  ]\n",
      "index: 82  actual y: 1  answer y: 1  prediction: [ 0.03076278 99.91961     0.02453498  0.04832063  0.08313864  0.01644689\n",
      "  0.05671591]\n",
      "index: 83  actual y: 2  answer y: 2  prediction: [ 0.02502829  3.2181914  18.490904    0.04938422  3.3808048   8.095226\n",
      "  1.2078516 ]\n",
      "index: 84  actual y: 0  answer y: 0  prediction: [87.73737     0.01094524  0.13113661  0.14053689  0.11197537 27.35653\n",
      "  0.02230875]\n",
      "index: 85  actual y: 6  answer y: 5  prediction: [ 0.40303433  0.01627767  0.32902887  1.4298176   3.1131675  94.89966\n",
      "  0.17082796]\n",
      "index: 86  actual y: 5  answer y: 5  prediction: [ 0.03252686  0.02720516  0.02649139  0.01651218  0.02655273 99.92825\n",
      "  0.01347955]\n",
      "index: 87  actual y: 5  answer y: 5  prediction: [ 0.22785075  0.04069739  0.06326815  2.5225885   0.35368875 98.215744\n",
      "  0.13406779]\n",
      "index: 88  actual y: 5  answer y: 5  prediction: [ 1.6739428   1.3793776   1.2946088   0.13024135  0.06653886 93.391594\n",
      "  0.16875409]\n",
      "index: 89  actual y: 6  answer y: 6  prediction: [ 0.09780262  0.03548099  0.05747209  0.0607629   0.03583697  0.03774121\n",
      " 99.84192   ]\n",
      "index: 90  actual y: 4  answer y: 4  prediction: [ 0.01904377  0.07708488  0.6834363   0.05115597 99.598434    0.00536461\n",
      "  0.21590844]\n",
      "index: 91  actual y: 3  answer y: 3  prediction: [ 1.0181203   0.0902065   0.17742664 91.24763     0.09311626  0.28278556\n",
      " 11.213452  ]\n",
      "index: 92  actual y: 1  answer y: 1  prediction: [ 0.11259725 99.440445    0.13729164  0.12578347  0.47951853  0.13905486\n",
      "  0.03639672]\n",
      "index: 93  actual y: 0  answer y: 0  prediction: [99.881065    0.01501668  0.02419118  0.07352988  0.02481395  0.24659927\n",
      "  0.00450222]\n",
      "index: 94  actual y: 6  answer y: 3  prediction: [ 0.5900613   0.08412476  0.04455485 96.036964    0.6551419   0.2906236\n",
      "  1.342826  ]\n",
      "index: 95  actual y: 1  answer y: 1  prediction: [ 0.01905127 99.7408      0.04323754  0.01257775  0.01408013  0.17555927\n",
      "  0.41681916]\n",
      "index: 96  actual y: 3  answer y: 1  prediction: [ 0.02485685 31.39488     0.09076266 10.896009    0.40885046  0.0954498\n",
      " 30.268642  ]\n",
      "index: 97  actual y: 4  answer y: 4  prediction: [ 0.00681585  0.01392431  0.06573793  0.04875457 99.89098     0.00594062\n",
      "  0.26164624]\n",
      "index: 98  actual y: 4  answer y: 4  prediction: [ 0.01699866  0.05913549  0.15110739 14.109787   89.363       0.03465543\n",
      "  2.3760011 ]\n",
      "index: 99  actual y: 1  answer y: 1  prediction: [ 0.05266673 99.75203     0.02568127  0.24840616  0.15336025  0.06935082\n",
      "  0.0280809 ]\n",
      "index: 100  actual y: 0  answer y: 0  prediction: [99.85969     0.03018098  0.19878727  0.04679155  0.056483    0.08196583\n",
      "  0.03601198]\n",
      "index: 101  actual y: 3  answer y: 5  prediction: [ 1.0985658   0.20814203  2.434404    0.12269352  3.8919244  31.711742\n",
      "  4.9364524 ]\n",
      "index: 102  actual y: 6  answer y: 6  prediction: [13.422031    0.15866123  0.02537542  1.3464373   0.34105742  0.15143241\n",
      " 83.48167   ]\n",
      "index: 103  actual y: 5  answer y: 5  prediction: [ 0.094401    0.01766868  0.01596707  0.08474292  0.02058321 99.93449\n",
      "  0.01764882]\n",
      "index: 104  actual y: 6  answer y: 6  prediction: [ 0.47636592  0.15757723  0.35641223  8.121574    0.8334687   0.8046837\n",
      " 83.551926  ]\n",
      "index: 105  actual y: 2  answer y: 2  prediction: [ 0.0376444   0.0217072  99.15041     0.6090076   0.03509276  0.53617704\n",
      "  0.05535568]\n",
      "index: 106  actual y: 2  answer y: 2  prediction: [ 0.00640694  0.04346709 98.344925    0.01734535  0.14910415  0.0241366\n",
      "  2.3077397 ]\n",
      "index: 107  actual y: 1  answer y: 1  prediction: [ 0.01532435 99.878555    0.00857551  0.24285537  0.01019867  0.0215254\n",
      "  0.0415578 ]\n",
      "index: 108  actual y: 2  answer y: 2  prediction: [ 0.00778572  0.03783633 99.89507     0.01719277  0.01198756  0.10258866\n",
      "  0.00404576]\n",
      "index: 109  actual y: 2  answer y: 2  prediction: [ 8.194371    0.08545382 93.93222     0.33958972  0.25947973  0.14890927\n",
      "  0.09171431]\n",
      "index: 110  actual y: 4  answer y: 4  prediction: [ 0.24162245  0.07520958  0.37269756  0.14806913 99.16285     0.14830776\n",
      "  0.33120212]\n",
      "index: 111  actual y: 4  answer y: 4  prediction: [ 0.07410234  0.18148594  0.07708913 46.397552   74.352196    0.08710988\n",
      "  0.27882323]\n",
      "index: 112  actual y: 2  answer y: 2  prediction: [ 0.05191794  0.04736938 99.67802     0.04995346  0.03808017  0.31560338\n",
      "  0.05171825]\n",
      "index: 113  actual y: 5  answer y: 5  prediction: [ 0.04756432  0.1745281   0.18589154  1.5048593   3.745191   91.47412\n",
      "  0.49267775]\n",
      "index: 114  actual y: 3  answer y: 3  prediction: [ 0.07279865  0.0576559   0.03756713 99.65413     0.24557188  0.02144609\n",
      "  0.18271287]\n",
      "index: 115  actual y: 0  answer y: 0  prediction: [99.84396     0.04270181  0.0093696   0.11452302  0.0413648   0.30508992\n",
      "  0.04472557]\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.1242 - accuracy: 0.8448 - precision: 0.9008 - recall: 0.8344 - f1score: 0.8661\n",
      "loss: 0.124, accuracy: 0.845, precision: 0.901, recall: 0.834, f1score: 0.866\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# output = classifier.predict_generator(test_set, steps=1)\n",
    "# print(test_set.class_indices)\n",
    "# print(output)\n",
    "size = y_test[:,-1]\n",
    "print(size.size)\n",
    "\n",
    "\n",
    "# predict 10 random hand-writing data\n",
    "y_predicted = model.predict(x_test)\n",
    "for x in range(0,size.size):\n",
    "    np.set_printoptions(suppress=True)\n",
    "    print(\"index:\", x,\n",
    "          \" actual y:\", np.argmax(y_test[x]),\n",
    "          \" answer y:\", np.argmax(y_predicted[x]),\n",
    "            \" prediction:\", np.array(y_predicted[x] * 100))\n",
    "\n",
    "_loss, _acc, _precision, _recall, _f1score = model.evaluate(x_test, y_test)\n",
    "print('loss: {:.3f}, accuracy: {:.3f}, precision: {:.3f}, recall: {:.3f}, f1score: {:.3f}'.format(_loss, _acc, _precision, _recall, _f1score))\n",
    "# print('loss: ', evaluation[0])\n",
    "# print('accuracy', evaluation[1])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 손실 그래프\n",
    "def plot_loss(history):\n",
    "   # 선 그리기\n",
    "    plt.plot(history.history['loss'], 'y', label='train loss')\n",
    "    plt.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "   # 그래프 제목\n",
    "    plt.title('Model Loss')\n",
    "   # x,y축 이름 표시\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "   # 각 라인 표식 표시\n",
    "    plt.legend(['Train','Validation'],loc=0)\n",
    "\n",
    "# 정확도 그래프\n",
    "def plot_acc(history):\n",
    "  # dir(history.history)\n",
    "    plt.plot(history.history['accuracy'], 'b', label='train accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], 'g', label='val accuracy')\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc=0)\n",
    "\n",
    "plot_loss(history)\n",
    "plt.show()\n",
    "plot_acc(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "xGR0eJBKxiG_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "animal_model_woman",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
