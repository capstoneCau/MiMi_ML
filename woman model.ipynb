{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "animal_model_woman",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AuZP58d8644",
        "outputId": "36f69364-8578-4e72-ba67-b5a4bf287dd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "import os # miscellaneous operating system interfaces\n",
        "import shutil # high-level file operations\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import random\n",
        "from itertools import product\n",
        "from keras.models import Sequential,load_model\n",
        "from keras.layers import Conv2D \n",
        "from keras.layers import MaxPooling2D \n",
        "from keras.layers import Flatten \n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.applications import MobileNetV2, Xception, DenseNet121,ResNet50V2,NASNetMobile\n",
        "from keras.applications.mobilenet_v2 import preprocess_input\n",
        "from keras.layers import  Input, Conv2D, Conv2DTranspose, ReLU,AveragePooling2D, MaxPooling2D, Dense, BatchNormalization, GlobalAveragePooling2D, Concatenate\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "\n",
        "base_dir = \"/content/drive/My Drive/Colab Notebooks/datasets/woman\"\n",
        "\n",
        "train_img_dir = \"/content/drive/My Drive/Colab Notebooks/datasets/woman/train\"\n",
        "\n",
        "test_img_dir = \"/content/drive/My Drive/Colab Notebooks/datasets/woman/test\"\n",
        "\n",
        "val_img_dir = \"/content/drive/My Drive/Colab Notebooks/datasets/woman/val\"\n",
        "\n",
        "\n",
        "learning_rate = 0.001\n",
        "training_epochs = 20 # traindata개수/batchsize\n",
        "batch_size = 512\n",
        "validation_steps = 20 # valdata개수/batchsize\n",
        "\n",
        "\n",
        "\n",
        "print(f'batchSIZE is {batch_size}, Learning Rate is {learning_rate}')\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "val_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "categories = ['dog','cat','rabbit','squirrel','deer','fox']\n",
        "\n",
        "training_set = train_datagen.flow_from_directory(train_img_dir, target_size=(128,128), \n",
        "                                             classes=categories, \n",
        "                                             batch_size=batch_size)\n",
        "\n",
        "test_set = test_datagen.flow_from_directory(test_img_dir,\n",
        "                                        target_size=(128,128), \n",
        "                                        classes=categories, \n",
        "                                        batch_size=batch_size)\n",
        "\n",
        "val_set = test_datagen.flow_from_directory(val_img_dir,\n",
        "                                        target_size=(128,128), \n",
        "                                        classes=categories, \n",
        "                                        batch_size=batch_size)\n",
        "\n",
        "x_train, y_train = next(training_set)\n",
        "x_test, y_test = next(test_set)\n",
        "x_val,y_val = next(val_set)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batchSIZE is 512, Learning Rate is 0.001\n",
            "Found 806 images belonging to 6 classes.\n",
            "Found 104 images belonging to 6 classes.\n",
            "Found 288 images belonging to 6 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDS8HPAA9CrJ"
      },
      "source": [
        "def create_model():\n",
        "    inputs = Input(shape=(128, 128, 3))\n",
        "    \n",
        "\n",
        "    conv2_1 = Conv2D(32, 3, 1, 'SAME')(inputs)\n",
        "    conv2_2 = Conv2D(32, 3, 1, 'SAME')(conv2_1)\n",
        "    pool2_3 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv2_2)\n",
        "    br1 = BatchNormalization()(pool2_3)\n",
        "    \n",
        "    conv2_1 = Conv2D(32, 3, 1, 'SAME')(br1)\n",
        "    conv2_2 = Conv2D(32, 3, 1, 'SAME')(conv2_1)\n",
        "    pool2_3 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv2_2)\n",
        "    br1 = BatchNormalization()(pool2_3)\n",
        "    \n",
        "    \n",
        "    conv3_1 = Conv2D(64, 3, 1, 'SAME')(br1)\n",
        "    conv3_2 = Conv2D(64, 3, 1, 'SAME')(conv3_1)\n",
        "    pool3_2 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv3_2)\n",
        "    br2 = BatchNormalization()(pool3_2)\n",
        "    \n",
        "    conv3_1 = Conv2D(64, 3, 1, 'SAME')(br2)\n",
        "    conv3_2 = Conv2D(64, 3, 1, 'SAME')(conv3_1)\n",
        "    pool3_2 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv3_2)\n",
        "    br2 = BatchNormalization()(pool3_2)\n",
        "    \n",
        "    \n",
        "    conv4_1 = Conv2D(128, 3, 1, 'SAME')(br2)\n",
        "    conv4_2 = Conv2D(128, 3, 1, 'SAME')(conv4_1)\n",
        "    pool4_2 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv4_2)\n",
        "    br3 = BatchNormalization()(pool4_2)\n",
        "    \n",
        "    conv4_1 = Conv2D(128, 3, 1, 'SAME')(br3)\n",
        "    conv4_2 = Conv2D(128, 3, 1, 'SAME')(conv4_1)\n",
        "    pool4_2 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv4_2)\n",
        "    br3 = BatchNormalization()(pool4_2)\n",
        "    \n",
        "    flatten1 = Flatten()(pool4_2)\n",
        "    dense1 = Dense(units = 512, activation = 'relu')(flatten1)\n",
        "    dense2 = Dense(units = 1024, activation = 'relu')(dense1)\n",
        "    dr1 = Dropout(0.7)(dense2)\n",
        "    dense3 = Dense(units = 5, activation = 'sigmoid')(dr1)\n",
        "    \n",
        "    return keras.Model(inputs=inputs, outputs=dense3) \n",
        "def mobile_net():\n",
        "    inputs = Input(shape=(128, 128, 3))\n",
        "    mobileNet = MobileNetV2(weights = \"imagenet\", include_top=False,input_shape = (128,128,3)\n",
        "                            ,input_tensor = inputs)\n",
        "    for layer in mobileNet.layers:\n",
        "        layer.trainable = True\n",
        "        \n",
        "    output = mobileNet.output\n",
        "    pooling = AveragePooling2D(pool_size=(16,16),padding='SAME')(output)\n",
        "    flatten1 = Flatten()(pooling)\n",
        "    dense1 = Dense(units = 256)(flatten1)\n",
        "    batch1 = BatchNormalization()(dense1)\n",
        "    relu1 = ReLU()(batch1)\n",
        "    dense2 = Dense(units = 64)(relu1)\n",
        "    batch2 = BatchNormalization()(dense2)\n",
        "    relu2 = ReLU()(batch2)\n",
        "    dense3 = Dense(units = 6, activation = 'sigmoid')(relu2)\n",
        "    \n",
        "    return keras.Model(inputs=inputs, outputs=dense3)\n",
        "\n",
        "def xception():\n",
        "    inputs = Input(shape=(128, 128, 3))\n",
        "    xception = Xception(weights = \"imagenet\", include_top=False,input_shape = (128,128,3)\n",
        "                            ,input_tensor = inputs)\n",
        "    for layer in xception.layers:\n",
        "        layer.trainable = True\n",
        "        \n",
        "    output = xception.output\n",
        "    pooling = AveragePooling2D(pool_size=(8,8),padding='SAME')(output)\n",
        "    flatten1 = Flatten()(pooling)\n",
        "    dense1 = Dense(units = 256)(flatten1)\n",
        "    batch1 = BatchNormalization()(dense1)\n",
        "    relu1 = ReLU()(batch1)\n",
        "    dense2 = Dense(units = 64)(relu1)\n",
        "    batch2 = BatchNormalization()(dense2)\n",
        "    relu2 = ReLU()(batch2)\n",
        "    dense3 = Dense(units = 6, activation = 'sigmoid')(relu2)\n",
        "    \n",
        "    return keras.Model(inputs=inputs, outputs=dense3)\n",
        "    # pooling = AveragePooling2D(pool_size=(4,4),padding='SAME')(output)\n",
        "    \n",
        "    # flatten1 = Flatten()(pooling)\n",
        "    # dense1 = Dense(units = 512, activation = 'relu')(flatten1)\n",
        "    # dense2 = Dense(units = 1024, activation = 'relu')(dense1)\n",
        "    # dr1 = Dropout(0.7)(dense2)\n",
        "    # dense3 = Dense(units = 5, activation = 'sigmoid')(dr1)\n",
        "    \n",
        "    # return keras.Model(inputs=inputs, outputs=dense3)\n",
        "\n",
        "def resnet():\n",
        "    inputs = Input(shape=(128, 128, 3))\n",
        "    resnet = ResNet50V2 (weights = \"imagenet\", include_top=False,input_shape = (128,128,3)\n",
        "                            ,input_tensor = inputs)\n",
        "    for layer in resnet.layers:\n",
        "        layer.trainable = True\n",
        "        \n",
        "    output = resnet.output\n",
        "    pooling = AveragePooling2D(pool_size=(8,8),padding='SAME')(output)\n",
        "    flatten1 = Flatten()(pooling)\n",
        "    dense1 = Dense(units = 256)(flatten1)\n",
        "    batch1 = BatchNormalization()(dense1)\n",
        "    relu1 = ReLU()(batch1)\n",
        "    dense2 = Dense(units = 64)(relu1)\n",
        "    batch2 = BatchNormalization()(dense2)\n",
        "    relu2 = ReLU()(batch2)\n",
        "    dense3 = Dense(units = 6, activation = 'sigmoid')(relu2)\n",
        "    \n",
        "    return keras.Model(inputs=inputs, outputs=dense3)\n",
        "\n",
        "def densenet():\n",
        "    inputs = Input(shape=(128, 128, 3))\n",
        "    densenet = DenseNet121(weights = \"imagenet\", include_top=False,input_shape = (128,128,3)\n",
        "                            ,input_tensor = inputs)\n",
        "    for layer in densenet.layers:\n",
        "        layer.trainable = True\n",
        "        \n",
        "    output = densenet.output\n",
        "    pooling = AveragePooling2D(pool_size=(32,32),padding='SAME')(output)\n",
        "    flatten1 = Flatten()(pooling)\n",
        "    dense1 = Dense(units = 128)(flatten1)\n",
        "    batch1 = BatchNormalization()(dense1)\n",
        "    relu1 = ReLU()(batch1)\n",
        "    dense2 = Dense(units = 32)(relu1)\n",
        "    batch2 = BatchNormalization()(dense2)\n",
        "    relu2 = ReLU()(batch2)\n",
        "    dense3 = Dense(units = 6, activation = 'sigmoid')(relu2)\n",
        "    \n",
        "    return keras.Model(inputs=inputs, outputs=dense3)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ResidualUnit(tf.keras.Model):\n",
        "    def __init__(self, filter_in, filter_out, kernel_size):\n",
        "        super(ResidualUnit, self).__init__()\n",
        "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
        "        self.conv1 = tf.keras.layers.Conv2D(filter_out, kernel_size, padding='same')\n",
        "        \n",
        "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
        "        self.conv2 = tf.keras.layers.Conv2D(filter_out, kernel_size, padding='same')\n",
        "        \n",
        "        if filter_in == filter_out:\n",
        "            self.identity = lambda x: x\n",
        "        else:\n",
        "            self.identity = tf.keras.layers.Conv2D(filter_out, (1,1), padding='same')\n",
        "\n",
        "    def call(self, x, training=False, mask=None):\n",
        "        h = self.bn1(x, training=training)\n",
        "        h = tf.nn.relu(h)\n",
        "        h = self.conv1(h)\n",
        "        \n",
        "        h = self.bn2(h, training=training)\n",
        "        h = tf.nn.relu(h)\n",
        "        h = self.conv2(h)\n",
        "        return self.identity(x) + h\n",
        "    \n",
        "class ResnetLayer(tf.keras.Model):\n",
        "    def __init__(self, filter_in, filters, kernel_size):\n",
        "        super(ResnetLayer, self).__init__()\n",
        "        self.sequence = list()\n",
        "        for f_in, f_out in zip([filter_in] + list(filters), filters):\n",
        "            self.sequence.append(ResidualUnit(f_in, f_out, kernel_size))\n",
        "\n",
        "    def call(self, x, training=False, mask=None):\n",
        "        for unit in self.sequence:\n",
        "            x = unit(x, training=training)\n",
        "        return x\n",
        "    \n",
        "class ResNet(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.conv1 = tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu') # 28x28x8\n",
        "        \n",
        "        self.res1 = ResnetLayer(64, (16, 16), (3, 3)) # 28x28x16\n",
        "        self.pool1 = tf.keras.layers.MaxPool2D((2, 2)) # 14x14x16\n",
        "        \n",
        "        \n",
        "        self.res2 = ResnetLayer(128, (32, 32), (3, 3)) # 14x14x32\n",
        "        self.pool2 = tf.keras.layers.MaxPool2D((2, 2)) # 7x7x32\n",
        "    \n",
        "        \n",
        "        self.res3 = ResnetLayer(256, (64, 64), (3, 3)) # 7x7x64\n",
        "        self.pool = tf.keras.layers.MaxPool2D((2, 2)) # 7x7x32\n",
        "        \n",
        "        \n",
        "        self.res4 = ResnetLayer(512, (64, 64), (3, 3)) # 7x7x64\n",
        "        self.pool = tf.keras.layers.MaxPool2D((2, 2)) # 7x7x32\n",
        "        \n",
        "        \n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "        self.dense1 = tf.keras.layers.Dense(1024, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(5, activation='softmax')\n",
        "        \n",
        "    def call(self, x, training=False, mask=None):\n",
        "        x = self.conv1(x)\n",
        "        \n",
        "        x = self.res1(x, training=training)\n",
        "        x = self.pool1(x)\n",
        "        x = self.res2(x, training=training)\n",
        "        x = self.pool2(x)\n",
        "        x = self.res3(x, training=training)\n",
        "        \n",
        "        x = self.flatten(x)\n",
        "        x = self.dense1(x)\n",
        "        return self.dense2(x)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVIEcU_U9C-h",
        "outputId": "5366fd3b-9bb9-4d70-cf52-f0996e7fd179",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def recall(y_target, y_pred):\n",
        "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
        "    # round : 반올림한다\n",
        "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "\n",
        "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
        "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
        "\n",
        "    # (True Positive + False Negative) = 실제 값이 1(Positive) 전체\n",
        "    count_true_positive_false_negative = K.sum(y_target_yn)\n",
        "\n",
        "    # Recall =  (True Positive) / (True Positive + False Negative)\n",
        "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
        "    recall = count_true_positive / (count_true_positive_false_negative + K.epsilon())\n",
        "\n",
        "    # return a single tensor value\n",
        "    return recall\n",
        "\n",
        "\n",
        "def precision(y_target, y_pred):\n",
        "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
        "    # round : 반올림한다\n",
        "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "\n",
        "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
        "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
        "\n",
        "    # (True Positive + False Positive) = 예측 값이 1(Positive) 전체\n",
        "    count_true_positive_false_positive = K.sum(y_pred_yn)\n",
        "\n",
        "    # Precision = (True Positive) / (True Positive + False Positive)\n",
        "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
        "    precision = count_true_positive / (count_true_positive_false_positive + K.epsilon())\n",
        "\n",
        "    # return a single tensor value\n",
        "    return precision\n",
        "\n",
        "\n",
        "def f1score(y_target, y_pred):\n",
        "    _recall = recall(y_target, y_pred)\n",
        "    _precision = precision(y_target, y_pred)\n",
        "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
        "    _f1score = ( 2 * _recall * _precision) / (_recall + _precision+ K.epsilon())\n",
        "    \n",
        "    # return a single tensor value\n",
        "    return _f1score\n",
        "\n",
        "\n",
        "\n",
        "#model = create_model()\n",
        "#model = ResNet()\n",
        "#model = mobile_net()\n",
        "#model = xception()\n",
        "model = densenet()\n",
        "#model = resnet()\n",
        "\n",
        "lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=learning_rate,\n",
        "                                                          decay_steps=training_epochs * 10,\n",
        "                                                          decay_rate=0.5,\n",
        "                                                          staircase=True)\n",
        "\n",
        "\n",
        "\n",
        "# lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "#     initial_learning_rate=learning_rate,\n",
        "#     decay_steps=100000,\n",
        "#     decay_rate=0.96)\n",
        "\n",
        "# optimizer는 Adam, loss는 sparse categorical crossentropy 사용\n",
        "# label이 ont-hot으로 encoding 안 된 경우에 sparse categorical corssentropy 및 sparse categorical accuracy 사용\n",
        "model.compile(keras.optimizers.Adam(lr_schedule), loss = 'binary_crossentropy', metrics=['accuracy', precision, recall, f1score])\n",
        "\n",
        "# model.compile(\n",
        "#     optimizer='adam',\n",
        "#     loss='binary_crossentropy', \n",
        "#     metrics=['accuracy', precision, recall, f1score],\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Train!\n",
        "history = model.fit(x_train, y_train, steps_per_epoch=training_epochs,\n",
        "         epochs=120, validation_data = (x_val,y_val),validation_steps=validation_steps)\n",
        "model.save('animal_model_woman.h5')\n",
        "# epochs = 30\n",
        "# history = model.fit(\n",
        "#     training_set, \n",
        "#     epochs=epochs,\n",
        "#     steps_per_epoch=training_set.samples / epochs, \n",
        "#     validation_data=val_set,\n",
        "#     validation_steps=val_set.samples / epochs,\n",
        "# )"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/120\n",
            "20/20 [==============================] - 4s 195ms/step - loss: 0.6949 - accuracy: 0.2305 - precision: 0.2133 - recall: 0.5075 - f1score: 0.2992 - val_loss: 19.3987 - val_accuracy: 0.1424 - val_precision: 0.1533 - val_recall: 0.3067 - val_f1score: 0.2044\n",
            "Epoch 2/120\n",
            "20/20 [==============================] - 2s 111ms/step - loss: 0.5556 - accuracy: 0.3320 - precision: 0.3360 - recall: 0.4895 - f1score: 0.3934 - val_loss: 0.9315 - val_accuracy: 0.1632 - val_precision: 0.1494 - val_recall: 0.2667 - val_f1score: 0.1910\n",
            "Epoch 3/120\n",
            "20/20 [==============================] - 2s 111ms/step - loss: 0.4545 - accuracy: 0.5273 - precision: 0.4770 - recall: 0.5767 - f1score: 0.5208 - val_loss: 10.8748 - val_accuracy: 0.1632 - val_precision: 0.1439 - val_recall: 0.2567 - val_f1score: 0.1842\n",
            "Epoch 4/120\n",
            "20/20 [==============================] - 2s 111ms/step - loss: 0.3534 - accuracy: 0.7480 - precision: 0.6504 - recall: 0.7639 - f1score: 0.7015 - val_loss: 0.8448 - val_accuracy: 0.2500 - val_precision: 0.2463 - val_recall: 0.2667 - val_f1score: 0.2551\n",
            "Epoch 5/120\n",
            "20/20 [==============================] - 2s 111ms/step - loss: 0.2920 - accuracy: 0.8008 - precision: 0.7674 - recall: 0.8137 - f1score: 0.7883 - val_loss: 0.6513 - val_accuracy: 0.1736 - val_precision: 0.1792 - val_recall: 0.2367 - val_f1score: 0.2034\n",
            "Epoch 6/120\n",
            "20/20 [==============================] - 2s 112ms/step - loss: 0.2269 - accuracy: 0.8848 - precision: 0.8679 - recall: 0.8677 - f1score: 0.8673 - val_loss: 0.8517 - val_accuracy: 0.2743 - val_precision: 0.2585 - val_recall: 0.3100 - val_f1score: 0.2805\n",
            "Epoch 7/120\n",
            "20/20 [==============================] - 2s 113ms/step - loss: 0.1827 - accuracy: 0.9180 - precision: 0.9153 - recall: 0.9109 - f1score: 0.9128 - val_loss: 0.5578 - val_accuracy: 0.3993 - val_precision: 0.3442 - val_recall: 0.4100 - val_f1score: 0.3733\n",
            "Epoch 8/120\n",
            "20/20 [==============================] - 2s 112ms/step - loss: 0.1703 - accuracy: 0.9102 - precision: 0.9240 - recall: 0.8925 - f1score: 0.9072 - val_loss: 0.5027 - val_accuracy: 0.5000 - val_precision: 0.5393 - val_recall: 0.4733 - val_f1score: 0.5021\n",
            "Epoch 9/120\n",
            "20/20 [==============================] - 2s 112ms/step - loss: 0.1395 - accuracy: 0.9316 - precision: 0.9407 - recall: 0.9241 - f1score: 0.9318 - val_loss: 0.3810 - val_accuracy: 0.5590 - val_precision: 0.5511 - val_recall: 0.5333 - val_f1score: 0.5408\n",
            "Epoch 10/120\n",
            "20/20 [==============================] - 2s 112ms/step - loss: 0.1269 - accuracy: 0.9395 - precision: 0.9452 - recall: 0.9312 - f1score: 0.9376 - val_loss: 0.6524 - val_accuracy: 0.2465 - val_precision: 0.2347 - val_recall: 0.2300 - val_f1score: 0.2320\n",
            "Epoch 11/120\n",
            "20/20 [==============================] - 2s 112ms/step - loss: 0.1150 - accuracy: 0.9395 - precision: 0.9440 - recall: 0.9357 - f1score: 0.9395 - val_loss: 0.3781 - val_accuracy: 0.5417 - val_precision: 0.5381 - val_recall: 0.5500 - val_f1score: 0.5426\n",
            "Epoch 12/120\n",
            "20/20 [==============================] - 2s 112ms/step - loss: 0.0912 - accuracy: 0.9766 - precision: 0.9846 - recall: 0.9712 - f1score: 0.9776 - val_loss: 0.3727 - val_accuracy: 0.5347 - val_precision: 0.5652 - val_recall: 0.5100 - val_f1score: 0.5343\n",
            "Epoch 13/120\n",
            "20/20 [==============================] - 2s 112ms/step - loss: 0.0826 - accuracy: 0.9863 - precision: 0.9902 - recall: 0.9808 - f1score: 0.9854 - val_loss: 0.2617 - val_accuracy: 0.7083 - val_precision: 0.6854 - val_recall: 0.6533 - val_f1score: 0.6679\n",
            "Epoch 14/120\n",
            "20/20 [==============================] - 2s 113ms/step - loss: 0.0647 - accuracy: 0.9980 - precision: 0.9962 - recall: 0.9953 - f1score: 0.9957 - val_loss: 0.2450 - val_accuracy: 0.7292 - val_precision: 0.7223 - val_recall: 0.6633 - val_f1score: 0.6904\n",
            "Epoch 15/120\n",
            "20/20 [==============================] - 2s 113ms/step - loss: 0.0648 - accuracy: 0.9941 - precision: 0.9925 - recall: 0.9962 - f1score: 0.9943 - val_loss: 0.2734 - val_accuracy: 0.6979 - val_precision: 0.6942 - val_recall: 0.6233 - val_f1score: 0.6546\n",
            "Epoch 16/120\n",
            "20/20 [==============================] - 2s 113ms/step - loss: 0.0591 - accuracy: 0.9922 - precision: 0.9942 - recall: 0.9915 - f1score: 0.9928 - val_loss: 0.2652 - val_accuracy: 0.6875 - val_precision: 0.6963 - val_recall: 0.6333 - val_f1score: 0.6620\n",
            "Epoch 17/120\n",
            "20/20 [==============================] - 2s 113ms/step - loss: 0.0598 - accuracy: 0.9941 - precision: 0.9962 - recall: 0.9923 - f1score: 0.9942 - val_loss: 0.2588 - val_accuracy: 0.6944 - val_precision: 0.6879 - val_recall: 0.6433 - val_f1score: 0.6639\n",
            "Epoch 18/120\n",
            "20/20 [==============================] - 2s 112ms/step - loss: 0.0579 - accuracy: 0.9922 - precision: 0.9915 - recall: 0.9895 - f1score: 0.9905 - val_loss: 0.2446 - val_accuracy: 0.7292 - val_precision: 0.7447 - val_recall: 0.6567 - val_f1score: 0.6961\n",
            "Epoch 19/120\n",
            "20/20 [==============================] - 2s 112ms/step - loss: 0.0573 - accuracy: 0.9902 - precision: 0.9883 - recall: 0.9868 - f1score: 0.9874 - val_loss: 0.2488 - val_accuracy: 0.7049 - val_precision: 0.7399 - val_recall: 0.6500 - val_f1score: 0.6901\n",
            "Epoch 20/120\n",
            "20/20 [==============================] - 2s 113ms/step - loss: 0.0557 - accuracy: 0.9922 - precision: 0.9922 - recall: 0.9827 - f1score: 0.9873 - val_loss: 0.2410 - val_accuracy: 0.6979 - val_precision: 0.7751 - val_recall: 0.6700 - val_f1score: 0.7166\n",
            "Epoch 21/120\n",
            "20/20 [==============================] - 2s 113ms/step - loss: 0.0505 - accuracy: 0.9883 - precision: 0.9887 - recall: 0.9848 - f1score: 0.9867 - val_loss: 0.1891 - val_accuracy: 0.7986 - val_precision: 0.8235 - val_recall: 0.7700 - val_f1score: 0.7951\n",
            "Epoch 22/120\n",
            "20/20 [==============================] - 2s 113ms/step - loss: 0.0436 - accuracy: 0.9961 - precision: 0.9962 - recall: 0.9942 - f1score: 0.9952 - val_loss: 0.2220 - val_accuracy: 0.7431 - val_precision: 0.7828 - val_recall: 0.6833 - val_f1score: 0.7271\n",
            "Epoch 23/120\n",
            "20/20 [==============================] - 2s 113ms/step - loss: 0.0428 - accuracy: 1.0000 - precision: 0.9981 - recall: 1.0000 - f1score: 0.9991 - val_loss: 0.1890 - val_accuracy: 0.7986 - val_precision: 0.8335 - val_recall: 0.7633 - val_f1score: 0.7948\n",
            "Epoch 24/120\n",
            "20/20 [==============================] - 2s 113ms/step - loss: 0.0456 - accuracy: 0.9922 - precision: 0.9889 - recall: 0.9889 - f1score: 0.9889 - val_loss: 0.2130 - val_accuracy: 0.7569 - val_precision: 0.7809 - val_recall: 0.6933 - val_f1score: 0.7332\n",
            "Epoch 25/120\n",
            "20/20 [==============================] - 2s 113ms/step - loss: 0.0400 - accuracy: 1.0000 - precision: 1.0000 - recall: 0.9962 - f1score: 0.9980 - val_loss: 0.2045 - val_accuracy: 0.7708 - val_precision: 0.8178 - val_recall: 0.7467 - val_f1score: 0.7787\n",
            "Epoch 26/120\n",
            "20/20 [==============================] - 2s 113ms/step - loss: 0.0388 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1627 - val_accuracy: 0.8160 - val_precision: 0.8532 - val_recall: 0.7767 - val_f1score: 0.8117\n",
            "Epoch 27/120\n",
            "20/20 [==============================] - 2s 113ms/step - loss: 0.0367 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1651 - val_accuracy: 0.8056 - val_precision: 0.8529 - val_recall: 0.7667 - val_f1score: 0.8052\n",
            "Epoch 28/120\n",
            "20/20 [==============================] - 2s 114ms/step - loss: 0.0349 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1559 - val_accuracy: 0.8333 - val_precision: 0.8651 - val_recall: 0.7733 - val_f1score: 0.8144\n",
            "Epoch 29/120\n",
            "20/20 [==============================] - 2s 114ms/step - loss: 0.0347 - accuracy: 0.9980 - precision: 0.9981 - recall: 0.9981 - f1score: 0.9981 - val_loss: 0.1639 - val_accuracy: 0.8542 - val_precision: 0.8696 - val_recall: 0.8233 - val_f1score: 0.8449\n",
            "Epoch 30/120\n",
            "20/20 [==============================] - 2s 113ms/step - loss: 0.0360 - accuracy: 0.9961 - precision: 0.9981 - recall: 0.9962 - f1score: 0.9971 - val_loss: 0.1612 - val_accuracy: 0.8299 - val_precision: 0.8399 - val_recall: 0.7800 - val_f1score: 0.8076\n",
            "Epoch 31/120\n",
            "20/20 [==============================] - 2s 114ms/step - loss: 0.0325 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1681 - val_accuracy: 0.8090 - val_precision: 0.8190 - val_recall: 0.7500 - val_f1score: 0.7819\n",
            "Epoch 32/120\n",
            "20/20 [==============================] - 2s 113ms/step - loss: 0.0306 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1670 - val_accuracy: 0.8090 - val_precision: 0.8414 - val_recall: 0.7833 - val_f1score: 0.8098\n",
            "Epoch 33/120\n",
            "20/20 [==============================] - 2s 113ms/step - loss: 0.0326 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1668 - val_accuracy: 0.8160 - val_precision: 0.8649 - val_recall: 0.7600 - val_f1score: 0.8009\n",
            "Epoch 34/120\n",
            "20/20 [==============================] - 2s 113ms/step - loss: 0.0313 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1623 - val_accuracy: 0.8229 - val_precision: 0.8758 - val_recall: 0.7667 - val_f1score: 0.8091\n",
            "Epoch 35/120\n",
            "20/20 [==============================] - 2s 113ms/step - loss: 0.0291 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1590 - val_accuracy: 0.8229 - val_precision: 0.8780 - val_recall: 0.7900 - val_f1score: 0.8287\n",
            "Epoch 36/120\n",
            "20/20 [==============================] - 2s 113ms/step - loss: 0.0296 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1622 - val_accuracy: 0.8194 - val_precision: 0.8543 - val_recall: 0.7967 - val_f1score: 0.8226\n",
            "Epoch 37/120\n",
            "20/20 [==============================] - 2s 113ms/step - loss: 0.0287 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1543 - val_accuracy: 0.8368 - val_precision: 0.8546 - val_recall: 0.8133 - val_f1score: 0.8327\n",
            "Epoch 38/120\n",
            "20/20 [==============================] - 2s 113ms/step - loss: 0.0285 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1507 - val_accuracy: 0.8403 - val_precision: 0.8724 - val_recall: 0.8133 - val_f1score: 0.8406\n",
            "Epoch 39/120\n",
            "20/20 [==============================] - 2s 114ms/step - loss: 0.0311 - accuracy: 0.9980 - precision: 0.9980 - recall: 0.9962 - f1score: 0.9971 - val_loss: 0.1827 - val_accuracy: 0.7847 - val_precision: 0.8054 - val_recall: 0.7767 - val_f1score: 0.7899\n",
            "Epoch 40/120\n",
            "20/20 [==============================] - 2s 114ms/step - loss: 0.0280 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.2082 - val_accuracy: 0.7639 - val_precision: 0.8007 - val_recall: 0.7233 - val_f1score: 0.7571\n",
            "Epoch 41/120\n",
            "20/20 [==============================] - 2s 114ms/step - loss: 0.0261 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1909 - val_accuracy: 0.7778 - val_precision: 0.7859 - val_recall: 0.7633 - val_f1score: 0.7739\n",
            "Epoch 42/120\n",
            "20/20 [==============================] - 2s 113ms/step - loss: 0.0285 - accuracy: 0.9980 - precision: 0.9981 - recall: 0.9981 - f1score: 0.9981 - val_loss: 0.1791 - val_accuracy: 0.8056 - val_precision: 0.8099 - val_recall: 0.7567 - val_f1score: 0.7799\n",
            "Epoch 43/120\n",
            "20/20 [==============================] - 2s 114ms/step - loss: 0.0285 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1736 - val_accuracy: 0.8125 - val_precision: 0.8320 - val_recall: 0.7700 - val_f1score: 0.7989\n",
            "Epoch 44/120\n",
            "20/20 [==============================] - 2s 114ms/step - loss: 0.0271 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1649 - val_accuracy: 0.8264 - val_precision: 0.8429 - val_recall: 0.7900 - val_f1score: 0.8146\n",
            "Epoch 45/120\n",
            "20/20 [==============================] - 2s 113ms/step - loss: 0.0281 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1624 - val_accuracy: 0.8368 - val_precision: 0.8530 - val_recall: 0.8033 - val_f1score: 0.8267\n",
            "Epoch 46/120\n",
            "20/20 [==============================] - 2s 114ms/step - loss: 0.0264 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1596 - val_accuracy: 0.8368 - val_precision: 0.8535 - val_recall: 0.8067 - val_f1score: 0.8287\n",
            "Epoch 47/120\n",
            "20/20 [==============================] - 2s 114ms/step - loss: 0.0260 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1576 - val_accuracy: 0.8333 - val_precision: 0.8538 - val_recall: 0.8033 - val_f1score: 0.8269\n",
            "Epoch 48/120\n",
            "20/20 [==============================] - 2s 114ms/step - loss: 0.0287 - accuracy: 1.0000 - precision: 0.9981 - recall: 1.0000 - f1score: 0.9991 - val_loss: 0.1530 - val_accuracy: 0.8438 - val_precision: 0.8587 - val_recall: 0.8133 - val_f1score: 0.8347\n",
            "Epoch 49/120\n",
            "20/20 [==============================] - 2s 113ms/step - loss: 0.0246 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1503 - val_accuracy: 0.8472 - val_precision: 0.8592 - val_recall: 0.8167 - val_f1score: 0.8367\n",
            "Epoch 50/120\n",
            "20/20 [==============================] - 2s 114ms/step - loss: 0.0268 - accuracy: 0.9980 - precision: 0.9981 - recall: 0.9981 - f1score: 0.9981 - val_loss: 0.1482 - val_accuracy: 0.8438 - val_precision: 0.8633 - val_recall: 0.8200 - val_f1score: 0.8403\n",
            "Epoch 51/120\n",
            "20/20 [==============================] - 2s 114ms/step - loss: 0.0266 - accuracy: 1.0000 - precision: 1.0000 - recall: 0.9981 - f1score: 0.9990 - val_loss: 0.1478 - val_accuracy: 0.8542 - val_precision: 0.8657 - val_recall: 0.8167 - val_f1score: 0.8396\n",
            "Epoch 52/120\n",
            "20/20 [==============================] - 2s 115ms/step - loss: 0.0256 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1497 - val_accuracy: 0.8403 - val_precision: 0.8532 - val_recall: 0.8167 - val_f1score: 0.8339\n",
            "Epoch 53/120\n",
            "20/20 [==============================] - 2s 114ms/step - loss: 0.0261 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1491 - val_accuracy: 0.8438 - val_precision: 0.8541 - val_recall: 0.8133 - val_f1score: 0.8324\n",
            "Epoch 54/120\n",
            "20/20 [==============================] - 2s 114ms/step - loss: 0.0251 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1491 - val_accuracy: 0.8438 - val_precision: 0.8568 - val_recall: 0.8167 - val_f1score: 0.8356\n",
            "Epoch 55/120\n",
            "20/20 [==============================] - 2s 113ms/step - loss: 0.0264 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1488 - val_accuracy: 0.8438 - val_precision: 0.8617 - val_recall: 0.8133 - val_f1score: 0.8361\n",
            "Epoch 56/120\n",
            "20/20 [==============================] - 2s 114ms/step - loss: 0.0245 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1486 - val_accuracy: 0.8438 - val_precision: 0.8612 - val_recall: 0.8100 - val_f1score: 0.8341\n",
            "Epoch 57/120\n",
            "20/20 [==============================] - 2s 115ms/step - loss: 0.0275 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1485 - val_accuracy: 0.8472 - val_precision: 0.8615 - val_recall: 0.8133 - val_f1score: 0.8361\n",
            "Epoch 58/120\n",
            "20/20 [==============================] - 2s 114ms/step - loss: 0.0305 - accuracy: 0.9922 - precision: 0.9957 - recall: 0.9904 - f1score: 0.9929 - val_loss: 0.1687 - val_accuracy: 0.8056 - val_precision: 0.8088 - val_recall: 0.7833 - val_f1score: 0.7955\n",
            "Epoch 59/120\n",
            "20/20 [==============================] - 2s 114ms/step - loss: 0.0252 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1681 - val_accuracy: 0.8125 - val_precision: 0.8220 - val_recall: 0.7900 - val_f1score: 0.8050\n",
            "Epoch 60/120\n",
            "20/20 [==============================] - 2s 113ms/step - loss: 0.0243 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1627 - val_accuracy: 0.8194 - val_precision: 0.8288 - val_recall: 0.7967 - val_f1score: 0.8120\n",
            "Epoch 61/120\n",
            "20/20 [==============================] - 2s 114ms/step - loss: 0.0241 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1595 - val_accuracy: 0.8194 - val_precision: 0.8326 - val_recall: 0.8000 - val_f1score: 0.8154\n",
            "Epoch 62/120\n",
            "20/20 [==============================] - 2s 115ms/step - loss: 0.0258 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1583 - val_accuracy: 0.8333 - val_precision: 0.8359 - val_recall: 0.8033 - val_f1score: 0.8188\n",
            "Epoch 63/120\n",
            "20/20 [==============================] - 2s 116ms/step - loss: 0.0268 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1548 - val_accuracy: 0.8333 - val_precision: 0.8456 - val_recall: 0.8067 - val_f1score: 0.8252\n",
            "Epoch 64/120\n",
            "20/20 [==============================] - 2s 115ms/step - loss: 0.0248 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1520 - val_accuracy: 0.8333 - val_precision: 0.8485 - val_recall: 0.8100 - val_f1score: 0.8282\n",
            "Epoch 65/120\n",
            "20/20 [==============================] - 2s 116ms/step - loss: 0.0234 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1511 - val_accuracy: 0.8368 - val_precision: 0.8544 - val_recall: 0.8067 - val_f1score: 0.8292\n",
            "Epoch 66/120\n",
            "20/20 [==============================] - 2s 115ms/step - loss: 0.0253 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1508 - val_accuracy: 0.8368 - val_precision: 0.8581 - val_recall: 0.8100 - val_f1score: 0.8326\n",
            "Epoch 67/120\n",
            "20/20 [==============================] - 2s 115ms/step - loss: 0.0239 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1521 - val_accuracy: 0.8368 - val_precision: 0.8519 - val_recall: 0.8067 - val_f1score: 0.8282\n",
            "Epoch 68/120\n",
            "20/20 [==============================] - 2s 115ms/step - loss: 0.0266 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1512 - val_accuracy: 0.8403 - val_precision: 0.8493 - val_recall: 0.8133 - val_f1score: 0.8302\n",
            "Epoch 69/120\n",
            "20/20 [==============================] - 2s 114ms/step - loss: 0.0246 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1506 - val_accuracy: 0.8403 - val_precision: 0.8493 - val_recall: 0.8133 - val_f1score: 0.8302\n",
            "Epoch 70/120\n",
            "20/20 [==============================] - 2s 115ms/step - loss: 0.0242 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1501 - val_accuracy: 0.8438 - val_precision: 0.8519 - val_recall: 0.8167 - val_f1score: 0.8332\n",
            "Epoch 71/120\n",
            "20/20 [==============================] - 2s 114ms/step - loss: 0.0235 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1501 - val_accuracy: 0.8438 - val_precision: 0.8514 - val_recall: 0.8133 - val_f1score: 0.8312\n",
            "Epoch 72/120\n",
            "20/20 [==============================] - 2s 114ms/step - loss: 0.0250 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1499 - val_accuracy: 0.8438 - val_precision: 0.8493 - val_recall: 0.8167 - val_f1score: 0.8319\n",
            "Epoch 73/120\n",
            "20/20 [==============================] - 2s 115ms/step - loss: 0.0241 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1499 - val_accuracy: 0.8438 - val_precision: 0.8519 - val_recall: 0.8167 - val_f1score: 0.8332\n",
            "Epoch 74/120\n",
            "20/20 [==============================] - 2s 115ms/step - loss: 0.0248 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1498 - val_accuracy: 0.8403 - val_precision: 0.8519 - val_recall: 0.8133 - val_f1score: 0.8314\n",
            "Epoch 75/120\n",
            "20/20 [==============================] - 2s 114ms/step - loss: 0.0238 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1499 - val_accuracy: 0.8403 - val_precision: 0.8519 - val_recall: 0.8133 - val_f1score: 0.8314\n",
            "Epoch 76/120\n",
            "20/20 [==============================] - 2s 116ms/step - loss: 0.0239 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1498 - val_accuracy: 0.8403 - val_precision: 0.8519 - val_recall: 0.8133 - val_f1score: 0.8314\n",
            "Epoch 77/120\n",
            "20/20 [==============================] - 2s 115ms/step - loss: 0.0255 - accuracy: 1.0000 - precision: 1.0000 - recall: 0.9981 - f1score: 0.9990 - val_loss: 0.1506 - val_accuracy: 0.8368 - val_precision: 0.8528 - val_recall: 0.8133 - val_f1score: 0.8318\n",
            "Epoch 78/120\n",
            "20/20 [==============================] - 2s 115ms/step - loss: 0.0245 - accuracy: 1.0000 - precision: 1.0000 - recall: 0.9981 - f1score: 0.9990 - val_loss: 0.1523 - val_accuracy: 0.8333 - val_precision: 0.8478 - val_recall: 0.8133 - val_f1score: 0.8295\n",
            "Epoch 79/120\n",
            "20/20 [==============================] - 2s 114ms/step - loss: 0.0253 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1529 - val_accuracy: 0.8333 - val_precision: 0.8481 - val_recall: 0.8133 - val_f1score: 0.8298\n",
            "Epoch 80/120\n",
            "20/20 [==============================] - 2s 115ms/step - loss: 0.0255 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1525 - val_accuracy: 0.8333 - val_precision: 0.8478 - val_recall: 0.8133 - val_f1score: 0.8295\n",
            "Epoch 81/120\n",
            "20/20 [==============================] - 2s 114ms/step - loss: 0.0232 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1523 - val_accuracy: 0.8333 - val_precision: 0.8511 - val_recall: 0.8133 - val_f1score: 0.8312\n",
            "Epoch 82/120\n",
            "20/20 [==============================] - 2s 115ms/step - loss: 0.0239 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1520 - val_accuracy: 0.8333 - val_precision: 0.8501 - val_recall: 0.8100 - val_f1score: 0.8288\n",
            "Epoch 83/120\n",
            "20/20 [==============================] - 2s 115ms/step - loss: 0.0256 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1516 - val_accuracy: 0.8333 - val_precision: 0.8501 - val_recall: 0.8100 - val_f1score: 0.8288\n",
            "Epoch 84/120\n",
            "20/20 [==============================] - 2s 115ms/step - loss: 0.0229 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1513 - val_accuracy: 0.8333 - val_precision: 0.8501 - val_recall: 0.8100 - val_f1score: 0.8288\n",
            "Epoch 85/120\n",
            "20/20 [==============================] - 2s 114ms/step - loss: 0.0246 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1506 - val_accuracy: 0.8403 - val_precision: 0.8504 - val_recall: 0.8133 - val_f1score: 0.8308\n",
            "Epoch 86/120\n",
            "20/20 [==============================] - 2s 115ms/step - loss: 0.0244 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1505 - val_accuracy: 0.8403 - val_precision: 0.8528 - val_recall: 0.8133 - val_f1score: 0.8319\n",
            "Epoch 87/120\n",
            "20/20 [==============================] - 2s 114ms/step - loss: 0.0219 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1505 - val_accuracy: 0.8403 - val_precision: 0.8535 - val_recall: 0.8133 - val_f1score: 0.8321\n",
            "Epoch 88/120\n",
            "20/20 [==============================] - 2s 114ms/step - loss: 0.0267 - accuracy: 0.9961 - precision: 0.9980 - recall: 0.9962 - f1score: 0.9971 - val_loss: 0.1502 - val_accuracy: 0.8403 - val_precision: 0.8537 - val_recall: 0.8167 - val_f1score: 0.8339\n",
            "Epoch 89/120\n",
            "20/20 [==============================] - 2s 115ms/step - loss: 0.0228 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1500 - val_accuracy: 0.8403 - val_precision: 0.8573 - val_recall: 0.8167 - val_f1score: 0.8356\n",
            "Epoch 90/120\n",
            "20/20 [==============================] - 2s 115ms/step - loss: 0.0249 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1498 - val_accuracy: 0.8403 - val_precision: 0.8573 - val_recall: 0.8167 - val_f1score: 0.8356\n",
            "Epoch 91/120\n",
            "20/20 [==============================] - 2s 115ms/step - loss: 0.0234 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1496 - val_accuracy: 0.8403 - val_precision: 0.8564 - val_recall: 0.8167 - val_f1score: 0.8352\n",
            "Epoch 92/120\n",
            "20/20 [==============================] - 2s 114ms/step - loss: 0.0240 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1494 - val_accuracy: 0.8403 - val_precision: 0.8588 - val_recall: 0.8167 - val_f1score: 0.8364\n",
            "Epoch 93/120\n",
            "20/20 [==============================] - 2s 114ms/step - loss: 0.0244 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1493 - val_accuracy: 0.8403 - val_precision: 0.8588 - val_recall: 0.8167 - val_f1score: 0.8364\n",
            "Epoch 94/120\n",
            "20/20 [==============================] - 2s 114ms/step - loss: 0.0248 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1493 - val_accuracy: 0.8403 - val_precision: 0.8588 - val_recall: 0.8167 - val_f1score: 0.8364\n",
            "Epoch 95/120\n",
            "20/20 [==============================] - 2s 114ms/step - loss: 0.0247 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1494 - val_accuracy: 0.8403 - val_precision: 0.8588 - val_recall: 0.8167 - val_f1score: 0.8364\n",
            "Epoch 96/120\n",
            "20/20 [==============================] - 2s 115ms/step - loss: 0.0233 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1494 - val_accuracy: 0.8403 - val_precision: 0.8554 - val_recall: 0.8167 - val_f1score: 0.8348\n",
            "Epoch 97/120\n",
            "20/20 [==============================] - 2s 114ms/step - loss: 0.0244 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1493 - val_accuracy: 0.8403 - val_precision: 0.8554 - val_recall: 0.8167 - val_f1score: 0.8348\n",
            "Epoch 98/120\n",
            "20/20 [==============================] - 2s 114ms/step - loss: 0.0231 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1493 - val_accuracy: 0.8403 - val_precision: 0.8554 - val_recall: 0.8167 - val_f1score: 0.8348\n",
            "Epoch 99/120\n",
            "20/20 [==============================] - 2s 114ms/step - loss: 0.0235 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1491 - val_accuracy: 0.8403 - val_precision: 0.8554 - val_recall: 0.8167 - val_f1score: 0.8348\n",
            "Epoch 100/120\n",
            "20/20 [==============================] - 2s 115ms/step - loss: 0.0241 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1491 - val_accuracy: 0.8403 - val_precision: 0.8554 - val_recall: 0.8167 - val_f1score: 0.8348\n",
            "Epoch 101/120\n",
            "20/20 [==============================] - 2s 115ms/step - loss: 0.0234 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1489 - val_accuracy: 0.8403 - val_precision: 0.8554 - val_recall: 0.8167 - val_f1score: 0.8348\n",
            "Epoch 102/120\n",
            "20/20 [==============================] - 2s 114ms/step - loss: 0.0253 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1491 - val_accuracy: 0.8403 - val_precision: 0.8554 - val_recall: 0.8167 - val_f1score: 0.8348\n",
            "Epoch 103/120\n",
            "20/20 [==============================] - 2s 115ms/step - loss: 0.0246 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1492 - val_accuracy: 0.8403 - val_precision: 0.8554 - val_recall: 0.8167 - val_f1score: 0.8348\n",
            "Epoch 104/120\n",
            "20/20 [==============================] - 2s 116ms/step - loss: 0.0240 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1492 - val_accuracy: 0.8403 - val_precision: 0.8554 - val_recall: 0.8167 - val_f1score: 0.8348\n",
            "Epoch 105/120\n",
            "20/20 [==============================] - 2s 115ms/step - loss: 0.0250 - accuracy: 1.0000 - precision: 0.9981 - recall: 1.0000 - f1score: 0.9991 - val_loss: 0.1491 - val_accuracy: 0.8403 - val_precision: 0.8554 - val_recall: 0.8167 - val_f1score: 0.8348\n",
            "Epoch 106/120\n",
            "20/20 [==============================] - 2s 115ms/step - loss: 0.0235 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1488 - val_accuracy: 0.8403 - val_precision: 0.8554 - val_recall: 0.8167 - val_f1score: 0.8348\n",
            "Epoch 107/120\n",
            "20/20 [==============================] - 2s 114ms/step - loss: 0.0226 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1489 - val_accuracy: 0.8403 - val_precision: 0.8552 - val_recall: 0.8133 - val_f1score: 0.8329\n",
            "Epoch 108/120\n",
            "20/20 [==============================] - 2s 115ms/step - loss: 0.0232 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1488 - val_accuracy: 0.8438 - val_precision: 0.8552 - val_recall: 0.8133 - val_f1score: 0.8329\n",
            "Epoch 109/120\n",
            "20/20 [==============================] - 2s 116ms/step - loss: 0.0225 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1486 - val_accuracy: 0.8438 - val_precision: 0.8552 - val_recall: 0.8133 - val_f1score: 0.8329\n",
            "Epoch 110/120\n",
            "20/20 [==============================] - 2s 115ms/step - loss: 0.0229 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1486 - val_accuracy: 0.8438 - val_precision: 0.8554 - val_recall: 0.8167 - val_f1score: 0.8348\n",
            "Epoch 111/120\n",
            "20/20 [==============================] - 2s 116ms/step - loss: 0.0227 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1486 - val_accuracy: 0.8438 - val_precision: 0.8554 - val_recall: 0.8167 - val_f1score: 0.8348\n",
            "Epoch 112/120\n",
            "20/20 [==============================] - 2s 116ms/step - loss: 0.0238 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1486 - val_accuracy: 0.8438 - val_precision: 0.8554 - val_recall: 0.8167 - val_f1score: 0.8348\n",
            "Epoch 113/120\n",
            "20/20 [==============================] - 2s 116ms/step - loss: 0.0229 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1487 - val_accuracy: 0.8403 - val_precision: 0.8554 - val_recall: 0.8167 - val_f1score: 0.8348\n",
            "Epoch 114/120\n",
            "20/20 [==============================] - 2s 116ms/step - loss: 0.0232 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1489 - val_accuracy: 0.8403 - val_precision: 0.8588 - val_recall: 0.8167 - val_f1score: 0.8364\n",
            "Epoch 115/120\n",
            "20/20 [==============================] - 2s 115ms/step - loss: 0.0239 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1489 - val_accuracy: 0.8438 - val_precision: 0.8552 - val_recall: 0.8133 - val_f1score: 0.8329\n",
            "Epoch 116/120\n",
            "20/20 [==============================] - 2s 115ms/step - loss: 0.0242 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1490 - val_accuracy: 0.8438 - val_precision: 0.8585 - val_recall: 0.8133 - val_f1score: 0.8345\n",
            "Epoch 117/120\n",
            "20/20 [==============================] - 2s 115ms/step - loss: 0.0222 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1490 - val_accuracy: 0.8403 - val_precision: 0.8585 - val_recall: 0.8133 - val_f1score: 0.8345\n",
            "Epoch 118/120\n",
            "20/20 [==============================] - 2s 115ms/step - loss: 0.0242 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1491 - val_accuracy: 0.8403 - val_precision: 0.8585 - val_recall: 0.8133 - val_f1score: 0.8345\n",
            "Epoch 119/120\n",
            "20/20 [==============================] - 2s 115ms/step - loss: 0.0226 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1490 - val_accuracy: 0.8403 - val_precision: 0.8552 - val_recall: 0.8133 - val_f1score: 0.8329\n",
            "Epoch 120/120\n",
            "20/20 [==============================] - 2s 116ms/step - loss: 0.0243 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1490 - val_accuracy: 0.8403 - val_precision: 0.8552 - val_recall: 0.8133 - val_f1score: 0.8329\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkq1yTnE9CYr",
        "outputId": "d6dd4410-569a-4f26-ada8-226f640247e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# output = classifier.predict_generator(test_set, steps=1)\n",
        "# print(test_set.class_indices)\n",
        "# print(output)\n",
        "size = y_test[:,-1]\n",
        "print(size.size)\n",
        "\n",
        "\n",
        "# predict 10 random hand-writing data\n",
        "y_predicted = model.predict(x_test)\n",
        "for x in range(0,size.size):\n",
        "    np.set_printoptions(suppress=True)\n",
        "    print(\"index:\", x,\n",
        "          \" actual y:\", np.argmax(y_test[x]),\n",
        "          \" answer y:\", np.argmax(y_predicted[x]),\n",
        "            \" prediction:\", np.array(y_predicted[x] * 100))\n",
        "\n",
        "_loss, _acc, _precision, _recall, _f1score = model.evaluate(x_test, y_test)\n",
        "print('loss: {:.3f}, accuracy: {:.3f}, precision: {:.3f}, recall: {:.3f}, f1score: {:.3f}'.format(_loss, _acc, _precision, _recall, _f1score))\n",
        "# print('loss: ', evaluation[0])\n",
        "# print('accuracy', evaluation[1])\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 손실 그래프\n",
        "def plot_loss(history):\n",
        "   # 선 그리기\n",
        "    plt.plot(history.history['loss'], 'y', label='train loss')\n",
        "    plt.plot(history.history['val_loss'], 'r', label='val loss')\n",
        "   # 그래프 제목\n",
        "    plt.title('Model Loss')\n",
        "   # x,y축 이름 표시\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "   # 각 라인 표식 표시\n",
        "    plt.legend(['Train','Validation'],loc=0)\n",
        "\n",
        "# 정확도 그래프\n",
        "def plot_acc(history):\n",
        "  # dir(history.history)\n",
        "    plt.plot(history.history['accuracy'], 'b', label='train accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], 'g', label='val accuracy')\n",
        "    plt.title('Model accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc=0)\n",
        "\n",
        "plot_loss(history)\n",
        "plt.show()\n",
        "plot_acc(history)\n",
        "plt.show()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "104\n",
            "index: 0  actual y: 5  answer y: 5  prediction: [ 2.2155118   0.78522146  0.77981746  1.4443141   1.2066171  95.91608   ]\n",
            "index: 1  actual y: 3  answer y: 3  prediction: [ 0.24651484  0.8175222   0.770071   91.941025    3.0059555   6.623601  ]\n",
            "index: 2  actual y: 0  answer y: 0  prediction: [95.642296    0.9740151   2.944135    0.23742545  1.5580027   2.656536  ]\n",
            "index: 3  actual y: 3  answer y: 3  prediction: [ 0.87677425  2.3203964   3.4907846  78.93523     0.48067406  7.8322515 ]\n",
            "index: 4  actual y: 4  answer y: 4  prediction: [ 5.1210523   1.1370631   1.3827604   0.92721564 94.04104     5.025118  ]\n",
            "index: 5  actual y: 0  answer y: 0  prediction: [93.909386    1.3822957   3.2890265   0.37250197  2.0974069   2.4155884 ]\n",
            "index: 6  actual y: 2  answer y: 2  prediction: [ 2.370309   3.488185  73.53576    1.5911499  1.6940988 28.408522 ]\n",
            "index: 7  actual y: 2  answer y: 2  prediction: [ 3.022977   0.4170508 92.80417    1.6573356  4.5069265  4.0688596]\n",
            "index: 8  actual y: 5  answer y: 5  prediction: [ 1.5801513  0.7578148  1.1337055  3.826673   1.0633401 92.465195 ]\n",
            "index: 9  actual y: 2  answer y: 2  prediction: [ 1.4382545   0.31000564 93.297424    2.233249    7.0984015   1.3060571 ]\n",
            "index: 10  actual y: 1  answer y: 1  prediction: [ 2.0446906 97.7782     0.9884018  2.0655758  2.27372    3.2908566]\n",
            "index: 11  actual y: 0  answer y: 0  prediction: [96.73171     0.55543214  1.6261969   0.15069398  0.64896625  2.5814471 ]\n",
            "index: 12  actual y: 4  answer y: 4  prediction: [ 1.1404912 12.07253    3.6489449  3.4856207 82.93961    1.981934 ]\n",
            "index: 13  actual y: 2  answer y: 2  prediction: [ 3.9241076   0.1787204  93.784676    1.8965957   1.2380053   0.89754856]\n",
            "index: 14  actual y: 1  answer y: 1  prediction: [ 2.1126692 97.574776   1.0396523  2.1351569  2.2504532  3.8690336]\n",
            "index: 15  actual y: 3  answer y: 3  prediction: [ 0.44156858  1.5658474   1.4788283  95.34077     0.9115557   7.4290094 ]\n",
            "index: 16  actual y: 3  answer y: 3  prediction: [ 0.2737555  1.0818112  1.7064551 96.10406    0.5808532  4.061909 ]\n",
            "index: 17  actual y: 2  answer y: 5  prediction: [ 3.3985906   0.52807474 26.62142     1.906436    1.2871537  51.158928  ]\n",
            "index: 18  actual y: 0  answer y: 0  prediction: [94.72344     1.0204551   4.053993    0.31077462  0.7885789   1.9593769 ]\n",
            "index: 19  actual y: 4  answer y: 4  prediction: [ 1.5309422  1.0352644  1.5773379  1.1527015 96.760216   3.4749303]\n",
            "index: 20  actual y: 4  answer y: 4  prediction: [ 0.86854196  0.81244826  4.70965     2.2746742  95.333565    2.1745179 ]\n",
            "index: 21  actual y: 0  answer y: 0  prediction: [96.22131     0.71986735  2.3857055   0.23623452  0.941367    2.7929137 ]\n",
            "index: 22  actual y: 2  answer y: 2  prediction: [ 2.4846792   0.18185905 95.29007     2.0570376   2.442006    1.6214483 ]\n",
            "index: 23  actual y: 2  answer y: 2  prediction: [ 2.8486934  1.5262927 65.52025    2.0495963 46.402176   1.3851482]\n",
            "index: 24  actual y: 1  answer y: 1  prediction: [ 1.813655  98.08073    0.9562398  2.2480466  1.7161887  4.062255 ]\n",
            "index: 25  actual y: 1  answer y: 1  prediction: [ 1.618994   97.76346     0.63831174  2.0389774   1.5764335   2.2333157 ]\n",
            "index: 26  actual y: 3  answer y: 3  prediction: [ 1.8651714  1.649297   2.2900887 89.03988    2.1783483  6.802293 ]\n",
            "index: 27  actual y: 2  answer y: 2  prediction: [ 1.7611688  1.2038225 60.66671    3.5054517  2.1464677 34.92547  ]\n",
            "index: 28  actual y: 5  answer y: 3  prediction: [ 0.6624392  1.4159614 11.780767  70.37156    0.6500109  5.84154  ]\n",
            "index: 29  actual y: 0  answer y: 0  prediction: [95.68505     0.8378017   2.7747738   0.22099423  0.86593145  2.4877522 ]\n",
            "index: 30  actual y: 2  answer y: 2  prediction: [11.773099   1.1210158 65.193146   7.204351   2.4618     3.0583797]\n",
            "index: 31  actual y: 0  answer y: 0  prediction: [96.98273     0.8411669   2.0058265   0.1841217   0.95393294  2.6408992 ]\n",
            "index: 32  actual y: 0  answer y: 0  prediction: [96.22769     0.8803483   2.0450103   0.21369106  0.96530336  2.3580344 ]\n",
            "index: 33  actual y: 1  answer y: 1  prediction: [ 1.9922969 97.13874    0.9061359  1.8549651  2.1081908  4.0263643]\n",
            "index: 34  actual y: 4  answer y: 4  prediction: [ 1.122554   1.4648947  3.9961777  1.2796174 95.2163     1.9430615]\n",
            "index: 35  actual y: 1  answer y: 1  prediction: [ 1.7899843  97.58929     0.86366785  2.1145864   2.3640628   2.620023  ]\n",
            "index: 36  actual y: 1  answer y: 1  prediction: [ 1.7093953 97.797874   0.9220943  2.324561   2.1054235  2.9864678]\n",
            "index: 37  actual y: 5  answer y: 5  prediction: [ 2.1307771   0.61458635  0.7540183   1.5042459   1.1288483  95.23531   ]\n",
            "index: 38  actual y: 5  answer y: 5  prediction: [ 2.0398743   0.8033651   0.98573613  1.4691046   1.0920267  95.21323   ]\n",
            "index: 39  actual y: 5  answer y: 5  prediction: [ 4.5073743   0.84454995  0.7607428   1.7032484   0.69649863 94.08723   ]\n",
            "index: 40  actual y: 5  answer y: 5  prediction: [ 2.2542026  1.0405436  1.2211468  1.8552415  1.4248502 95.122215 ]\n",
            "index: 41  actual y: 0  answer y: 0  prediction: [80.659904    0.6689539  25.409344    1.0628637   0.95451295  1.4986403 ]\n",
            "index: 42  actual y: 5  answer y: 5  prediction: [ 2.166494    1.2654877   0.83080745  1.2488848   0.6670458  95.956406  ]\n",
            "index: 43  actual y: 2  answer y: 2  prediction: [ 3.487461    0.20954062 93.1062      2.8265805   1.055618    0.77372324]\n",
            "index: 44  actual y: 3  answer y: 3  prediction: [ 0.695953   7.294864   1.4303524 80.53439    1.9409567  7.8551807]\n",
            "index: 45  actual y: 0  answer y: 0  prediction: [88.50553     0.75197285 12.796776    0.33845004  1.6104891   3.8947031 ]\n",
            "index: 46  actual y: 4  answer y: 4  prediction: [ 4.9427543   0.72555584  6.6709714   1.5041937  82.140045    7.7223496 ]\n",
            "index: 47  actual y: 5  answer y: 5  prediction: [ 2.583891    0.82395214  0.91242576  0.85830986  0.97070456 96.69602   ]\n",
            "index: 48  actual y: 4  answer y: 5  prediction: [ 3.254532    1.0074091   0.65011984 14.330229    7.036455   77.2503    ]\n",
            "index: 49  actual y: 5  answer y: 5  prediction: [ 1.1357318   0.49965465  0.84967166  6.4404206   0.7505799  89.02358   ]\n",
            "index: 50  actual y: 2  answer y: 2  prediction: [ 7.0499434   0.40891692 68.60865     5.369696    3.5401757   4.888228  ]\n",
            "index: 51  actual y: 5  answer y: 4  prediction: [ 0.75515926  1.9850252  19.87995    29.604145   32.030087    3.4166555 ]\n",
            "index: 52  actual y: 1  answer y: 4  prediction: [ 1.1702905  0.9808502  3.0608222  1.0052468 96.12689    1.8145176]\n",
            "index: 53  actual y: 0  answer y: 0  prediction: [95.617256   0.9092781  2.7532997  0.3105146  1.4517858  3.005393 ]\n",
            "index: 54  actual y: 3  answer y: 3  prediction: [ 0.32994616  1.1692216   1.4350702  94.93858     0.67341745  4.3626065 ]\n",
            "index: 55  actual y: 2  answer y: 2  prediction: [ 2.2215917   0.16598502 95.805084    1.8038174   1.5159647   1.1203141 ]\n",
            "index: 56  actual y: 5  answer y: 3  prediction: [ 0.574178   6.4331784  2.0816212 34.161945  10.956497  15.582718 ]\n",
            "index: 57  actual y: 0  answer y: 0  prediction: [96.520905    0.8956656   1.8631201   0.23033275  0.8546601   3.1529062 ]\n",
            "index: 58  actual y: 3  answer y: 3  prediction: [ 0.2861274   1.3797877   1.9509481  96.16728     0.47063416  3.7749133 ]\n",
            "index: 59  actual y: 3  answer y: 3  prediction: [ 0.32415926  1.1550118   1.3716841  94.91731     1.0251485   4.1662316 ]\n",
            "index: 60  actual y: 3  answer y: 2  prediction: [11.62367    4.2830358 23.626503  23.581158  15.652466   3.2208583]\n",
            "index: 61  actual y: 3  answer y: 3  prediction: [ 0.3708641  5.1594176  1.9120464 85.39092    1.2010247  9.088753 ]\n",
            "index: 62  actual y: 4  answer y: 4  prediction: [ 1.3371325  1.259976   2.244277   1.3567501 96.2833     2.5660732]\n",
            "index: 63  actual y: 3  answer y: 3  prediction: [ 4.7332006   1.9784411   1.6834137  67.75598     0.66774875  7.2377625 ]\n",
            "index: 64  actual y: 3  answer y: 3  prediction: [ 0.3521313  1.2475287  1.7726008 95.74236    0.6990751  4.588363 ]\n",
            "index: 65  actual y: 0  answer y: 0  prediction: [84.16869    1.7098112  6.4447546  1.1772693  2.3196416  2.1669552]\n",
            "index: 66  actual y: 3  answer y: 3  prediction: [ 0.2732072   0.740211    0.99656665 96.25055     0.4650952   3.4534752 ]\n",
            "index: 67  actual y: 3  answer y: 3  prediction: [ 0.2306063  2.5744236  1.3895053 93.66303    1.2653899  4.2203417]\n",
            "index: 68  actual y: 3  answer y: 4  prediction: [ 2.0101054  2.0249553  2.8187704 25.084507  58.14771    3.8653772]\n",
            "index: 69  actual y: 4  answer y: 4  prediction: [ 1.1652703  0.7139394  1.4057572  1.3722985 96.8021     2.4498885]\n",
            "index: 70  actual y: 2  answer y: 2  prediction: [ 2.4683647  0.2534968 93.90549    2.4411578  1.2383132  1.5608875]\n",
            "index: 71  actual y: 4  answer y: 4  prediction: [ 1.3854765   0.92541754  2.1748602   0.8574943  96.794655    2.1916025 ]\n",
            "index: 72  actual y: 0  answer y: 3  prediction: [ 6.297414   1.1564525  1.3915086 70.33461    1.2717277  6.502876 ]\n",
            "index: 73  actual y: 3  answer y: 3  prediction: [ 0.6676726   1.3478758   2.5992386  92.439064    0.84372854  5.8442545 ]\n",
            "index: 74  actual y: 3  answer y: 3  prediction: [ 0.39702106  1.3294995   1.8933634  95.45974     0.68734574  4.758827  ]\n",
            "index: 75  actual y: 0  answer y: 0  prediction: [95.88048     0.7237001   2.25637     0.19039805  0.9384962   2.0868318 ]\n",
            "index: 76  actual y: 0  answer y: 0  prediction: [96.43584     1.1645391   2.5549626   0.28909564  0.86267877  3.102176  ]\n",
            "index: 77  actual y: 1  answer y: 1  prediction: [ 2.1705472 83.18039    2.0806048  9.38498    3.632941   4.6621637]\n",
            "index: 78  actual y: 2  answer y: 2  prediction: [ 2.2795658   0.23946339 94.89878     1.5883611   1.5970508   1.3673577 ]\n",
            "index: 79  actual y: 5  answer y: 5  prediction: [ 1.2406206  3.990323   2.747991  15.795019  11.439891  47.092457 ]\n",
            "index: 80  actual y: 2  answer y: 2  prediction: [ 2.2197344   0.18545972 95.071915    1.432171    1.877242    1.6362017 ]\n",
            "index: 81  actual y: 3  answer y: 3  prediction: [ 0.3313901  1.1958388  1.3194021 94.899414   0.85934    3.9213784]\n",
            "index: 82  actual y: 4  answer y: 4  prediction: [ 1.2724991  3.2472544  1.4122314  1.2440565 95.120895   2.239112 ]\n",
            "index: 83  actual y: 5  answer y: 5  prediction: [ 2.3758848  0.6128047  0.7846239  1.5609131  0.9975641 95.484245 ]\n",
            "index: 84  actual y: 4  answer y: 4  prediction: [ 1.0576605   0.98842     1.7664129   0.77239555 97.2222      2.246444  ]\n",
            "index: 85  actual y: 2  answer y: 2  prediction: [ 2.4046366   0.21475458 95.170715    1.942171    3.1167314   2.6934934 ]\n",
            "index: 86  actual y: 4  answer y: 4  prediction: [ 1.6055303  0.9509492  1.1401123  0.7905459 97.034836   2.3477235]\n",
            "index: 87  actual y: 1  answer y: 1  prediction: [ 2.5088897  89.396706    0.64936817  7.7056513   1.5678147   3.3499267 ]\n",
            "index: 88  actual y: 5  answer y: 5  prediction: [ 1.7254019   1.4378226   0.7728708   1.1483057   0.83231664 96.11248   ]\n",
            "index: 89  actual y: 1  answer y: 3  prediction: [ 0.31130457  3.5087104   1.2879076  93.00261     1.0868847   5.36435   ]\n",
            "index: 90  actual y: 3  answer y: 3  prediction: [ 0.29435402  1.3076328   1.8949512  95.67336     0.5886259   4.252651  ]\n",
            "index: 91  actual y: 4  answer y: 4  prediction: [ 1.1179513  0.9833305  2.131331   1.1736667 96.49161    1.9563208]\n",
            "index: 92  actual y: 5  answer y: 5  prediction: [ 1.6530534   0.80081785  0.8534844   6.4527044   0.7843994  87.5329    ]\n",
            "index: 93  actual y: 1  answer y: 1  prediction: [ 1.8840294 88.62167    1.1809698  1.413439   6.893922   3.8504865]\n",
            "index: 94  actual y: 1  answer y: 1  prediction: [ 2.0523024  98.1235      0.92065686  2.3531933   1.8573482   3.3634913 ]\n",
            "index: 95  actual y: 4  answer y: 4  prediction: [ 1.7867422  1.0963207  1.9790102  1.8694218 95.42697    5.0112085]\n",
            "index: 96  actual y: 4  answer y: 4  prediction: [ 1.605531    0.9509492   1.1401123   0.79054505 97.034836    2.3477242 ]\n",
            "index: 97  actual y: 1  answer y: 1  prediction: [ 2.0016642  96.95562     0.68822426  2.3813183   2.188819    1.8923191 ]\n",
            "index: 98  actual y: 4  answer y: 4  prediction: [ 0.95730525  0.8466913   2.6728873   1.6192393  95.87831     1.9540293 ]\n",
            "index: 99  actual y: 4  answer y: 4  prediction: [ 0.9312916  0.6737793  1.4974856  1.15456   97.01418    2.3303998]\n",
            "index: 100  actual y: 1  answer y: 3  prediction: [ 1.3479475 34.737396   1.3618672 58.586727   1.3445132  6.9677725]\n",
            "index: 101  actual y: 0  answer y: 0  prediction: [95.5508      0.56781375  3.5028768   0.27824634  1.3118124   3.1575077 ]\n",
            "index: 102  actual y: 5  answer y: 5  prediction: [ 2.1086273  0.857983   0.9824909  1.0614955  1.3501757 96.0006   ]\n",
            "index: 103  actual y: 0  answer y: 0  prediction: [80.5297      0.4117081   4.757227    0.33223522  0.96031374 19.860624  ]\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.1003 - accuracy: 0.8942 - precision: 0.9117 - recall: 0.8828 - f1score: 0.8967\n",
            "loss: 0.100, accuracy: 0.894, precision: 0.912, recall: 0.883, f1score: 0.897\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGR0eJBKxiG_"
      },
      "source": [
        ""
      ],
      "execution_count": 46,
      "outputs": []
    }
  ]
}