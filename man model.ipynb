{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-dgO35Q0iPCC",
    "outputId": "ca67647d-e102-451a-8d27-02db2bbc47f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batchSIZE is 512, Learning Rate is 0.001\n",
      "Found 845 images belonging to 6 classes.\n",
      "Found 104 images belonging to 6 classes.\n",
      "Found 258 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os # miscellaneous operating system interfaces\n",
    "import shutil # high-level file operations\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import random\n",
    "from itertools import product\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Conv2D \n",
    "from keras.layers import MaxPooling2D \n",
    "from keras.layers import Flatten \n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.applications import MobileNetV2, Xception, DenseNet121,ResNet50V2,NASNetMobile\n",
    "from keras.applications.mobilenet_v2 import preprocess_input\n",
    "from keras.layers import  Input, Conv2D, Conv2DTranspose, ReLU,AveragePooling2D, MaxPooling2D, Dense, BatchNormalization, GlobalAveragePooling2D, Concatenate\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "\n",
    "base_dir = \"/content/drive/My Drive/Colab Notebooks/datasets/man\"\n",
    "\n",
    "train_img_dir = \"/content/drive/My Drive/Colab Notebooks/datasets/man/train\"\n",
    "\n",
    "test_img_dir = \"/content/drive/My Drive/Colab Notebooks/datasets/man/test\"\n",
    "\n",
    "val_img_dir = \"/content/drive/My Drive/Colab Notebooks/datasets/man/val\"\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "training_epochs = 20 # traindata개수/batchsize\n",
    "batch_size = 512\n",
    "validation_steps = 20 # valdata개수/batchsize\n",
    "\n",
    "\n",
    "\n",
    "print(f'batchSIZE is {batch_size}, Learning Rate is {learning_rate}')\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "val_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "categories = ['dog','cat','bear','hamster','horse','wolf']\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(train_img_dir, target_size=(128,128), \n",
    "                                             classes=categories, \n",
    "                                             batch_size=batch_size)\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(test_img_dir,\n",
    "                                        target_size=(128,128), \n",
    "                                        classes=categories, \n",
    "                                        batch_size=batch_size)\n",
    "\n",
    "val_set = test_datagen.flow_from_directory(val_img_dir,\n",
    "                                        target_size=(128,128), \n",
    "                                        classes=categories, \n",
    "                                        batch_size=batch_size)\n",
    "\n",
    "x_train, y_train = next(training_set)\n",
    "x_test, y_test = next(test_set)\n",
    "x_val,y_val = next(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "RzBS39E1jIg4"
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "    \n",
    "\n",
    "    conv2_1 = Conv2D(32, 3, 1, 'SAME')(inputs)\n",
    "    conv2_2 = Conv2D(32, 3, 1, 'SAME')(conv2_1)\n",
    "    pool2_3 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv2_2)\n",
    "    br1 = BatchNormalization()(pool2_3)\n",
    "    \n",
    "    conv2_1 = Conv2D(32, 3, 1, 'SAME')(br1)\n",
    "    conv2_2 = Conv2D(32, 3, 1, 'SAME')(conv2_1)\n",
    "    pool2_3 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv2_2)\n",
    "    br1 = BatchNormalization()(pool2_3)\n",
    "    \n",
    "    \n",
    "    conv3_1 = Conv2D(64, 3, 1, 'SAME')(br1)\n",
    "    conv3_2 = Conv2D(64, 3, 1, 'SAME')(conv3_1)\n",
    "    pool3_2 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv3_2)\n",
    "    br2 = BatchNormalization()(pool3_2)\n",
    "    \n",
    "    conv3_1 = Conv2D(64, 3, 1, 'SAME')(br2)\n",
    "    conv3_2 = Conv2D(64, 3, 1, 'SAME')(conv3_1)\n",
    "    pool3_2 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv3_2)\n",
    "    br2 = BatchNormalization()(pool3_2)\n",
    "    \n",
    "    \n",
    "    conv4_1 = Conv2D(128, 3, 1, 'SAME')(br2)\n",
    "    conv4_2 = Conv2D(128, 3, 1, 'SAME')(conv4_1)\n",
    "    pool4_2 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv4_2)\n",
    "    br3 = BatchNormalization()(pool4_2)\n",
    "    \n",
    "    conv4_1 = Conv2D(128, 3, 1, 'SAME')(br3)\n",
    "    conv4_2 = Conv2D(128, 3, 1, 'SAME')(conv4_1)\n",
    "    pool4_2 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv4_2)\n",
    "    br3 = BatchNormalization()(pool4_2)\n",
    "    \n",
    "    flatten1 = Flatten()(pool4_2)\n",
    "    dense1 = Dense(units = 512, activation = 'relu')(flatten1)\n",
    "    dense2 = Dense(units = 1024, activation = 'relu')(dense1)\n",
    "    dr1 = Dropout(0.7)(dense2)\n",
    "    dense3 = Dense(units = 5, activation = 'sigmoid')(dr1)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=dense3) \n",
    "def mobile_net():\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "    mobileNet = MobileNetV2(weights = \"imagenet\", include_top=False,input_shape = (128,128,3)\n",
    "                            ,input_tensor = inputs)\n",
    "    for layer in mobileNet.layers:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    output = mobileNet.output\n",
    "    pooling = AveragePooling2D(pool_size=(16,16),padding='SAME')(output)\n",
    "    flatten1 = Flatten()(pooling)\n",
    "    dense1 = Dense(units = 256)(flatten1)\n",
    "    batch1 = BatchNormalization()(dense1)\n",
    "    relu1 = ReLU()(batch1)\n",
    "    dense2 = Dense(units = 64)(relu1)\n",
    "    batch2 = BatchNormalization()(dense2)\n",
    "    relu2 = ReLU()(batch2)\n",
    "    dense3 = Dense(units = 6, activation = 'sigmoid')(relu2)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=dense3)\n",
    "\n",
    "def xception():\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "    xception = Xception(weights = \"imagenet\", include_top=False,input_shape = (128,128,3)\n",
    "                            ,input_tensor = inputs)\n",
    "    for layer in xception.layers:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    output = xception.output\n",
    "    pooling = AveragePooling2D(pool_size=(8,8),padding='SAME')(output)\n",
    "    flatten1 = Flatten()(pooling)\n",
    "    dense1 = Dense(units = 256)(flatten1)\n",
    "    batch1 = BatchNormalization()(dense1)\n",
    "    relu1 = ReLU()(batch1)\n",
    "    dense2 = Dense(units = 64)(relu1)\n",
    "    batch2 = BatchNormalization()(dense2)\n",
    "    relu2 = ReLU()(batch2)\n",
    "    dense3 = Dense(units = 6, activation = 'sigmoid')(relu2)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=dense3)\n",
    "    # pooling = AveragePooling2D(pool_size=(4,4),padding='SAME')(output)\n",
    "    \n",
    "    # flatten1 = Flatten()(pooling)\n",
    "    # dense1 = Dense(units = 512, activation = 'relu')(flatten1)\n",
    "    # dense2 = Dense(units = 1024, activation = 'relu')(dense1)\n",
    "    # dr1 = Dropout(0.7)(dense2)\n",
    "    # dense3 = Dense(units = 5, activation = 'sigmoid')(dr1)\n",
    "    \n",
    "    # return keras.Model(inputs=inputs, outputs=dense3)\n",
    "\n",
    "def resnet():\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "    resnet = ResNet50V2 (weights = \"imagenet\", include_top=False,input_shape = (128,128,3)\n",
    "                            ,input_tensor = inputs)\n",
    "    for layer in resnet.layers:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    output = resnet.output\n",
    "    pooling = AveragePooling2D(pool_size=(8,8),padding='SAME')(output)\n",
    "    flatten1 = Flatten()(pooling)\n",
    "    dense1 = Dense(units = 256)(flatten1)\n",
    "    batch1 = BatchNormalization()(dense1)\n",
    "    relu1 = ReLU()(batch1)\n",
    "    dense2 = Dense(units = 64)(relu1)\n",
    "    batch2 = BatchNormalization()(dense2)\n",
    "    relu2 = ReLU()(batch2)\n",
    "    dense3 = Dense(units = 6, activation = 'sigmoid')(relu2)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=dense3)\n",
    "\n",
    "def densenet():\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "    densenet = DenseNet121(weights = \"imagenet\", include_top=False,input_shape = (128,128,3)\n",
    "                            ,input_tensor = inputs)\n",
    "    for layer in densenet.layers:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    output = densenet.output\n",
    "    pooling = AveragePooling2D(pool_size=(16,16),padding='SAME')(output)\n",
    "    flatten1 = Flatten()(pooling)\n",
    "    dense1 = Dense(units = 128)(flatten1)\n",
    "    batch1 = BatchNormalization()(dense1)\n",
    "    relu1 = ReLU()(batch1)\n",
    "    dense2 = Dense(units = 32)(relu1)\n",
    "    batch2 = BatchNormalization()(dense2)\n",
    "    relu2 = ReLU()(batch2)\n",
    "    dense3 = Dense(units = 6, activation = 'sigmoid')(relu2)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=dense3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ResidualUnit(tf.keras.Model):\n",
    "    def __init__(self, filter_in, filter_out, kernel_size):\n",
    "        super(ResidualUnit, self).__init__()\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filter_out, kernel_size, padding='same')\n",
    "        \n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filter_out, kernel_size, padding='same')\n",
    "        \n",
    "        if filter_in == filter_out:\n",
    "            self.identity = lambda x: x\n",
    "        else:\n",
    "            self.identity = tf.keras.layers.Conv2D(filter_out, (1,1), padding='same')\n",
    "\n",
    "    def call(self, x, training=False, mask=None):\n",
    "        h = self.bn1(x, training=training)\n",
    "        h = tf.nn.relu(h)\n",
    "        h = self.conv1(h)\n",
    "        \n",
    "        h = self.bn2(h, training=training)\n",
    "        h = tf.nn.relu(h)\n",
    "        h = self.conv2(h)\n",
    "        return self.identity(x) + h\n",
    "    \n",
    "class ResnetLayer(tf.keras.Model):\n",
    "    def __init__(self, filter_in, filters, kernel_size):\n",
    "        super(ResnetLayer, self).__init__()\n",
    "        self.sequence = list()\n",
    "        for f_in, f_out in zip([filter_in] + list(filters), filters):\n",
    "            self.sequence.append(ResidualUnit(f_in, f_out, kernel_size))\n",
    "\n",
    "    def call(self, x, training=False, mask=None):\n",
    "        for unit in self.sequence:\n",
    "            x = unit(x, training=training)\n",
    "        return x\n",
    "    \n",
    "class ResNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu') # 28x28x8\n",
    "        \n",
    "        self.res1 = ResnetLayer(64, (16, 16), (3, 3)) # 28x28x16\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D((2, 2)) # 14x14x16\n",
    "        \n",
    "        \n",
    "        self.res2 = ResnetLayer(128, (32, 32), (3, 3)) # 14x14x32\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D((2, 2)) # 7x7x32\n",
    "    \n",
    "        \n",
    "        self.res3 = ResnetLayer(256, (64, 64), (3, 3)) # 7x7x64\n",
    "        self.pool = tf.keras.layers.MaxPool2D((2, 2)) # 7x7x32\n",
    "        \n",
    "        \n",
    "        self.res4 = ResnetLayer(512, (64, 64), (3, 3)) # 7x7x64\n",
    "        self.pool = tf.keras.layers.MaxPool2D((2, 2)) # 7x7x32\n",
    "        \n",
    "        \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(1024, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(5, activation='softmax')\n",
    "        \n",
    "    def call(self, x, training=False, mask=None):\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        x = self.res1(x, training=training)\n",
    "        x = self.pool1(x)\n",
    "        x = self.res2(x, training=training)\n",
    "        x = self.pool2(x)\n",
    "        x = self.res3(x, training=training)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        return self.dense2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2F8dolSzlByA",
    "outputId": "95882e07-4b27-48db-e4f6-34e9ef298bb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "20/20 [==============================] - 3s 157ms/step - loss: 0.6731 - accuracy: 0.3809 - precision: 0.2587 - recall: 0.6968 - f1score: 0.3765 - val_loss: 1.5002 - val_accuracy: 0.1977 - val_precision: 0.1670 - val_recall: 0.6003 - val_f1score: 0.2612\n",
      "Epoch 2/120\n",
      "20/20 [==============================] - 2s 82ms/step - loss: 0.4943 - accuracy: 0.6016 - precision: 0.4269 - recall: 0.8265 - f1score: 0.5618 - val_loss: 0.7223 - val_accuracy: 0.2093 - val_precision: 0.1812 - val_recall: 0.4003 - val_f1score: 0.2489\n",
      "Epoch 3/120\n",
      "20/20 [==============================] - 2s 81ms/step - loss: 0.3746 - accuracy: 0.7637 - precision: 0.6262 - recall: 0.8502 - f1score: 0.7194 - val_loss: 0.5074 - val_accuracy: 0.2636 - val_precision: 0.2837 - val_recall: 0.2997 - val_f1score: 0.2904\n",
      "Epoch 4/120\n",
      "20/20 [==============================] - 2s 82ms/step - loss: 0.2761 - accuracy: 0.8848 - precision: 0.8096 - recall: 0.9271 - f1score: 0.8629 - val_loss: 0.4916 - val_accuracy: 0.3178 - val_precision: 0.3220 - val_recall: 0.3895 - val_f1score: 0.3519\n",
      "Epoch 5/120\n",
      "20/20 [==============================] - 2s 81ms/step - loss: 0.2253 - accuracy: 0.9102 - precision: 0.8881 - recall: 0.9241 - f1score: 0.9053 - val_loss: 0.6723 - val_accuracy: 0.2442 - val_precision: 0.2740 - val_recall: 0.3119 - val_f1score: 0.2913\n",
      "Epoch 6/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.1795 - accuracy: 0.9512 - precision: 0.9402 - recall: 0.9607 - f1score: 0.9499 - val_loss: 0.6274 - val_accuracy: 0.3605 - val_precision: 0.3694 - val_recall: 0.3881 - val_f1score: 0.3783\n",
      "Epoch 7/120\n",
      "20/20 [==============================] - 2s 81ms/step - loss: 0.1462 - accuracy: 0.9629 - precision: 0.9522 - recall: 0.9577 - f1score: 0.9548 - val_loss: 0.3466 - val_accuracy: 0.6512 - val_precision: 0.6354 - val_recall: 0.6549 - val_f1score: 0.6443\n",
      "Epoch 8/120\n",
      "20/20 [==============================] - 2s 82ms/step - loss: 0.1411 - accuracy: 0.9395 - precision: 0.9388 - recall: 0.9378 - f1score: 0.9381 - val_loss: 0.3343 - val_accuracy: 0.6628 - val_precision: 0.6627 - val_recall: 0.6748 - val_f1score: 0.6682\n",
      "Epoch 9/120\n",
      "20/20 [==============================] - 2s 82ms/step - loss: 0.1339 - accuracy: 0.9375 - precision: 0.9336 - recall: 0.9321 - f1score: 0.9325 - val_loss: 0.5523 - val_accuracy: 0.3605 - val_precision: 0.3606 - val_recall: 0.3451 - val_f1score: 0.3521\n",
      "Epoch 10/120\n",
      "20/20 [==============================] - 2s 82ms/step - loss: 0.1280 - accuracy: 0.9199 - precision: 0.9217 - recall: 0.9158 - f1score: 0.9185 - val_loss: 0.5456 - val_accuracy: 0.2752 - val_precision: 0.2809 - val_recall: 0.2636 - val_f1score: 0.2716\n",
      "Epoch 11/120\n",
      "20/20 [==============================] - 2s 82ms/step - loss: 0.1148 - accuracy: 0.9316 - precision: 0.9326 - recall: 0.9194 - f1score: 0.9256 - val_loss: 0.6154 - val_accuracy: 0.2713 - val_precision: 0.2826 - val_recall: 0.2836 - val_f1score: 0.2828\n",
      "Epoch 12/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0858 - accuracy: 0.9766 - precision: 0.9744 - recall: 0.9647 - f1score: 0.9694 - val_loss: 0.2571 - val_accuracy: 0.7171 - val_precision: 0.7255 - val_recall: 0.7364 - val_f1score: 0.7302\n",
      "Epoch 13/120\n",
      "20/20 [==============================] - 2s 81ms/step - loss: 0.0721 - accuracy: 0.9941 - precision: 0.9961 - recall: 0.9885 - f1score: 0.9922 - val_loss: 0.2315 - val_accuracy: 0.7519 - val_precision: 0.7557 - val_recall: 0.7479 - val_f1score: 0.7513\n",
      "Epoch 14/120\n",
      "20/20 [==============================] - 2s 82ms/step - loss: 0.0645 - accuracy: 0.9980 - precision: 0.9962 - recall: 0.9981 - f1score: 0.9971 - val_loss: 0.1729 - val_accuracy: 0.8450 - val_precision: 0.8465 - val_recall: 0.8472 - val_f1score: 0.8464\n",
      "Epoch 15/120\n",
      "20/20 [==============================] - 2s 82ms/step - loss: 0.0588 - accuracy: 0.9961 - precision: 0.9981 - recall: 0.9981 - f1score: 0.9981 - val_loss: 0.1908 - val_accuracy: 0.8023 - val_precision: 0.8036 - val_recall: 0.7909 - val_f1score: 0.7967\n",
      "Epoch 16/120\n",
      "20/20 [==============================] - 2s 82ms/step - loss: 0.0532 - accuracy: 0.9980 - precision: 0.9981 - recall: 0.9981 - f1score: 0.9981 - val_loss: 0.1640 - val_accuracy: 0.8295 - val_precision: 0.8465 - val_recall: 0.8248 - val_f1score: 0.8350\n",
      "Epoch 17/120\n",
      "20/20 [==============================] - 2s 82ms/step - loss: 0.0547 - accuracy: 0.9922 - precision: 0.9895 - recall: 0.9917 - f1score: 0.9905 - val_loss: 0.1490 - val_accuracy: 0.8605 - val_precision: 0.8627 - val_recall: 0.8486 - val_f1score: 0.8548\n",
      "Epoch 18/120\n",
      "20/20 [==============================] - 2s 82ms/step - loss: 0.0502 - accuracy: 0.9941 - precision: 0.9942 - recall: 0.9904 - f1score: 0.9922 - val_loss: 0.3828 - val_accuracy: 0.4922 - val_precision: 0.5298 - val_recall: 0.4720 - val_f1score: 0.4976\n",
      "Epoch 19/120\n",
      "20/20 [==============================] - 2s 82ms/step - loss: 0.0480 - accuracy: 0.9961 - precision: 0.9944 - recall: 0.9923 - f1score: 0.9933 - val_loss: 0.1483 - val_accuracy: 0.8488 - val_precision: 0.8719 - val_recall: 0.8318 - val_f1score: 0.8500\n",
      "Epoch 20/120\n",
      "20/20 [==============================] - 2s 82ms/step - loss: 0.0432 - accuracy: 1.0000 - precision: 1.0000 - recall: 0.9981 - f1score: 0.9990 - val_loss: 0.1453 - val_accuracy: 0.8760 - val_precision: 0.8802 - val_recall: 0.8563 - val_f1score: 0.8674\n",
      "Epoch 21/120\n",
      "20/20 [==============================] - 2s 82ms/step - loss: 0.0423 - accuracy: 1.0000 - precision: 1.0000 - recall: 0.9962 - f1score: 0.9980 - val_loss: 0.1450 - val_accuracy: 0.8682 - val_precision: 0.8855 - val_recall: 0.8479 - val_f1score: 0.8650\n",
      "Epoch 22/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0405 - accuracy: 0.9980 - precision: 1.0000 - recall: 0.9962 - f1score: 0.9980 - val_loss: 0.1526 - val_accuracy: 0.8411 - val_precision: 0.8531 - val_recall: 0.8287 - val_f1score: 0.8402\n",
      "Epoch 23/120\n",
      "20/20 [==============================] - 2s 82ms/step - loss: 0.0392 - accuracy: 0.9961 - precision: 0.9962 - recall: 0.9934 - f1score: 0.9947 - val_loss: 0.1151 - val_accuracy: 0.8915 - val_precision: 0.9064 - val_recall: 0.8916 - val_f1score: 0.8987\n",
      "Epoch 24/120\n",
      "20/20 [==============================] - 2s 82ms/step - loss: 0.0396 - accuracy: 0.9980 - precision: 0.9962 - recall: 0.9981 - f1score: 0.9971 - val_loss: 0.1384 - val_accuracy: 0.8566 - val_precision: 0.8848 - val_recall: 0.8395 - val_f1score: 0.8605\n",
      "Epoch 25/120\n",
      "20/20 [==============================] - 2s 82ms/step - loss: 0.0362 - accuracy: 1.0000 - precision: 1.0000 - recall: 0.9981 - f1score: 0.9990 - val_loss: 0.1800 - val_accuracy: 0.7946 - val_precision: 0.8103 - val_recall: 0.7741 - val_f1score: 0.7911\n",
      "Epoch 26/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0344 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1521 - val_accuracy: 0.8295 - val_precision: 0.8413 - val_recall: 0.8248 - val_f1score: 0.8327\n",
      "Epoch 27/120\n",
      "20/20 [==============================] - 2s 82ms/step - loss: 0.0363 - accuracy: 0.9961 - precision: 0.9972 - recall: 0.9953 - f1score: 0.9962 - val_loss: 0.1276 - val_accuracy: 0.8643 - val_precision: 0.8930 - val_recall: 0.8434 - val_f1score: 0.8663\n",
      "Epoch 28/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0340 - accuracy: 1.0000 - precision: 1.0000 - recall: 0.9981 - f1score: 0.9990 - val_loss: 0.1388 - val_accuracy: 0.8411 - val_precision: 0.8783 - val_recall: 0.8203 - val_f1score: 0.8461\n",
      "Epoch 29/120\n",
      "20/20 [==============================] - 2s 82ms/step - loss: 0.0381 - accuracy: 0.9941 - precision: 0.9942 - recall: 0.9942 - f1score: 0.9942 - val_loss: 0.1827 - val_accuracy: 0.7829 - val_precision: 0.8103 - val_recall: 0.7549 - val_f1score: 0.7804\n",
      "Epoch 30/120\n",
      "20/20 [==============================] - 2s 82ms/step - loss: 0.0342 - accuracy: 0.9980 - precision: 0.9981 - recall: 1.0000 - f1score: 0.9991 - val_loss: 0.1734 - val_accuracy: 0.7829 - val_precision: 0.8199 - val_recall: 0.7710 - val_f1score: 0.7938\n",
      "Epoch 31/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0344 - accuracy: 0.9980 - precision: 0.9981 - recall: 0.9981 - f1score: 0.9981 - val_loss: 0.1347 - val_accuracy: 0.8682 - val_precision: 0.8894 - val_recall: 0.8479 - val_f1score: 0.8668\n",
      "Epoch 32/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0344 - accuracy: 0.9961 - precision: 0.9962 - recall: 0.9962 - f1score: 0.9962 - val_loss: 0.1272 - val_accuracy: 0.8643 - val_precision: 0.8872 - val_recall: 0.8524 - val_f1score: 0.8687\n",
      "Epoch 33/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0307 - accuracy: 0.9980 - precision: 0.9981 - recall: 0.9981 - f1score: 0.9981 - val_loss: 0.1124 - val_accuracy: 0.8953 - val_precision: 0.9065 - val_recall: 0.8794 - val_f1score: 0.8919\n",
      "Epoch 34/120\n",
      "20/20 [==============================] - 2s 82ms/step - loss: 0.0286 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1170 - val_accuracy: 0.8837 - val_precision: 0.9013 - val_recall: 0.8549 - val_f1score: 0.8762\n",
      "Epoch 35/120\n",
      "20/20 [==============================] - 2s 82ms/step - loss: 0.0281 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1100 - val_accuracy: 0.9031 - val_precision: 0.9117 - val_recall: 0.8909 - val_f1score: 0.9001\n",
      "Epoch 36/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0303 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1099 - val_accuracy: 0.8876 - val_precision: 0.9019 - val_recall: 0.8717 - val_f1score: 0.8857\n",
      "Epoch 37/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0280 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1129 - val_accuracy: 0.8760 - val_precision: 0.8957 - val_recall: 0.8633 - val_f1score: 0.8779\n",
      "Epoch 38/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0263 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1130 - val_accuracy: 0.8643 - val_precision: 0.8874 - val_recall: 0.8594 - val_f1score: 0.8719\n",
      "Epoch 39/120\n",
      "20/20 [==============================] - 2s 82ms/step - loss: 0.0261 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1129 - val_accuracy: 0.8643 - val_precision: 0.8906 - val_recall: 0.8594 - val_f1score: 0.8735\n",
      "Epoch 40/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0282 - accuracy: 0.9980 - precision: 1.0000 - recall: 0.9972 - f1score: 0.9986 - val_loss: 0.1094 - val_accuracy: 0.8798 - val_precision: 0.8884 - val_recall: 0.8671 - val_f1score: 0.8771\n",
      "Epoch 41/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0252 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1088 - val_accuracy: 0.8798 - val_precision: 0.8996 - val_recall: 0.8710 - val_f1score: 0.8847\n",
      "Epoch 42/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0259 - accuracy: 0.9980 - precision: 0.9981 - recall: 0.9981 - f1score: 0.9981 - val_loss: 0.1049 - val_accuracy: 0.8837 - val_precision: 0.9010 - val_recall: 0.8678 - val_f1score: 0.8835\n",
      "Epoch 43/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0252 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1037 - val_accuracy: 0.8837 - val_precision: 0.8971 - val_recall: 0.8678 - val_f1score: 0.8816\n",
      "Epoch 44/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0272 - accuracy: 1.0000 - precision: 1.0000 - recall: 0.9981 - f1score: 0.9990 - val_loss: 0.1072 - val_accuracy: 0.8837 - val_precision: 0.8983 - val_recall: 0.8671 - val_f1score: 0.8818\n",
      "Epoch 45/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0267 - accuracy: 1.0000 - precision: 1.0000 - recall: 0.9981 - f1score: 0.9990 - val_loss: 0.1151 - val_accuracy: 0.8643 - val_precision: 0.8896 - val_recall: 0.8556 - val_f1score: 0.8715\n",
      "Epoch 46/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0246 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1186 - val_accuracy: 0.8605 - val_precision: 0.8918 - val_recall: 0.8517 - val_f1score: 0.8706\n",
      "Epoch 47/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0250 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1159 - val_accuracy: 0.8682 - val_precision: 0.8880 - val_recall: 0.8556 - val_f1score: 0.8707\n",
      "Epoch 48/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0243 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1131 - val_accuracy: 0.8760 - val_precision: 0.8877 - val_recall: 0.8594 - val_f1score: 0.8728\n",
      "Epoch 49/120\n",
      "20/20 [==============================] - 2s 82ms/step - loss: 0.0231 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1108 - val_accuracy: 0.8798 - val_precision: 0.8803 - val_recall: 0.8671 - val_f1score: 0.8731\n",
      "Epoch 50/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0248 - accuracy: 1.0000 - precision: 0.9981 - recall: 1.0000 - f1score: 0.9991 - val_loss: 0.1116 - val_accuracy: 0.8837 - val_precision: 0.8918 - val_recall: 0.8748 - val_f1score: 0.8827\n",
      "Epoch 51/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0234 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1105 - val_accuracy: 0.8837 - val_precision: 0.8880 - val_recall: 0.8710 - val_f1score: 0.8790\n",
      "Epoch 52/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0241 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1096 - val_accuracy: 0.8837 - val_precision: 0.8887 - val_recall: 0.8787 - val_f1score: 0.8833\n",
      "Epoch 53/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0245 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1085 - val_accuracy: 0.8798 - val_precision: 0.8887 - val_recall: 0.8787 - val_f1score: 0.8833\n",
      "Epoch 54/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0237 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1077 - val_accuracy: 0.8798 - val_precision: 0.8887 - val_recall: 0.8787 - val_f1score: 0.8833\n",
      "Epoch 55/120\n",
      "20/20 [==============================] - 2s 82ms/step - loss: 0.0216 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1067 - val_accuracy: 0.8837 - val_precision: 0.8922 - val_recall: 0.8787 - val_f1score: 0.8852\n",
      "Epoch 56/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0231 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1062 - val_accuracy: 0.8837 - val_precision: 0.8922 - val_recall: 0.8787 - val_f1score: 0.8852\n",
      "Epoch 57/120\n",
      "20/20 [==============================] - 2s 82ms/step - loss: 0.0242 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1058 - val_accuracy: 0.8837 - val_precision: 0.8922 - val_recall: 0.8787 - val_f1score: 0.8852\n",
      "Epoch 58/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0230 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1055 - val_accuracy: 0.8837 - val_precision: 0.8925 - val_recall: 0.8825 - val_f1score: 0.8872\n",
      "Epoch 59/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0239 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1052 - val_accuracy: 0.8876 - val_precision: 0.8925 - val_recall: 0.8825 - val_f1score: 0.8872\n",
      "Epoch 60/120\n",
      "20/20 [==============================] - 2s 82ms/step - loss: 0.0224 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1050 - val_accuracy: 0.8876 - val_precision: 0.8995 - val_recall: 0.8825 - val_f1score: 0.8905\n",
      "Epoch 61/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0239 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1051 - val_accuracy: 0.8837 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 62/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0254 - accuracy: 0.9961 - precision: 0.9962 - recall: 0.9962 - f1score: 0.9962 - val_loss: 0.1118 - val_accuracy: 0.8876 - val_precision: 0.8999 - val_recall: 0.8825 - val_f1score: 0.8907\n",
      "Epoch 63/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0235 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1138 - val_accuracy: 0.8876 - val_precision: 0.8999 - val_recall: 0.8825 - val_f1score: 0.8907\n",
      "Epoch 64/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0232 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1129 - val_accuracy: 0.8876 - val_precision: 0.9003 - val_recall: 0.8864 - val_f1score: 0.8930\n",
      "Epoch 65/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0230 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1127 - val_accuracy: 0.8876 - val_precision: 0.8999 - val_recall: 0.8825 - val_f1score: 0.8909\n",
      "Epoch 66/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0226 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1126 - val_accuracy: 0.8876 - val_precision: 0.8999 - val_recall: 0.8825 - val_f1score: 0.8909\n",
      "Epoch 67/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0228 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1122 - val_accuracy: 0.8876 - val_precision: 0.9003 - val_recall: 0.8864 - val_f1score: 0.8930\n",
      "Epoch 68/120\n",
      "20/20 [==============================] - 2s 84ms/step - loss: 0.0237 - accuracy: 1.0000 - precision: 1.0000 - recall: 0.9981 - f1score: 0.9990 - val_loss: 0.1104 - val_accuracy: 0.8876 - val_precision: 0.9003 - val_recall: 0.8864 - val_f1score: 0.8930\n",
      "Epoch 69/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0230 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1107 - val_accuracy: 0.8876 - val_precision: 0.9003 - val_recall: 0.8864 - val_f1score: 0.8930\n",
      "Epoch 70/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0231 - accuracy: 0.9980 - precision: 1.0000 - recall: 0.9972 - f1score: 0.9986 - val_loss: 0.1108 - val_accuracy: 0.8876 - val_precision: 0.9003 - val_recall: 0.8864 - val_f1score: 0.8930\n",
      "Epoch 71/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0228 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1111 - val_accuracy: 0.8876 - val_precision: 0.9003 - val_recall: 0.8864 - val_f1score: 0.8930\n",
      "Epoch 72/120\n",
      "20/20 [==============================] - 2s 84ms/step - loss: 0.0227 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1107 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 73/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0227 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1107 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 74/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0226 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1100 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 75/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0219 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1106 - val_accuracy: 0.8876 - val_precision: 0.8964 - val_recall: 0.8825 - val_f1score: 0.8890\n",
      "Epoch 76/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0228 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1102 - val_accuracy: 0.8915 - val_precision: 0.8964 - val_recall: 0.8825 - val_f1score: 0.8890\n",
      "Epoch 77/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0226 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1102 - val_accuracy: 0.8915 - val_precision: 0.8964 - val_recall: 0.8825 - val_f1score: 0.8890\n",
      "Epoch 78/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0233 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1097 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 79/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0222 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1096 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 80/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0237 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1090 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 81/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0217 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1090 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 82/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0220 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1087 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 83/120\n",
      "20/20 [==============================] - 2s 85ms/step - loss: 0.0214 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1087 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 84/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0217 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1088 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 85/120\n",
      "20/20 [==============================] - 2s 84ms/step - loss: 0.0216 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1087 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 86/120\n",
      "20/20 [==============================] - 2s 84ms/step - loss: 0.0213 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1088 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 87/120\n",
      "20/20 [==============================] - 2s 84ms/step - loss: 0.0223 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1086 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 88/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0209 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1087 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 89/120\n",
      "20/20 [==============================] - 2s 84ms/step - loss: 0.0227 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1087 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 90/120\n",
      "20/20 [==============================] - 2s 85ms/step - loss: 0.0219 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1084 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 91/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0228 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1085 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 92/120\n",
      "20/20 [==============================] - 2s 84ms/step - loss: 0.0233 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1083 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 93/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0221 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1081 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 94/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0214 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1080 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 95/120\n",
      "20/20 [==============================] - 2s 84ms/step - loss: 0.0228 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1081 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 96/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0220 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1081 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 97/120\n",
      "20/20 [==============================] - 2s 84ms/step - loss: 0.0216 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1082 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 98/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0219 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1082 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 99/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0221 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1083 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 100/120\n",
      "20/20 [==============================] - 2s 84ms/step - loss: 0.0216 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1082 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 101/120\n",
      "20/20 [==============================] - 2s 84ms/step - loss: 0.0214 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1081 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 102/120\n",
      "20/20 [==============================] - 2s 84ms/step - loss: 0.0218 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1090 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 103/120\n",
      "20/20 [==============================] - 2s 84ms/step - loss: 0.0214 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1087 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 104/120\n",
      "20/20 [==============================] - 2s 84ms/step - loss: 0.0213 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1086 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 105/120\n",
      "20/20 [==============================] - 2s 84ms/step - loss: 0.0221 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1085 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 106/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0215 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1083 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 107/120\n",
      "20/20 [==============================] - 2s 84ms/step - loss: 0.0214 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1085 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 108/120\n",
      "20/20 [==============================] - 2s 84ms/step - loss: 0.0222 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1085 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 109/120\n",
      "20/20 [==============================] - 2s 84ms/step - loss: 0.0214 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1084 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 110/120\n",
      "20/20 [==============================] - 2s 85ms/step - loss: 0.0213 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1082 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 111/120\n",
      "20/20 [==============================] - 2s 84ms/step - loss: 0.0214 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1080 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 112/120\n",
      "20/20 [==============================] - 2s 84ms/step - loss: 0.0231 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1080 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 113/120\n",
      "20/20 [==============================] - 2s 84ms/step - loss: 0.0210 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1081 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 114/120\n",
      "20/20 [==============================] - 2s 84ms/step - loss: 0.0214 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1081 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 115/120\n",
      "20/20 [==============================] - 2s 84ms/step - loss: 0.0218 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1084 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 116/120\n",
      "20/20 [==============================] - 2s 84ms/step - loss: 0.0226 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1082 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 117/120\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.0216 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1082 - val_accuracy: 0.8876 - val_precision: 0.8967 - val_recall: 0.8864 - val_f1score: 0.8912\n",
      "Epoch 118/120\n",
      "20/20 [==============================] - 2s 84ms/step - loss: 0.0233 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1084 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 119/120\n",
      "20/20 [==============================] - 2s 84ms/step - loss: 0.0227 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1086 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n",
      "Epoch 120/120\n",
      "20/20 [==============================] - 2s 84ms/step - loss: 0.0213 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1083 - val_accuracy: 0.8876 - val_precision: 0.8935 - val_recall: 0.8864 - val_f1score: 0.8896\n"
     ]
    }
   ],
   "source": [
    "def recall(y_target, y_pred):\n",
    "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
    "    # round : 반올림한다\n",
    "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "\n",
    "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
    "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
    "\n",
    "    # (True Positive + False Negative) = 실제 값이 1(Positive) 전체\n",
    "    count_true_positive_false_negative = K.sum(y_target_yn)\n",
    "\n",
    "    # Recall =  (True Positive) / (True Positive + False Negative)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    recall = count_true_positive / (count_true_positive_false_negative + K.epsilon())\n",
    "\n",
    "    # return a single tensor value\n",
    "    return recall\n",
    "\n",
    "\n",
    "def precision(y_target, y_pred):\n",
    "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
    "    # round : 반올림한다\n",
    "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "\n",
    "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
    "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
    "\n",
    "    # (True Positive + False Positive) = 예측 값이 1(Positive) 전체\n",
    "    count_true_positive_false_positive = K.sum(y_pred_yn)\n",
    "\n",
    "    # Precision = (True Positive) / (True Positive + False Positive)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    precision = count_true_positive / (count_true_positive_false_positive + K.epsilon())\n",
    "\n",
    "    # return a single tensor value\n",
    "    return precision\n",
    "\n",
    "\n",
    "def f1score(y_target, y_pred):\n",
    "    _recall = recall(y_target, y_pred)\n",
    "    _precision = precision(y_target, y_pred)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    _f1score = ( 2 * _recall * _precision) / (_recall + _precision+ K.epsilon())\n",
    "    \n",
    "    # return a single tensor value\n",
    "    return _f1score\n",
    "\n",
    "\n",
    "\n",
    "#model = create_model()\n",
    "#model = ResNet()\n",
    "#model = mobile_net()\n",
    "#model = xception()\n",
    "model = densenet()\n",
    "#model = resnet()\n",
    "\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=learning_rate,\n",
    "                                                          decay_steps=training_epochs * 10,\n",
    "                                                          decay_rate=0.5,\n",
    "                                                          staircase=True)\n",
    "\n",
    "\n",
    "\n",
    "# lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "#     initial_learning_rate=learning_rate,\n",
    "#     decay_steps=100000,\n",
    "#     decay_rate=0.96)\n",
    "\n",
    "# optimizer는 Adam, loss는 sparse categorical crossentropy 사용\n",
    "# label이 ont-hot으로 encoding 안 된 경우에 sparse categorical corssentropy 및 sparse categorical accuracy 사용\n",
    "model.compile(keras.optimizers.Adam(lr_schedule), loss = 'binary_crossentropy', metrics=['accuracy', precision, recall, f1score])\n",
    "\n",
    "# model.compile(\n",
    "#     optimizer='adam',\n",
    "#     loss='binary_crossentropy', \n",
    "#     metrics=['accuracy', precision, recall, f1score],\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Train!\n",
    "history = model.fit(x_train, y_train, steps_per_epoch=training_epochs,\n",
    "         epochs=120, validation_data = (x_val,y_val),validation_steps=validation_steps)\n",
    "model.save('animal_model_man.h5')\n",
    "# epochs = 30\n",
    "# history = model.fit(\n",
    "#     training_set, \n",
    "#     epochs=epochs,\n",
    "#     steps_per_epoch=training_set.samples / epochs, \n",
    "#     validation_data=val_set,\n",
    "#     validation_steps=val_set.samples / epochs,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hnA6J84ElBkJ",
    "outputId": "b1049b31-06ea-4b80-e576-02eae9265dce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n",
      "index: 0  actual y: 5  answer y: 5  prediction: [ 2.560756    1.1146052   0.84967726  0.9324652   1.050377   97.15257   ]\n",
      "index: 1  actual y: 3  answer y: 3  prediction: [ 3.3606722  1.6511025  0.8466049 97.75278    0.8293806  2.1572044]\n",
      "index: 2  actual y: 3  answer y: 3  prediction: [ 4.6821904  1.2730659  0.7322619 96.417656   0.7202256  1.4864858]\n",
      "index: 3  actual y: 0  answer y: 0  prediction: [87.98194    1.0818326  1.3455998  6.1084843  3.1886072  4.7620864]\n",
      "index: 4  actual y: 0  answer y: 0  prediction: [96.54185     0.7839932   0.38949838  3.80206     0.5993994   1.0794374 ]\n",
      "index: 5  actual y: 4  answer y: 4  prediction: [ 1.8154513  1.2745944  1.0824374 17.125717  59.77439    2.8289094]\n",
      "index: 6  actual y: 2  answer y: 2  prediction: [ 2.4303443  2.767369  97.91232    1.3767083  1.0484796  1.2946862]\n",
      "index: 7  actual y: 1  answer y: 1  prediction: [ 1.3843036 97.01122    0.2383416  1.1191009  0.5500484  1.5882697]\n",
      "index: 8  actual y: 1  answer y: 1  prediction: [ 1.9395735  96.938286    0.41753685  0.8254245   1.176707    1.9588218 ]\n",
      "index: 9  actual y: 5  answer y: 3  prediction: [ 0.98455834  1.1337017   0.37203017 73.87481     0.31057987 51.15655   ]\n",
      "index: 10  actual y: 1  answer y: 1  prediction: [ 2.3325183 96.5985     0.5870315  1.0132133  1.7430141  1.8669578]\n",
      "index: 11  actual y: 2  answer y: 2  prediction: [ 2.5996993   1.8872706  98.3808      0.98884004  0.88716483  1.6345547 ]\n",
      "index: 12  actual y: 2  answer y: 2  prediction: [ 2.6146715  2.0038702 98.28365    1.0247374  1.0175362  1.2947594]\n",
      "index: 13  actual y: 1  answer y: 1  prediction: [ 2.101029  96.51845    0.5397269  0.9143954  1.5685803  1.8390083]\n",
      "index: 14  actual y: 1  answer y: 1  prediction: [ 1.8469523 96.2064     0.800078   0.815797   1.5763123  2.406701 ]\n",
      "index: 15  actual y: 3  answer y: 3  prediction: [ 2.3135095  1.1941334  0.8061088 96.864746   0.9661968  3.2110288]\n",
      "index: 16  actual y: 0  answer y: 0  prediction: [52.986736    8.261171    0.44381198 38.285618    1.171271    2.3017104 ]\n",
      "index: 17  actual y: 1  answer y: 1  prediction: [ 2.1458676  92.50483     4.102562    0.74878705  9.338748    2.0758429 ]\n",
      "index: 18  actual y: 2  answer y: 2  prediction: [ 2.441792   1.9265155 98.197845   1.0616459  0.9916694  1.4837823]\n",
      "index: 19  actual y: 2  answer y: 2  prediction: [ 2.486083   1.736109  98.140236   2.1648684  0.7663343  1.3146662]\n",
      "index: 20  actual y: 3  answer y: 3  prediction: [ 2.1779659  1.0882298  1.1801192 74.66554    2.8296402 16.29425  ]\n",
      "index: 21  actual y: 4  answer y: 4  prediction: [ 2.9709566   3.2911453   0.84140563  0.5978474  94.65159     2.830067  ]\n",
      "index: 22  actual y: 2  answer y: 2  prediction: [ 2.3962069  1.996756  98.340744   1.0197718  0.9833301  1.3990873]\n",
      "index: 23  actual y: 3  answer y: 3  prediction: [ 2.574241   1.33734    0.8422574 97.73185    1.0702913  2.4270372]\n",
      "index: 24  actual y: 4  answer y: 4  prediction: [ 2.8954237   1.9844836   0.34096196  0.54884547 96.456245    2.3674555 ]\n",
      "index: 25  actual y: 2  answer y: 2  prediction: [ 2.4976504  2.1905801 98.198906   1.2543828  1.4078988  1.6095586]\n",
      "index: 26  actual y: 3  answer y: 3  prediction: [ 3.2554348  1.2545794  0.7331664 97.90035    0.7034081  1.8100191]\n",
      "index: 27  actual y: 2  answer y: 2  prediction: [ 2.119855   2.0293775 98.33574    1.0629882  1.0717478  1.9542409]\n",
      "index: 28  actual y: 5  answer y: 5  prediction: [ 1.32086     0.49692714  0.85247374 57.93124     0.37999478 63.37612   ]\n",
      "index: 29  actual y: 4  answer y: 4  prediction: [ 3.0701447   2.5796955   0.49654883  0.8652526  95.840385    2.842487  ]\n",
      "index: 30  actual y: 1  answer y: 1  prediction: [ 1.5403956  96.7358      0.49802867  0.7159866   0.92192954  1.3945904 ]\n",
      "index: 31  actual y: 3  answer y: 3  prediction: [ 3.066772   1.535908   1.264861  97.1904     1.2710453  2.5907683]\n",
      "index: 32  actual y: 2  answer y: 2  prediction: [ 2.288953   2.2393017 98.331955   1.0377383  0.8580681  1.275986 ]\n",
      "index: 33  actual y: 3  answer y: 3  prediction: [ 3.1287658  1.3158506  1.0734113 97.66922    0.7368455  1.8761607]\n",
      "index: 34  actual y: 4  answer y: 4  prediction: [ 6.016897   2.1638296  0.6461095  1.3320333 93.560875   1.7494783]\n",
      "index: 35  actual y: 0  answer y: 0  prediction: [97.1789      0.6699343   0.49631047  1.6798692   0.8397744   1.4164275 ]\n",
      "index: 36  actual y: 0  answer y: 0  prediction: [96.685295   1.0320791  0.7731389  2.193715   1.8382927  1.6782287]\n",
      "index: 37  actual y: 2  answer y: 2  prediction: [ 2.5724444  1.6903105 98.62318    1.151339   0.8420443  1.0950912]\n",
      "index: 38  actual y: 5  answer y: 5  prediction: [ 2.1155753   1.3720583   0.42309746  1.3512602   2.5193586  96.28953   ]\n",
      "index: 39  actual y: 0  answer y: 0  prediction: [95.887       1.9116435   0.27607358  2.8455696   0.49901786  1.1195632 ]\n",
      "index: 40  actual y: 5  answer y: 4  prediction: [ 2.067784    3.5409002   0.667615    0.38487107 94.82651     2.230054  ]\n",
      "index: 41  actual y: 1  answer y: 5  prediction: [ 1.8658446  18.94107     0.50091505  9.571704    1.8516196  48.605644  ]\n",
      "index: 42  actual y: 2  answer y: 2  prediction: [ 2.4182324   2.076404   98.367836    1.1693109   0.94144696  1.2948848 ]\n",
      "index: 43  actual y: 0  answer y: 0  prediction: [37.660477   1.0991011  1.1971753  4.7299886 30.935282   5.7410316]\n",
      "index: 44  actual y: 4  answer y: 4  prediction: [ 9.708082    1.6344222   0.91109246  1.6609659  74.68425    15.61353   ]\n",
      "index: 45  actual y: 1  answer y: 1  prediction: [ 2.9699721 96.538536   0.8164068  1.0051159  2.1775768  2.477284 ]\n",
      "index: 46  actual y: 4  answer y: 4  prediction: [ 7.6402674  3.038675   0.7688994  1.3911939 78.618515  10.301138 ]\n",
      "index: 47  actual y: 3  answer y: 4  prediction: [ 6.0248895  2.5489838  1.7966632 28.908577  41.180355   1.3798368]\n",
      "index: 48  actual y: 1  answer y: 1  prediction: [19.002869   30.323252    0.93361026  4.93214    16.404955    7.1978188 ]\n",
      "index: 49  actual y: 1  answer y: 1  prediction: [ 1.8470881 97.11816    0.7957232  0.5735166  1.6156048  1.74165  ]\n",
      "index: 50  actual y: 0  answer y: 1  prediction: [14.527607  86.66629    0.5591537  3.1086257  0.9804682  1.2383418]\n",
      "index: 51  actual y: 5  answer y: 5  prediction: [ 2.3856125  1.4594225  0.5039718  1.7699699  1.6478451 96.52245  ]\n",
      "index: 52  actual y: 2  answer y: 2  prediction: [ 2.3367622  2.605648  98.11316    1.0570997  0.7327483  1.6872783]\n",
      "index: 53  actual y: 2  answer y: 2  prediction: [ 2.2431412  2.2035663 98.48196    1.1490023  1.0314121  1.8804243]\n",
      "index: 54  actual y: 0  answer y: 0  prediction: [81.15874     7.9879003   0.52070606  3.7253878   1.374894    9.564505  ]\n",
      "index: 55  actual y: 0  answer y: 0  prediction: [78.20022     4.2686505   4.279015    0.8036845   1.0281812   0.50813556]\n",
      "index: 56  actual y: 3  answer y: 3  prediction: [ 2.8113925  1.1713707  1.1423033 97.53568    0.7569923  2.1832466]\n",
      "index: 57  actual y: 0  answer y: 0  prediction: [91.47742    1.608427   0.9788233  4.0079527  2.1424222  1.6275637]\n",
      "index: 58  actual y: 5  answer y: 5  prediction: [ 2.346715    1.2498925   0.65351796  1.1999999   1.4105196  97.34335   ]\n",
      "index: 59  actual y: 3  answer y: 0  prediction: [96.192375    1.9839098   0.2561659   3.6547165   0.51672363  1.0296515 ]\n",
      "index: 60  actual y: 3  answer y: 3  prediction: [ 3.4464653  1.3884407  0.8718281 97.62492    0.9014501  2.4023495]\n",
      "index: 61  actual y: 3  answer y: 3  prediction: [ 4.099655    1.7364475   0.5536753  96.902016    0.70587504  1.7099152 ]\n",
      "index: 62  actual y: 4  answer y: 3  prediction: [ 4.731413   1.2125903  1.0699389 93.56162    2.0201077  1.1412598]\n",
      "index: 63  actual y: 2  answer y: 2  prediction: [ 2.2294936  2.2101157 98.048965   1.4189539  0.8538354  1.6925137]\n",
      "index: 64  actual y: 4  answer y: 4  prediction: [ 3.5342958  2.1327457  0.4968137  0.8108436 95.63304    2.1376529]\n",
      "index: 65  actual y: 4  answer y: 4  prediction: [ 3.672484    1.1534444   0.9546158  28.090275   45.355927    0.86316264]\n",
      "index: 66  actual y: 5  answer y: 5  prediction: [ 4.8001623  6.8004475  0.5313261  0.978517   1.5495076 89.28325  ]\n",
      "index: 67  actual y: 0  answer y: 0  prediction: [91.63803    6.5420427  0.4193826  3.8884091  1.7057985  1.3254852]\n",
      "index: 68  actual y: 5  answer y: 5  prediction: [ 1.9795991   1.1758722   0.2897103   0.84448606  0.9053331  98.16036   ]\n",
      "index: 69  actual y: 2  answer y: 2  prediction: [ 2.0995867  1.9320558 98.36357    1.3310255  0.8691498  1.2914746]\n",
      "index: 70  actual y: 3  answer y: 3  prediction: [ 3.3973672   1.8394053   0.43790922 97.0052      0.63626266  1.5034273 ]\n",
      "index: 71  actual y: 1  answer y: 1  prediction: [ 7.982649  70.42467    0.9436668  0.7539177 14.014403   2.0169199]\n",
      "index: 72  actual y: 1  answer y: 1  prediction: [ 1.5331651  97.13823     0.6931226   0.53646445  1.480135    1.3637933 ]\n",
      "index: 73  actual y: 1  answer y: 1  prediction: [ 2.0331755 96.22741    1.0075157  0.7805978  1.7897185  2.6816947]\n",
      "index: 74  actual y: 4  answer y: 3  prediction: [16.936783    2.498393    0.33674252 89.48319     0.3197262   1.8119452 ]\n",
      "index: 75  actual y: 2  answer y: 2  prediction: [ 2.4939837  2.3417754 98.10773    1.417845   1.0532544  1.6233897]\n",
      "index: 76  actual y: 4  answer y: 4  prediction: [ 1.9380523  3.5703812  0.4737023  0.332621  95.379265   2.4363377]\n",
      "index: 77  actual y: 4  answer y: 4  prediction: [ 2.851851    2.792777    0.5716508   0.54321855 95.035       2.653042  ]\n",
      "index: 78  actual y: 4  answer y: 2  prediction: [ 2.1593833  1.4837885 98.30384    1.1522374  0.9102965  0.6181631]\n",
      "index: 79  actual y: 5  answer y: 5  prediction: [ 2.344595   1.6787112  0.3427999  2.6610875  1.3248062 96.72058  ]\n",
      "index: 80  actual y: 0  answer y: 0  prediction: [94.444534   0.8626246  1.0517026  3.0195215  2.1593142  1.2062192]\n",
      "index: 81  actual y: 0  answer y: 4  prediction: [ 5.1729326   1.8434376   0.80879825  1.387437   91.59384     2.0712485 ]\n",
      "index: 82  actual y: 0  answer y: 0  prediction: [55.411266   0.9760625  7.8058004 14.518873   8.852888   0.8358569]\n",
      "index: 83  actual y: 0  answer y: 0  prediction: [50.741734    4.1090274   0.72074604  1.4735819  22.053701    6.579826  ]\n",
      "index: 84  actual y: 2  answer y: 2  prediction: [ 2.4874275  2.080556  98.07422    1.46593    0.9176085  1.5314505]\n",
      "index: 85  actual y: 5  answer y: 5  prediction: [ 3.3008754  1.4183161  0.6118654  1.8347708  1.3453513 96.47748  ]\n",
      "index: 86  actual y: 1  answer y: 1  prediction: [ 2.6922271  96.5327      0.7045918   0.88595647  2.0669868   2.4312534 ]\n",
      "index: 87  actual y: 1  answer y: 1  prediction: [ 2.5432591  96.52663     0.69842696  0.8850985   1.7202519   2.2688537 ]\n",
      "index: 88  actual y: 5  answer y: 4  prediction: [ 3.0776021   2.7546144   0.70460045  0.84648234 91.75554     7.320866  ]\n",
      "index: 89  actual y: 2  answer y: 2  prediction: [ 2.3061423   1.6630653  98.30141     0.91598237  0.6658219   1.6931835 ]\n",
      "index: 90  actual y: 5  answer y: 5  prediction: [ 2.1819599  1.3413726  0.5053497  1.6021696  1.5583361 97.26236  ]\n",
      "index: 91  actual y: 3  answer y: 3  prediction: [13.132202   1.0298123  1.0797435 82.37287    3.3300166  1.0739548]\n",
      "index: 92  actual y: 5  answer y: 5  prediction: [ 2.0042107   2.5301802   0.41662544  1.1157668   1.4342281  96.62343   ]\n",
      "index: 93  actual y: 4  answer y: 3  prediction: [ 2.400455    2.520778    0.63039225 67.06381    17.263803    2.8161452 ]\n",
      "index: 94  actual y: 0  answer y: 0  prediction: [86.090965    1.0128428   0.86400926 10.316447    1.4679868   0.81227374]\n",
      "index: 95  actual y: 5  answer y: 5  prediction: [ 2.4924035   1.9284921   0.46709892  1.5245229  34.30087    59.081703  ]\n",
      "index: 96  actual y: 4  answer y: 4  prediction: [ 2.548783    2.6076252   0.6232807   0.45735952 95.60737     2.531242  ]\n",
      "index: 97  actual y: 2  answer y: 2  prediction: [ 2.2960892   1.7588959  98.14968     1.4611193   0.78864944  1.0425311 ]\n",
      "index: 98  actual y: 3  answer y: 3  prediction: [ 3.9349444   0.80598027  1.0009729  95.14097     0.647833    7.49797   ]\n",
      "index: 99  actual y: 5  answer y: 5  prediction: [ 2.3654668   1.4756387   0.46491176  1.0887024   1.5277961  97.54147   ]\n",
      "index: 100  actual y: 4  answer y: 5  prediction: [ 1.3586974  22.829311    0.44070244  0.8053238  19.82467    35.822803  ]\n",
      "index: 101  actual y: 1  answer y: 1  prediction: [ 1.6974468  96.79584     0.50190973  0.8237239   1.327064    1.9037985 ]\n",
      "index: 102  actual y: 4  answer y: 4  prediction: [ 2.7996137   3.1142602   0.84470445  0.7067077  94.67232     2.2790296 ]\n",
      "index: 103  actual y: 5  answer y: 5  prediction: [ 1.8270257  1.5004399  0.4712598  2.552361   3.374319  94.76315  ]\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.1396 - accuracy: 0.8750 - precision: 0.9093 - recall: 0.8594 - f1score: 0.8821\n",
      "loss: 0.140, accuracy: 0.875, precision: 0.909, recall: 0.859, f1score: 0.882\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# output = classifier.predict_generator(test_set, steps=1)\n",
    "# print(test_set.class_indices)\n",
    "# print(output)\n",
    "size = y_test[:,-1]\n",
    "print(size.size)\n",
    "\n",
    "\n",
    "# predict 10 random hand-writing data\n",
    "y_predicted = model.predict(x_test)\n",
    "for x in range(0,size.size):\n",
    "    \n",
    "   np.set_printoptions(suppress=True)\n",
    "   print(\"index:\", x,\n",
    "          \" actual y:\", np.argmax(y_test[x]),\n",
    "          \" answer y:\", np.argmax(y_predicted[x]),\n",
    "          \" prediction:\", np.array(y_predicted[x] * 100))\n",
    "\n",
    "_loss, _acc, _precision, _recall, _f1score = model.evaluate(x_test, y_test)\n",
    "print('loss: {:.3f}, accuracy: {:.3f}, precision: {:.3f}, recall: {:.3f}, f1score: {:.3f}'.format(_loss, _acc, _precision, _recall, _f1score))\n",
    "# print('loss: ', evaluation[0])\n",
    "# print('accuracy', evaluation[1])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 손실 그래프\n",
    "def plot_loss(history):\n",
    "   # 선 그리기\n",
    "    plt.plot(history.history['loss'], 'y', label='train loss')\n",
    "    plt.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "   # 그래프 제목\n",
    "    plt.title('Model Loss')\n",
    "   # x,y축 이름 표시\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "   # 각 라인 표식 표시\n",
    "    plt.legend(['Train','Validation'],loc=0)\n",
    "\n",
    "# 정확도 그래프\n",
    "def plot_acc(history):\n",
    "  # dir(history.history)\n",
    "    plt.plot(history.history['accuracy'], 'b', label='train accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], 'g', label='val accuracy')\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc=0)\n",
    "\n",
    "plot_loss(history)\n",
    "plt.show()\n",
    "plot_acc(history)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "animal_model_man",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
