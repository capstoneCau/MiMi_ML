{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-dgO35Q0iPCC",
    "outputId": "0fe8bd45-5fd2-40dc-fc5c-1f5e5ef7370c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batchSIZE is 512, Learning Rate is 0.001\n",
      "Found 992 images belonging to 7 classes.\n",
      "Found 115 images belonging to 7 classes.\n",
      "Found 301 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os # miscellaneous operating system interfaces\n",
    "import shutil # high-level file operations\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import random\n",
    "from itertools import product\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Conv2D \n",
    "from keras.layers import MaxPooling2D \n",
    "from keras.layers import Flatten \n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.applications import MobileNetV2, Xception, DenseNet121,ResNet50V2,NASNetMobile\n",
    "from keras.applications.mobilenet_v2 import preprocess_input\n",
    "from keras.layers import  Input, Conv2D, Conv2DTranspose, ReLU,AveragePooling2D, MaxPooling2D, Dense, BatchNormalization, GlobalAveragePooling2D, Concatenate\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "\n",
    "base_dir = \"/content/drive/My Drive/Colab Notebooks/datasets/man\"\n",
    "\n",
    "train_img_dir = \"/content/drive/My Drive/Colab Notebooks/datasets/man/train\"\n",
    "\n",
    "test_img_dir = \"/content/drive/My Drive/Colab Notebooks/datasets/man/test\"\n",
    "\n",
    "val_img_dir = \"/content/drive/My Drive/Colab Notebooks/datasets/man/val\"\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "training_epochs = 20 # traindata개수/batchsize\n",
    "batch_size = 512\n",
    "validation_steps = 10 # valdata개수/batchsize\n",
    "\n",
    "\n",
    "\n",
    "print(f'batchSIZE is {batch_size}, Learning Rate is {learning_rate}')\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "val_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "categories = ['dog','cat','bear','hamster','horse','wolf','dinosaur']\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(train_img_dir, target_size=(128,128), \n",
    "                                             classes=categories, \n",
    "                                             batch_size=batch_size)\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(test_img_dir,\n",
    "                                        target_size=(128,128), \n",
    "                                        classes=categories, \n",
    "                                        batch_size=batch_size)\n",
    "\n",
    "val_set = test_datagen.flow_from_directory(val_img_dir,\n",
    "                                        target_size=(128,128), \n",
    "                                        classes=categories, \n",
    "                                        batch_size=batch_size)\n",
    "\n",
    "x_train, y_train = next(training_set)\n",
    "x_test, y_test = next(test_set)\n",
    "x_val,y_val = next(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "RzBS39E1jIg4"
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "    \n",
    "\n",
    "    conv2_1 = Conv2D(32, 3, 1, 'SAME')(inputs)\n",
    "    conv2_2 = Conv2D(32, 3, 1, 'SAME')(conv2_1)\n",
    "    pool2_3 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv2_2)\n",
    "    br1 = BatchNormalization()(pool2_3)\n",
    "    \n",
    "    conv2_1 = Conv2D(32, 3, 1, 'SAME')(br1)\n",
    "    conv2_2 = Conv2D(32, 3, 1, 'SAME')(conv2_1)\n",
    "    pool2_3 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv2_2)\n",
    "    br1 = BatchNormalization()(pool2_3)\n",
    "    \n",
    "    \n",
    "    conv3_1 = Conv2D(64, 3, 1, 'SAME')(br1)\n",
    "    conv3_2 = Conv2D(64, 3, 1, 'SAME')(conv3_1)\n",
    "    pool3_2 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv3_2)\n",
    "    br2 = BatchNormalization()(pool3_2)\n",
    "    \n",
    "    conv3_1 = Conv2D(64, 3, 1, 'SAME')(br2)\n",
    "    conv3_2 = Conv2D(64, 3, 1, 'SAME')(conv3_1)\n",
    "    pool3_2 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv3_2)\n",
    "    br2 = BatchNormalization()(pool3_2)\n",
    "    \n",
    "    \n",
    "    conv4_1 = Conv2D(128, 3, 1, 'SAME')(br2)\n",
    "    conv4_2 = Conv2D(128, 3, 1, 'SAME')(conv4_1)\n",
    "    pool4_2 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv4_2)\n",
    "    br3 = BatchNormalization()(pool4_2)\n",
    "    \n",
    "    conv4_1 = Conv2D(128, 3, 1, 'SAME')(br3)\n",
    "    conv4_2 = Conv2D(128, 3, 1, 'SAME')(conv4_1)\n",
    "    pool4_2 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv4_2)\n",
    "    br3 = BatchNormalization()(pool4_2)\n",
    "    \n",
    "    flatten1 = Flatten()(pool4_2)\n",
    "    dense1 = Dense(units = 512, activation = 'relu')(flatten1)\n",
    "    dense2 = Dense(units = 1024, activation = 'relu')(dense1)\n",
    "    dr1 = Dropout(0.7)(dense2)\n",
    "    dense3 = Dense(units = 5, activation = 'sigmoid')(dr1)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=dense3) \n",
    "def mobile_net():\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "    mobileNet = MobileNetV2(weights = \"imagenet\", include_top=False,input_shape = (128,128,3)\n",
    "                            ,input_tensor = inputs)\n",
    "    for layer in mobileNet.layers:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    output = mobileNet.output\n",
    "    pooling = AveragePooling2D(pool_size=(16,16),padding='SAME')(output)\n",
    "    flatten1 = Flatten()(pooling)\n",
    "    dense1 = Dense(units = 256)(flatten1)\n",
    "    batch1 = BatchNormalization()(dense1)\n",
    "    relu1 = ReLU()(batch1)\n",
    "    dense2 = Dense(units = 64)(relu1)\n",
    "    batch2 = BatchNormalization()(dense2)\n",
    "    relu2 = ReLU()(batch2)\n",
    "    dense3 = Dense(units = 6, activation = 'sigmoid')(relu2)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=dense3)\n",
    "\n",
    "def xception():\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "    xception = Xception(weights = \"imagenet\", include_top=False,input_shape = (128,128,3)\n",
    "                            ,input_tensor = inputs)\n",
    "    for layer in xception.layers:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    output = xception.output\n",
    "    pooling = AveragePooling2D(pool_size=(8,8),padding='SAME')(output)\n",
    "    flatten1 = Flatten()(pooling)\n",
    "    dense1 = Dense(units = 256)(flatten1)\n",
    "    batch1 = BatchNormalization()(dense1)\n",
    "    relu1 = ReLU()(batch1)\n",
    "    dense2 = Dense(units = 64)(relu1)\n",
    "    batch2 = BatchNormalization()(dense2)\n",
    "    relu2 = ReLU()(batch2)\n",
    "    dense3 = Dense(units = 6, activation = 'sigmoid')(relu2)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=dense3)\n",
    "    # pooling = AveragePooling2D(pool_size=(4,4),padding='SAME')(output)\n",
    "    \n",
    "    # flatten1 = Flatten()(pooling)\n",
    "    # dense1 = Dense(units = 512, activation = 'relu')(flatten1)\n",
    "    # dense2 = Dense(units = 1024, activation = 'relu')(dense1)\n",
    "    # dr1 = Dropout(0.7)(dense2)\n",
    "    # dense3 = Dense(units = 5, activation = 'sigmoid')(dr1)\n",
    "    \n",
    "    # return keras.Model(inputs=inputs, outputs=dense3)\n",
    "\n",
    "def resnet():\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "    resnet = ResNet50V2 (weights = \"imagenet\", include_top=False,input_shape = (128,128,3)\n",
    "                            ,input_tensor = inputs)\n",
    "    for layer in resnet.layers:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    output = resnet.output\n",
    "    pooling = AveragePooling2D(pool_size=(8,8),padding='SAME')(output)\n",
    "    flatten1 = Flatten()(pooling)\n",
    "    dense1 = Dense(units = 256)(flatten1)\n",
    "    batch1 = BatchNormalization()(dense1)\n",
    "    relu1 = ReLU()(batch1)\n",
    "    dense2 = Dense(units = 64)(relu1)\n",
    "    batch2 = BatchNormalization()(dense2)\n",
    "    relu2 = ReLU()(batch2)\n",
    "    dense3 = Dense(units = 6, activation = 'sigmoid')(relu2)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=dense3)\n",
    "\n",
    "def densenet():\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "    densenet = DenseNet121(weights = \"imagenet\", include_top=False,input_shape = (128,128,3)\n",
    "                            ,input_tensor = inputs)\n",
    "    for layer in densenet.layers:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    output = densenet.output\n",
    "    pooling = MaxPooling2D(pool_size=(32,32),padding='SAME')(output)\n",
    "    flatten1 = Flatten()(pooling)\n",
    "    dense1 = Dense(units = 112)(flatten1)\n",
    "    batch1 = BatchNormalization()(dense1)\n",
    "    relu1 = ReLU()(batch1)\n",
    "    dense2 = Dense(units = 28)(relu1)\n",
    "    batch2 = BatchNormalization()(dense2)\n",
    "    relu2 = ReLU()(batch2)\n",
    "    dense3 = Dense(units = 7, activation = 'sigmoid')(relu2)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=dense3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ResidualUnit(tf.keras.Model):\n",
    "    def __init__(self, filter_in, filter_out, kernel_size):\n",
    "        super(ResidualUnit, self).__init__()\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filter_out, kernel_size, padding='same')\n",
    "        \n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filter_out, kernel_size, padding='same')\n",
    "        \n",
    "        if filter_in == filter_out:\n",
    "            self.identity = lambda x: x\n",
    "        else:\n",
    "            self.identity = tf.keras.layers.Conv2D(filter_out, (1,1), padding='same')\n",
    "\n",
    "    def call(self, x, training=False, mask=None):\n",
    "        h = self.bn1(x, training=training)\n",
    "        h = tf.nn.relu(h)\n",
    "        h = self.conv1(h)\n",
    "        \n",
    "        h = self.bn2(h, training=training)\n",
    "        h = tf.nn.relu(h)\n",
    "        h = self.conv2(h)\n",
    "        return self.identity(x) + h\n",
    "    \n",
    "class ResnetLayer(tf.keras.Model):\n",
    "    def __init__(self, filter_in, filters, kernel_size):\n",
    "        super(ResnetLayer, self).__init__()\n",
    "        self.sequence = list()\n",
    "        for f_in, f_out in zip([filter_in] + list(filters), filters):\n",
    "            self.sequence.append(ResidualUnit(f_in, f_out, kernel_size))\n",
    "\n",
    "    def call(self, x, training=False, mask=None):\n",
    "        for unit in self.sequence:\n",
    "            x = unit(x, training=training)\n",
    "        return x\n",
    "    \n",
    "class ResNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu') # 28x28x8\n",
    "        \n",
    "        self.res1 = ResnetLayer(64, (16, 16), (3, 3)) # 28x28x16\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D((2, 2)) # 14x14x16\n",
    "        \n",
    "        \n",
    "        self.res2 = ResnetLayer(128, (32, 32), (3, 3)) # 14x14x32\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D((2, 2)) # 7x7x32\n",
    "    \n",
    "        \n",
    "        self.res3 = ResnetLayer(256, (64, 64), (3, 3)) # 7x7x64\n",
    "        self.pool = tf.keras.layers.MaxPool2D((2, 2)) # 7x7x32\n",
    "        \n",
    "        \n",
    "        self.res4 = ResnetLayer(512, (64, 64), (3, 3)) # 7x7x64\n",
    "        self.pool = tf.keras.layers.MaxPool2D((2, 2)) # 7x7x32\n",
    "        \n",
    "        \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(1024, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(5, activation='softmax')\n",
    "        \n",
    "    def call(self, x, training=False, mask=None):\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        x = self.res1(x, training=training)\n",
    "        x = self.pool1(x)\n",
    "        x = self.res2(x, training=training)\n",
    "        x = self.pool2(x)\n",
    "        x = self.res3(x, training=training)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        return self.dense2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2F8dolSzlByA",
    "outputId": "e93219f5-270a-4d86-d3c9-ecd0b9c44000"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "20/20 [==============================] - 4s 205ms/step - loss: 0.6638 - accuracy: 0.2793 - precision: 0.2047 - recall: 0.5609 - f1score: 0.2976 - val_loss: 2.8351 - val_accuracy: 0.1329 - val_precision: 0.1334 - val_recall: 0.2963 - val_f1score: 0.1840\n",
      "Epoch 2/120\n",
      "20/20 [==============================] - 2s 118ms/step - loss: 0.5242 - accuracy: 0.3906 - precision: 0.3376 - recall: 0.4521 - f1score: 0.3832 - val_loss: 2.1636 - val_accuracy: 0.1329 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1score: 0.0000e+00\n",
      "Epoch 3/120\n",
      "20/20 [==============================] - 2s 118ms/step - loss: 0.4372 - accuracy: 0.5000 - precision: 0.5239 - recall: 0.4771 - f1score: 0.4963 - val_loss: 1.8792 - val_accuracy: 0.1395 - val_precision: 0.1454 - val_recall: 0.3358 - val_f1score: 0.2029\n",
      "Epoch 4/120\n",
      "20/20 [==============================] - 2s 119ms/step - loss: 0.3655 - accuracy: 0.6797 - precision: 0.7038 - recall: 0.5962 - f1score: 0.6446 - val_loss: 0.5116 - val_accuracy: 0.2425 - val_precision: 0.3140 - val_recall: 0.2370 - val_f1score: 0.2692\n",
      "Epoch 5/120\n",
      "20/20 [==============================] - 2s 119ms/step - loss: 0.3173 - accuracy: 0.7266 - precision: 0.7363 - recall: 0.6855 - f1score: 0.7083 - val_loss: 0.6734 - val_accuracy: 0.1628 - val_precision: 0.1928 - val_recall: 0.2260 - val_f1score: 0.2079\n",
      "Epoch 6/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.2675 - accuracy: 0.8125 - precision: 0.8204 - recall: 0.7526 - f1score: 0.7832 - val_loss: 0.4553 - val_accuracy: 0.4419 - val_precision: 0.4452 - val_recall: 0.4551 - val_f1score: 0.4496\n",
      "Epoch 7/120\n",
      "20/20 [==============================] - 2s 119ms/step - loss: 0.2245 - accuracy: 0.8359 - precision: 0.8463 - recall: 0.8015 - f1score: 0.8214 - val_loss: 0.4793 - val_accuracy: 0.3189 - val_precision: 0.3304 - val_recall: 0.2988 - val_f1score: 0.3134\n",
      "Epoch 8/120\n",
      "20/20 [==============================] - 2s 119ms/step - loss: 0.1893 - accuracy: 0.8828 - precision: 0.9030 - recall: 0.8560 - f1score: 0.8782 - val_loss: 0.4357 - val_accuracy: 0.4352 - val_precision: 0.4063 - val_recall: 0.4286 - val_f1score: 0.4164\n",
      "Epoch 9/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.1506 - accuracy: 0.9375 - precision: 0.9495 - recall: 0.9071 - f1score: 0.9273 - val_loss: 0.3479 - val_accuracy: 0.5681 - val_precision: 0.5743 - val_recall: 0.5422 - val_f1score: 0.5568\n",
      "Epoch 10/120\n",
      "20/20 [==============================] - 2s 122ms/step - loss: 0.1231 - accuracy: 0.9668 - precision: 0.9703 - recall: 0.9442 - f1score: 0.9568 - val_loss: 0.2906 - val_accuracy: 0.6512 - val_precision: 0.6911 - val_recall: 0.6095 - val_f1score: 0.6471\n",
      "Epoch 11/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.1061 - accuracy: 0.9785 - precision: 0.9788 - recall: 0.9513 - f1score: 0.9643 - val_loss: 0.2469 - val_accuracy: 0.7243 - val_precision: 0.7725 - val_recall: 0.6792 - val_f1score: 0.7217\n",
      "Epoch 12/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0922 - accuracy: 0.9883 - precision: 0.9875 - recall: 0.9818 - f1score: 0.9846 - val_loss: 0.2577 - val_accuracy: 0.6944 - val_precision: 0.7197 - val_recall: 0.6721 - val_f1score: 0.6944\n",
      "Epoch 13/120\n",
      "20/20 [==============================] - 2s 122ms/step - loss: 0.0841 - accuracy: 0.9941 - precision: 0.9923 - recall: 0.9923 - f1score: 0.9923 - val_loss: 0.2405 - val_accuracy: 0.7076 - val_precision: 0.7551 - val_recall: 0.6754 - val_f1score: 0.7124\n",
      "Epoch 14/120\n",
      "20/20 [==============================] - 2s 122ms/step - loss: 0.0769 - accuracy: 0.9941 - precision: 0.9942 - recall: 0.9942 - f1score: 0.9942 - val_loss: 0.2160 - val_accuracy: 0.7309 - val_precision: 0.7818 - val_recall: 0.6966 - val_f1score: 0.7362\n",
      "Epoch 15/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0709 - accuracy: 1.0000 - precision: 1.0000 - recall: 0.9942 - f1score: 0.9971 - val_loss: 0.1906 - val_accuracy: 0.7841 - val_precision: 0.8185 - val_recall: 0.7412 - val_f1score: 0.7767\n",
      "Epoch 16/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0646 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1802 - val_accuracy: 0.7774 - val_precision: 0.8304 - val_recall: 0.7509 - val_f1score: 0.7874\n",
      "Epoch 17/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0609 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1678 - val_accuracy: 0.8206 - val_precision: 0.8745 - val_recall: 0.7554 - val_f1score: 0.8083\n",
      "Epoch 18/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0567 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1608 - val_accuracy: 0.8173 - val_precision: 0.8783 - val_recall: 0.7735 - val_f1score: 0.8217\n",
      "Epoch 19/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0523 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1640 - val_accuracy: 0.7841 - val_precision: 0.8777 - val_recall: 0.7573 - val_f1score: 0.8120\n",
      "Epoch 20/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0511 - accuracy: 0.9961 - precision: 0.9953 - recall: 0.9953 - f1score: 0.9953 - val_loss: 0.1818 - val_accuracy: 0.7508 - val_precision: 0.8074 - val_recall: 0.7141 - val_f1score: 0.7574\n",
      "Epoch 21/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0523 - accuracy: 0.9941 - precision: 0.9942 - recall: 0.9904 - f1score: 0.9923 - val_loss: 0.2430 - val_accuracy: 0.6711 - val_precision: 0.7367 - val_recall: 0.6114 - val_f1score: 0.6675\n",
      "Epoch 22/120\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0517 - accuracy: 0.9941 - precision: 0.9961 - recall: 0.9865 - f1score: 0.9911 - val_loss: 0.1771 - val_accuracy: 0.7674 - val_precision: 0.8486 - val_recall: 0.7076 - val_f1score: 0.7696\n",
      "Epoch 23/120\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0482 - accuracy: 0.9980 - precision: 1.0000 - recall: 0.9962 - f1score: 0.9980 - val_loss: 0.2148 - val_accuracy: 0.7043 - val_precision: 0.7497 - val_recall: 0.6630 - val_f1score: 0.7032\n",
      "Epoch 24/120\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0485 - accuracy: 0.9941 - precision: 0.9972 - recall: 0.9895 - f1score: 0.9933 - val_loss: 0.1831 - val_accuracy: 0.7475 - val_precision: 0.8150 - val_recall: 0.7012 - val_f1score: 0.7531\n",
      "Epoch 25/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0481 - accuracy: 0.9941 - precision: 0.9962 - recall: 0.9904 - f1score: 0.9932 - val_loss: 0.1974 - val_accuracy: 0.7243 - val_precision: 0.7715 - val_recall: 0.6870 - val_f1score: 0.7256\n",
      "Epoch 26/120\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0430 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1717 - val_accuracy: 0.8073 - val_precision: 0.8304 - val_recall: 0.7340 - val_f1score: 0.7788\n",
      "Epoch 27/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0431 - accuracy: 0.9961 - precision: 0.9962 - recall: 0.9962 - f1score: 0.9962 - val_loss: 0.1543 - val_accuracy: 0.8106 - val_precision: 0.8632 - val_recall: 0.7611 - val_f1score: 0.8081\n",
      "Epoch 28/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0439 - accuracy: 0.9941 - precision: 0.9951 - recall: 0.9925 - f1score: 0.9938 - val_loss: 0.1692 - val_accuracy: 0.7841 - val_precision: 0.8309 - val_recall: 0.7496 - val_f1score: 0.7868\n",
      "Epoch 29/120\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0403 - accuracy: 0.9980 - precision: 1.0000 - recall: 0.9962 - f1score: 0.9980 - val_loss: 0.1525 - val_accuracy: 0.8040 - val_precision: 0.8722 - val_recall: 0.7812 - val_f1score: 0.8230\n",
      "Epoch 30/120\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0377 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1636 - val_accuracy: 0.8040 - val_precision: 0.8398 - val_recall: 0.7670 - val_f1score: 0.8010\n",
      "Epoch 31/120\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0387 - accuracy: 0.9980 - precision: 1.0000 - recall: 0.9981 - f1score: 0.9990 - val_loss: 0.1616 - val_accuracy: 0.8140 - val_precision: 0.8520 - val_recall: 0.7780 - val_f1score: 0.8121\n",
      "Epoch 32/120\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0364 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1580 - val_accuracy: 0.8007 - val_precision: 0.8555 - val_recall: 0.7735 - val_f1score: 0.8111\n",
      "Epoch 33/120\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0345 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1559 - val_accuracy: 0.8040 - val_precision: 0.8529 - val_recall: 0.7780 - val_f1score: 0.8122\n",
      "Epoch 34/120\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0351 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1536 - val_accuracy: 0.8073 - val_precision: 0.8696 - val_recall: 0.7812 - val_f1score: 0.8215\n",
      "Epoch 35/120\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0358 - accuracy: 0.9980 - precision: 0.9981 - recall: 0.9981 - f1score: 0.9981 - val_loss: 0.1565 - val_accuracy: 0.7973 - val_precision: 0.8551 - val_recall: 0.7598 - val_f1score: 0.8039\n",
      "Epoch 36/120\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0365 - accuracy: 0.9961 - precision: 0.9962 - recall: 0.9962 - f1score: 0.9962 - val_loss: 0.2201 - val_accuracy: 0.6977 - val_precision: 0.7459 - val_recall: 0.6702 - val_f1score: 0.7050\n",
      "Epoch 37/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0355 - accuracy: 0.9980 - precision: 0.9981 - recall: 0.9981 - f1score: 0.9981 - val_loss: 0.1822 - val_accuracy: 0.7608 - val_precision: 0.7960 - val_recall: 0.7122 - val_f1score: 0.7509\n",
      "Epoch 38/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0344 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1595 - val_accuracy: 0.7940 - val_precision: 0.8479 - val_recall: 0.7651 - val_f1score: 0.8037\n",
      "Epoch 39/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0328 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1506 - val_accuracy: 0.7973 - val_precision: 0.8573 - val_recall: 0.7735 - val_f1score: 0.8125\n",
      "Epoch 40/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0307 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1477 - val_accuracy: 0.8073 - val_precision: 0.8606 - val_recall: 0.7799 - val_f1score: 0.8177\n",
      "Epoch 41/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0322 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1464 - val_accuracy: 0.8140 - val_precision: 0.8672 - val_recall: 0.7831 - val_f1score: 0.8224\n",
      "Epoch 42/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0317 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1456 - val_accuracy: 0.8173 - val_precision: 0.8645 - val_recall: 0.7845 - val_f1score: 0.8214\n",
      "Epoch 43/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0312 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1443 - val_accuracy: 0.8073 - val_precision: 0.8636 - val_recall: 0.7877 - val_f1score: 0.8232\n",
      "Epoch 44/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0301 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1439 - val_accuracy: 0.8106 - val_precision: 0.8621 - val_recall: 0.7812 - val_f1score: 0.8187\n",
      "Epoch 45/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0303 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1429 - val_accuracy: 0.8140 - val_precision: 0.8620 - val_recall: 0.7845 - val_f1score: 0.8207\n",
      "Epoch 46/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0313 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1435 - val_accuracy: 0.8173 - val_precision: 0.8646 - val_recall: 0.7812 - val_f1score: 0.8204\n",
      "Epoch 47/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0307 - accuracy: 1.0000 - precision: 1.0000 - recall: 0.9981 - f1score: 0.9990 - val_loss: 0.1418 - val_accuracy: 0.8206 - val_precision: 0.8762 - val_recall: 0.7922 - val_f1score: 0.8314\n",
      "Epoch 48/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0287 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1404 - val_accuracy: 0.8306 - val_precision: 0.8692 - val_recall: 0.7909 - val_f1score: 0.8278\n",
      "Epoch 49/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0294 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1402 - val_accuracy: 0.8239 - val_precision: 0.8694 - val_recall: 0.7955 - val_f1score: 0.8302\n",
      "Epoch 50/120\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0291 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1402 - val_accuracy: 0.8239 - val_precision: 0.8696 - val_recall: 0.7955 - val_f1score: 0.8304\n",
      "Epoch 51/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0315 - accuracy: 1.0000 - precision: 1.0000 - recall: 0.9981 - f1score: 0.9990 - val_loss: 0.1396 - val_accuracy: 0.8239 - val_precision: 0.8696 - val_recall: 0.7955 - val_f1score: 0.8304\n",
      "Epoch 52/120\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0300 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1407 - val_accuracy: 0.8239 - val_precision: 0.8661 - val_recall: 0.7987 - val_f1score: 0.8305\n",
      "Epoch 53/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0296 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1401 - val_accuracy: 0.8206 - val_precision: 0.8721 - val_recall: 0.7987 - val_f1score: 0.8332\n",
      "Epoch 54/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0298 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1404 - val_accuracy: 0.8206 - val_precision: 0.8661 - val_recall: 0.7987 - val_f1score: 0.8303\n",
      "Epoch 55/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0293 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1404 - val_accuracy: 0.8239 - val_precision: 0.8724 - val_recall: 0.7955 - val_f1score: 0.8316\n",
      "Epoch 56/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0288 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1407 - val_accuracy: 0.8206 - val_precision: 0.8665 - val_recall: 0.7987 - val_f1score: 0.8305\n",
      "Epoch 57/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0305 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1400 - val_accuracy: 0.8239 - val_precision: 0.8707 - val_recall: 0.8019 - val_f1score: 0.8343\n",
      "Epoch 58/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0284 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1398 - val_accuracy: 0.8306 - val_precision: 0.8700 - val_recall: 0.7987 - val_f1score: 0.8321\n",
      "Epoch 59/120\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0285 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1387 - val_accuracy: 0.8272 - val_precision: 0.8769 - val_recall: 0.8051 - val_f1score: 0.8387\n",
      "Epoch 60/120\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0275 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1388 - val_accuracy: 0.8272 - val_precision: 0.8735 - val_recall: 0.8051 - val_f1score: 0.8371\n",
      "Epoch 61/120\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0276 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1389 - val_accuracy: 0.8239 - val_precision: 0.8735 - val_recall: 0.8051 - val_f1score: 0.8371\n",
      "Epoch 62/120\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0274 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1389 - val_accuracy: 0.8239 - val_precision: 0.8704 - val_recall: 0.8051 - val_f1score: 0.8358\n",
      "Epoch 63/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0268 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1390 - val_accuracy: 0.8239 - val_precision: 0.8704 - val_recall: 0.8051 - val_f1score: 0.8358\n",
      "Epoch 64/120\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0285 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1383 - val_accuracy: 0.8239 - val_precision: 0.8736 - val_recall: 0.8051 - val_f1score: 0.8371\n",
      "Epoch 65/120\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0273 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1381 - val_accuracy: 0.8272 - val_precision: 0.8796 - val_recall: 0.8051 - val_f1score: 0.8400\n",
      "Epoch 66/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0275 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1389 - val_accuracy: 0.8239 - val_precision: 0.8730 - val_recall: 0.8019 - val_f1score: 0.8352\n",
      "Epoch 67/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0277 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1394 - val_accuracy: 0.8239 - val_precision: 0.8670 - val_recall: 0.8019 - val_f1score: 0.8326\n",
      "Epoch 68/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0265 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1393 - val_accuracy: 0.8239 - val_precision: 0.8670 - val_recall: 0.8019 - val_f1score: 0.8326\n",
      "Epoch 69/120\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0281 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1388 - val_accuracy: 0.8239 - val_precision: 0.8763 - val_recall: 0.8019 - val_f1score: 0.8367\n",
      "Epoch 70/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0266 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1387 - val_accuracy: 0.8239 - val_precision: 0.8763 - val_recall: 0.8019 - val_f1score: 0.8367\n",
      "Epoch 71/120\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0289 - accuracy: 0.9980 - precision: 1.0000 - recall: 0.9981 - f1score: 0.9990 - val_loss: 0.1387 - val_accuracy: 0.8272 - val_precision: 0.8755 - val_recall: 0.7955 - val_f1score: 0.8328\n",
      "Epoch 72/120\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0279 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1387 - val_accuracy: 0.8272 - val_precision: 0.8726 - val_recall: 0.7955 - val_f1score: 0.8316\n",
      "Epoch 73/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0271 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1389 - val_accuracy: 0.8239 - val_precision: 0.8726 - val_recall: 0.7955 - val_f1score: 0.8316\n",
      "Epoch 74/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0275 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1391 - val_accuracy: 0.8239 - val_precision: 0.8734 - val_recall: 0.8019 - val_f1score: 0.8353\n",
      "Epoch 75/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0273 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1386 - val_accuracy: 0.8239 - val_precision: 0.8755 - val_recall: 0.7987 - val_f1score: 0.8346\n",
      "Epoch 76/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0261 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1386 - val_accuracy: 0.8239 - val_precision: 0.8762 - val_recall: 0.8019 - val_f1score: 0.8366\n",
      "Epoch 77/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0264 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1386 - val_accuracy: 0.8239 - val_precision: 0.8762 - val_recall: 0.8019 - val_f1score: 0.8366\n",
      "Epoch 78/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0268 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1386 - val_accuracy: 0.8239 - val_precision: 0.8762 - val_recall: 0.8019 - val_f1score: 0.8366\n",
      "Epoch 79/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0283 - accuracy: 1.0000 - precision: 0.9981 - recall: 1.0000 - f1score: 0.9991 - val_loss: 0.1383 - val_accuracy: 0.8206 - val_precision: 0.8757 - val_recall: 0.7955 - val_f1score: 0.8328\n",
      "Epoch 80/120\n",
      "20/20 [==============================] - 2s 122ms/step - loss: 0.0272 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1381 - val_accuracy: 0.8140 - val_precision: 0.8753 - val_recall: 0.7955 - val_f1score: 0.8325\n",
      "Epoch 81/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0279 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1382 - val_accuracy: 0.8173 - val_precision: 0.8726 - val_recall: 0.7955 - val_f1score: 0.8313\n",
      "Epoch 82/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0282 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1381 - val_accuracy: 0.8140 - val_precision: 0.8753 - val_recall: 0.7955 - val_f1score: 0.8325\n",
      "Epoch 83/120\n",
      "20/20 [==============================] - 2s 122ms/step - loss: 0.0268 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1380 - val_accuracy: 0.8173 - val_precision: 0.8753 - val_recall: 0.7955 - val_f1score: 0.8325\n",
      "Epoch 84/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0265 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1380 - val_accuracy: 0.8173 - val_precision: 0.8753 - val_recall: 0.7955 - val_f1score: 0.8325\n",
      "Epoch 85/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0277 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1380 - val_accuracy: 0.8173 - val_precision: 0.8753 - val_recall: 0.7955 - val_f1score: 0.8325\n",
      "Epoch 86/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0272 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1380 - val_accuracy: 0.8173 - val_precision: 0.8721 - val_recall: 0.7922 - val_f1score: 0.8295\n",
      "Epoch 87/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0263 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1381 - val_accuracy: 0.8173 - val_precision: 0.8751 - val_recall: 0.7922 - val_f1score: 0.8308\n",
      "Epoch 88/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0265 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1380 - val_accuracy: 0.8173 - val_precision: 0.8779 - val_recall: 0.7922 - val_f1score: 0.8321\n",
      "Epoch 89/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0278 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1380 - val_accuracy: 0.8206 - val_precision: 0.8779 - val_recall: 0.7922 - val_f1score: 0.8321\n",
      "Epoch 90/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0272 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1381 - val_accuracy: 0.8206 - val_precision: 0.8758 - val_recall: 0.7955 - val_f1score: 0.8330\n",
      "Epoch 91/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0276 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1382 - val_accuracy: 0.8206 - val_precision: 0.8779 - val_recall: 0.7922 - val_f1score: 0.8321\n",
      "Epoch 92/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0262 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1381 - val_accuracy: 0.8239 - val_precision: 0.8779 - val_recall: 0.7922 - val_f1score: 0.8321\n",
      "Epoch 93/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0264 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1382 - val_accuracy: 0.8206 - val_precision: 0.8779 - val_recall: 0.7922 - val_f1score: 0.8321\n",
      "Epoch 94/120\n",
      "20/20 [==============================] - 2s 122ms/step - loss: 0.0274 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1382 - val_accuracy: 0.8206 - val_precision: 0.8779 - val_recall: 0.7922 - val_f1score: 0.8321\n",
      "Epoch 95/120\n",
      "20/20 [==============================] - 2s 122ms/step - loss: 0.0277 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1384 - val_accuracy: 0.8206 - val_precision: 0.8749 - val_recall: 0.7922 - val_f1score: 0.8308\n",
      "Epoch 96/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0271 - accuracy: 1.0000 - precision: 1.0000 - recall: 0.9972 - f1score: 0.9986 - val_loss: 0.1384 - val_accuracy: 0.8206 - val_precision: 0.8779 - val_recall: 0.7922 - val_f1score: 0.8321\n",
      "Epoch 97/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0270 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1379 - val_accuracy: 0.8206 - val_precision: 0.8749 - val_recall: 0.7922 - val_f1score: 0.8308\n",
      "Epoch 98/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0266 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1378 - val_accuracy: 0.8206 - val_precision: 0.8779 - val_recall: 0.7922 - val_f1score: 0.8321\n",
      "Epoch 99/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0266 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1378 - val_accuracy: 0.8206 - val_precision: 0.8779 - val_recall: 0.7922 - val_f1score: 0.8321\n",
      "Epoch 100/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0287 - accuracy: 0.9980 - precision: 1.0000 - recall: 0.9944 - f1score: 0.9971 - val_loss: 0.1378 - val_accuracy: 0.8206 - val_precision: 0.8779 - val_recall: 0.7922 - val_f1score: 0.8321\n",
      "Epoch 101/120\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0262 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1380 - val_accuracy: 0.8206 - val_precision: 0.8749 - val_recall: 0.7922 - val_f1score: 0.8308\n",
      "Epoch 102/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0270 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1380 - val_accuracy: 0.8206 - val_precision: 0.8749 - val_recall: 0.7922 - val_f1score: 0.8308\n",
      "Epoch 103/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0272 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1379 - val_accuracy: 0.8206 - val_precision: 0.8749 - val_recall: 0.7922 - val_f1score: 0.8308\n",
      "Epoch 104/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0266 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1379 - val_accuracy: 0.8206 - val_precision: 0.8779 - val_recall: 0.7922 - val_f1score: 0.8321\n",
      "Epoch 105/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0271 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1379 - val_accuracy: 0.8206 - val_precision: 0.8749 - val_recall: 0.7922 - val_f1score: 0.8308\n",
      "Epoch 106/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0280 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1378 - val_accuracy: 0.8206 - val_precision: 0.8749 - val_recall: 0.7922 - val_f1score: 0.8308\n",
      "Epoch 107/120\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0275 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1380 - val_accuracy: 0.8206 - val_precision: 0.8749 - val_recall: 0.7922 - val_f1score: 0.8308\n",
      "Epoch 108/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0267 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1379 - val_accuracy: 0.8206 - val_precision: 0.8779 - val_recall: 0.7922 - val_f1score: 0.8321\n",
      "Epoch 109/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0281 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1380 - val_accuracy: 0.8206 - val_precision: 0.8749 - val_recall: 0.7922 - val_f1score: 0.8308\n",
      "Epoch 110/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0277 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1379 - val_accuracy: 0.8206 - val_precision: 0.8749 - val_recall: 0.7922 - val_f1score: 0.8308\n",
      "Epoch 111/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0259 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1379 - val_accuracy: 0.8206 - val_precision: 0.8749 - val_recall: 0.7922 - val_f1score: 0.8308\n",
      "Epoch 112/120\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0270 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1378 - val_accuracy: 0.8206 - val_precision: 0.8749 - val_recall: 0.7922 - val_f1score: 0.8308\n",
      "Epoch 113/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0272 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1378 - val_accuracy: 0.8206 - val_precision: 0.8749 - val_recall: 0.7922 - val_f1score: 0.8308\n",
      "Epoch 114/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0273 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1379 - val_accuracy: 0.8206 - val_precision: 0.8779 - val_recall: 0.7922 - val_f1score: 0.8321\n",
      "Epoch 115/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0263 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1379 - val_accuracy: 0.8206 - val_precision: 0.8779 - val_recall: 0.7922 - val_f1score: 0.8321\n",
      "Epoch 116/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0281 - accuracy: 1.0000 - precision: 1.0000 - recall: 0.9944 - f1score: 0.9971 - val_loss: 0.1379 - val_accuracy: 0.8206 - val_precision: 0.8749 - val_recall: 0.7922 - val_f1score: 0.8308\n",
      "Epoch 117/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0260 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1380 - val_accuracy: 0.8206 - val_precision: 0.8779 - val_recall: 0.7922 - val_f1score: 0.8321\n",
      "Epoch 118/120\n",
      "20/20 [==============================] - 2s 122ms/step - loss: 0.0255 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1379 - val_accuracy: 0.8206 - val_precision: 0.8779 - val_recall: 0.7922 - val_f1score: 0.8321\n",
      "Epoch 119/120\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 0.0265 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1381 - val_accuracy: 0.8206 - val_precision: 0.8779 - val_recall: 0.7922 - val_f1score: 0.8321\n",
      "Epoch 120/120\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.0265 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1381 - val_accuracy: 0.8206 - val_precision: 0.8749 - val_recall: 0.7922 - val_f1score: 0.8308\n"
     ]
    }
   ],
   "source": [
    "def recall(y_target, y_pred):\n",
    "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
    "    # round : 반올림한다\n",
    "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "\n",
    "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
    "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
    "\n",
    "    # (True Positive + False Negative) = 실제 값이 1(Positive) 전체\n",
    "    count_true_positive_false_negative = K.sum(y_target_yn)\n",
    "\n",
    "    # Recall =  (True Positive) / (True Positive + False Negative)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    recall = count_true_positive / (count_true_positive_false_negative + K.epsilon())\n",
    "\n",
    "    # return a single tensor value\n",
    "    return recall\n",
    "\n",
    "\n",
    "def precision(y_target, y_pred):\n",
    "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
    "    # round : 반올림한다\n",
    "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "\n",
    "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
    "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
    "\n",
    "    # (True Positive + False Positive) = 예측 값이 1(Positive) 전체\n",
    "    count_true_positive_false_positive = K.sum(y_pred_yn)\n",
    "\n",
    "    # Precision = (True Positive) / (True Positive + False Positive)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    precision = count_true_positive / (count_true_positive_false_positive + K.epsilon())\n",
    "\n",
    "    # return a single tensor value\n",
    "    return precision\n",
    "\n",
    "\n",
    "def f1score(y_target, y_pred):\n",
    "    _recall = recall(y_target, y_pred)\n",
    "    _precision = precision(y_target, y_pred)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    _f1score = ( 2 * _recall * _precision) / (_recall + _precision+ K.epsilon())\n",
    "    \n",
    "    # return a single tensor value\n",
    "    return _f1score\n",
    "\n",
    "\n",
    "\n",
    "#model = create_model()\n",
    "#model = ResNet()\n",
    "#model = mobile_net()\n",
    "#model = xception()\n",
    "model = densenet()\n",
    "#model = resnet()\n",
    "\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=learning_rate,\n",
    "                                                          decay_steps=training_epochs * 10,\n",
    "                                                          decay_rate=0.5,\n",
    "                                                          staircase=True)\n",
    "\n",
    "\n",
    "\n",
    "# lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "#     initial_learning_rate=learning_rate,\n",
    "#     decay_steps=100000,\n",
    "#     decay_rate=0.96)\n",
    "\n",
    "# optimizer는 Adam, loss는 sparse categorical crossentropy 사용\n",
    "# label이 ont-hot으로 encoding 안 된 경우에 sparse categorical corssentropy 및 sparse categorical accuracy 사용\n",
    "model.compile(keras.optimizers.Adam(lr_schedule), loss = 'binary_crossentropy', metrics=['accuracy', precision, recall, f1score])\n",
    "\n",
    "# model.compile(\n",
    "#     optimizer='adam',\n",
    "#     loss='binary_crossentropy', \n",
    "#     metrics=['accuracy', precision, recall, f1score],\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Train!\n",
    "history = model.fit(x_train, y_train, steps_per_epoch=training_epochs,\n",
    "         epochs=120, validation_data = (x_val,y_val),validation_steps=validation_steps)\n",
    "model.save('animal_model_man.h5')\n",
    "# epochs = 30\n",
    "# history = model.fit(\n",
    "#     training_set, \n",
    "#     epochs=epochs,\n",
    "#     steps_per_epoch=training_set.samples / epochs, \n",
    "#     validation_data=val_set,\n",
    "#     validation_steps=val_set.samples / epochs,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hnA6J84ElBkJ",
    "outputId": "d922284c-c03f-436d-8b2d-383eba89a817"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n",
      "index: 0  actual y: 3  answer y: 4  prediction: [ 1.8484335  1.1221274  2.1676385 24.287315  59.603584   3.2275915\n",
      "  2.5246496]\n",
      "index: 1  actual y: 0  answer y: 0  prediction: [93.60515    3.2316785  1.8675348  3.1175637  1.5538856  2.8197954\n",
      "  2.359176 ]\n",
      "index: 2  actual y: 2  answer y: 2  prediction: [ 0.91205156  1.9186261  96.579994    1.0081613   1.4212829   4.8555775\n",
      "  2.322705  ]\n",
      "index: 3  actual y: 2  answer y: 2  prediction: [ 0.975441    1.3863869  95.65664     0.94386256  1.3695673   4.0425634\n",
      "  1.5572246 ]\n",
      "index: 4  actual y: 5  answer y: 3  prediction: [ 3.1883206  1.089025   4.801732  62.4909     2.5398657 23.498428\n",
      "  2.173149 ]\n",
      "index: 5  actual y: 0  answer y: 0  prediction: [95.58491    2.1120358  1.3402631  1.6567668  1.2416791  2.1781232\n",
      "  1.777985 ]\n",
      "index: 6  actual y: 0  answer y: 0  prediction: [95.74666    1.8295354  1.4090124  1.6976532  1.2121563  1.9834764\n",
      "  1.6836679]\n",
      "index: 7  actual y: 1  answer y: 1  prediction: [ 2.288584   91.6692      1.2053517   1.047128    0.56846213  2.573822\n",
      "  1.4953933 ]\n",
      "index: 8  actual y: 2  answer y: 2  prediction: [ 0.6488079  2.4658003 95.15135    0.8379411  1.0617204  3.7750118\n",
      "  1.5635567]\n",
      "index: 9  actual y: 3  answer y: 5  prediction: [ 1.2057242  1.1940502  5.129861   8.053784   1.1432941 78.73912\n",
      "  2.501154 ]\n",
      "index: 10  actual y: 4  answer y: 4  prediction: [ 1.0187149  2.2188919  1.7567662  1.4459434 96.37464    1.7301708\n",
      "  2.652387 ]\n",
      "index: 11  actual y: 6  answer y: 6  prediction: [ 0.32042038  2.5427217   2.0170906   1.6932232   2.7264373   1.789188\n",
      " 94.19591   ]\n",
      "index: 12  actual y: 4  answer y: 4  prediction: [ 0.68722993  1.3272756   1.2759615   1.5335115  96.34491     1.4575604\n",
      "  1.9011073 ]\n",
      "index: 13  actual y: 1  answer y: 1  prediction: [ 0.7533528 88.07544    3.5009887  1.1451818  5.081306   1.8494809\n",
      "  2.530783 ]\n",
      "index: 14  actual y: 5  answer y: 4  prediction: [ 5.3743687   2.8445435   1.6956981   0.68837315 56.167366    9.831823\n",
      "  5.8261385 ]\n",
      "index: 15  actual y: 6  answer y: 6  prediction: [ 0.4542777  2.422689   1.9669441  1.3902249  1.6942838  2.3187804\n",
      " 94.98009  ]\n",
      "index: 16  actual y: 4  answer y: 4  prediction: [ 0.85555726  1.8120074   1.7860436   1.6190913  96.26715     1.8951148\n",
      "  2.1077585 ]\n",
      "index: 17  actual y: 4  answer y: 4  prediction: [ 0.6251421  1.2582508  1.1776466  1.1028395 96.392      1.8392999\n",
      "  1.7512051]\n",
      "index: 18  actual y: 2  answer y: 2  prediction: [ 0.73176324  0.80897    95.14643     2.4047084   1.6705605   4.6274824\n",
      "  1.7207608 ]\n",
      "index: 19  actual y: 0  answer y: 0  prediction: [95.90619    1.7440833  1.3288503  1.6797843  1.3669115  1.3792263\n",
      "  1.3442355]\n",
      "index: 20  actual y: 1  answer y: 0  prediction: [57.41353    5.7852507  2.9850445  5.8180137  1.8192133 10.925649\n",
      "  2.372182 ]\n",
      "index: 21  actual y: 1  answer y: 1  prediction: [ 1.7458777 94.325836   1.6696889  1.5170637  1.4058214  2.7466424\n",
      "  1.753416 ]\n",
      "index: 22  actual y: 0  answer y: 0  prediction: [95.90333    1.6641818  1.5964553  1.2336628  1.4405978  2.6179223\n",
      "  1.9773664]\n",
      "index: 23  actual y: 0  answer y: 0  prediction: [94.136566   2.857604   1.549429   2.5331724  1.039593   2.9484463\n",
      "  1.9378957]\n",
      "index: 24  actual y: 5  answer y: 5  prediction: [ 2.2486382   2.3515968   3.7130058   0.31912586  0.6511768  92.43594\n",
      "  1.5189614 ]\n",
      "index: 25  actual y: 2  answer y: 2  prediction: [ 0.8921708  1.6508602 96.31914    1.2882501  1.7094328  4.2674212\n",
      "  1.9128904]\n",
      "index: 26  actual y: 0  answer y: 0  prediction: [94.45393    3.0524905  1.328747   2.80745    1.1082916  2.2307613\n",
      "  1.9493116]\n",
      "index: 27  actual y: 1  answer y: 1  prediction: [ 1.436442  94.79235    2.0965335  1.394433   1.0826534  2.3701353\n",
      "  2.2299192]\n",
      "index: 28  actual y: 3  answer y: 3  prediction: [ 3.1044753  1.5032841  2.7978554 94.37014    3.2917671  1.260615\n",
      "  1.1133001]\n",
      "index: 29  actual y: 6  answer y: 6  prediction: [ 0.42655542  2.6488407   1.71803     1.2331058   3.4567463   1.6451107\n",
      " 93.65551   ]\n",
      "index: 30  actual y: 2  answer y: 2  prediction: [ 1.0243418  1.5092788 95.845345   1.1462171  1.9296596  3.6411366\n",
      "  1.8621174]\n",
      "index: 31  actual y: 6  answer y: 6  prediction: [ 0.56061065  1.8884554   1.7701633   1.1183313   1.8372245   2.2088947\n",
      " 95.89058   ]\n",
      "index: 32  actual y: 6  answer y: 6  prediction: [ 0.23550346  1.4050794   1.8072      1.484264    1.4544966   2.2184367\n",
      " 95.11266   ]\n",
      "index: 33  actual y: 4  answer y: 4  prediction: [ 1.0008764   2.4268808   1.6472069   0.96119845 96.351555    1.7674955\n",
      "  2.7558851 ]\n",
      "index: 34  actual y: 2  answer y: 2  prediction: [ 1.0882435  1.6450446 95.68454    0.5775718  2.3863752  5.1056437\n",
      "  2.042655 ]\n",
      "index: 35  actual y: 1  answer y: 1  prediction: [ 1.6853557 94.19783    1.7612374  1.6185127  1.4910613  3.1860032\n",
      "  1.7461611]\n",
      "index: 36  actual y: 4  answer y: 4  prediction: [ 0.7612624  1.5962582  1.4586635  1.8655992 95.84043    1.599245\n",
      "  2.4263675]\n",
      "index: 37  actual y: 2  answer y: 2  prediction: [ 0.9966552   1.7461578  96.648605    0.63299656  1.9630519   4.0517707\n",
      "  1.8564284 ]\n",
      "index: 38  actual y: 0  answer y: 0  prediction: [94.94737    2.8079298  1.1709701  1.9426829  1.1520605  2.1474843\n",
      "  1.6764382]\n",
      "index: 39  actual y: 5  answer y: 5  prediction: [ 1.4013863   1.5529689   3.0058594   0.2967581   0.66776866 92.4631\n",
      "  1.7141647 ]\n",
      "index: 40  actual y: 5  answer y: 4  prediction: [ 8.099189    2.2577367   2.2562075   0.69356817 35.738747   20.462566\n",
      "  1.4861695 ]\n",
      "index: 41  actual y: 1  answer y: 1  prediction: [ 3.0272648 68.90695    1.7523597  1.901066   1.1574258 17.184887\n",
      "  2.3133273]\n",
      "index: 42  actual y: 3  answer y: 3  prediction: [ 2.231212   1.0598623  2.9031672 94.43168    4.480443   1.308563\n",
      "  1.1718926]\n",
      "index: 43  actual y: 1  answer y: 1  prediction: [ 1.4915875 95.38985    1.6894976  1.0277421  1.3991432  2.1766343\n",
      "  2.5254476]\n",
      "index: 44  actual y: 1  answer y: 1  prediction: [ 1.1893059 93.71113    2.0728338  1.5931938  1.6818725  1.8491572\n",
      "  2.9448838]\n",
      "index: 45  actual y: 0  answer y: 3  prediction: [21.239073   3.323161   4.0732937 64.941055   9.318452   0.7076489\n",
      "  2.2701554]\n",
      "index: 46  actual y: 2  answer y: 2  prediction: [ 1.0703735  1.6439393 95.691795   0.8188623  2.4304926  4.6780934\n",
      "  1.4598167]\n",
      "index: 47  actual y: 3  answer y: 3  prediction: [ 1.6155753   1.3426965   2.9580388  93.70672     2.8486333   0.51514816\n",
      "  1.4474888 ]\n",
      "index: 48  actual y: 6  answer y: 6  prediction: [ 0.2215601  1.3100399  1.8961678  1.6693741  1.3187219  1.8784198\n",
      " 92.39117  ]\n",
      "index: 49  actual y: 3  answer y: 3  prediction: [ 2.2164495  1.2931436  3.1172304 94.47823    4.214304   1.1660905\n",
      "  1.358663 ]\n",
      "index: 50  actual y: 5  answer y: 5  prediction: [ 3.6704705   1.9469943   1.1862302   0.16617514  0.93407613 64.34763\n",
      " 25.42829   ]\n",
      "index: 51  actual y: 0  answer y: 0  prediction: [95.95587    1.6891062  1.3943229  1.5012125  1.1858355  1.8868142\n",
      "  1.5622296]\n",
      "index: 52  actual y: 6  answer y: 5  prediction: [13.72936    2.7256074  3.62668    0.2840352  0.5473227 87.63339\n",
      "  6.4477854]\n",
      "index: 53  actual y: 1  answer y: 1  prediction: [ 1.240238  35.154842   1.7443875  1.2590449  0.8954523 30.061987\n",
      " 23.318163 ]\n",
      "index: 54  actual y: 4  answer y: 4  prediction: [ 1.1151278  2.4523206  2.018883   1.7611871 96.346596   1.9172357\n",
      "  2.5017686]\n",
      "index: 55  actual y: 2  answer y: 2  prediction: [ 0.50935125  0.7908429  91.26592     1.3846501   1.3901758   2.8525698\n",
      "  1.0465624 ]\n",
      "index: 56  actual y: 1  answer y: 1  prediction: [ 3.138501  91.89495    2.4090235  2.8545237  0.8989774  2.4012995\n",
      "  1.8041123]\n",
      "index: 57  actual y: 3  answer y: 0  prediction: [80.904785   3.5247042  2.4184484 16.121525   2.8389864  1.8226858\n",
      "  2.3543448]\n",
      "index: 58  actual y: 2  answer y: 2  prediction: [ 0.98067886  1.2709575  95.76545     0.949999    2.12264     4.206378\n",
      "  1.6420975 ]\n",
      "index: 59  actual y: 4  answer y: 4  prediction: [16.812302    1.4045854   1.5485325   0.81895673 69.05296     1.1113771\n",
      "  2.1295135 ]\n",
      "index: 60  actual y: 4  answer y: 4  prediction: [ 0.66349155  1.5141064   1.4273874   1.6433371  96.73319     1.4764196\n",
      "  1.8615525 ]\n",
      "index: 61  actual y: 5  answer y: 5  prediction: [ 2.2363932   1.5674895   5.026509    0.37129936  0.6525565  92.02501\n",
      "  3.1014082 ]\n",
      "index: 62  actual y: 4  answer y: 4  prediction: [ 0.6450076  1.499574   1.3047192  0.9733498 97.39691    1.082727\n",
      "  1.9443824]\n",
      "index: 63  actual y: 2  answer y: 2  prediction: [ 0.74753356  1.5181755  94.73976     0.7124367   1.6041336   3.7448559\n",
      "  0.9093184 ]\n",
      "index: 64  actual y: 4  answer y: 4  prediction: [ 5.5132995  1.3817147  2.1660602  0.9782925 56.21093    3.8409126\n",
      " 14.027638 ]\n",
      "index: 65  actual y: 5  answer y: 5  prediction: [ 1.6693726   1.8868146   2.6971188   0.41129923  0.40537548 93.422615\n",
      "  1.307957  ]\n",
      "index: 66  actual y: 1  answer y: 1  prediction: [ 1.3361999 94.84016    1.9509144  1.2452719  1.3216546  2.3255384\n",
      "  2.0478559]\n",
      "index: 67  actual y: 1  answer y: 6  prediction: [ 8.443735   9.64789    2.2008007  0.8456088  3.586414   2.279312\n",
      " 42.07522  ]\n",
      "index: 68  actual y: 6  answer y: 6  prediction: [ 0.6451928  2.3592412  1.3970705  1.1546623  2.292056   2.4160235\n",
      " 94.91797  ]\n",
      "index: 69  actual y: 3  answer y: 4  prediction: [ 3.8401499  1.1549863  2.63687   11.956362  60.82464    1.8721037\n",
      "  2.2315733]\n",
      "index: 70  actual y: 1  answer y: 1  prediction: [ 3.8466613 83.90734    1.252092   2.2097933  0.7827312  1.5379691\n",
      "  2.898468 ]\n",
      "index: 71  actual y: 0  answer y: 0  prediction: [90.98114    4.490195   1.6880062  2.0077837  1.2043512  5.104766\n",
      "  2.1527967]\n",
      "index: 72  actual y: 5  answer y: 5  prediction: [ 1.6686777  1.4387975  3.9137673  0.5062222  0.4336109 93.00945\n",
      "  2.060941 ]\n",
      "index: 73  actual y: 0  answer y: 0  prediction: [37.222538    2.0092177   0.79384977  4.390111   18.113583    1.7359707\n",
      " 15.039177  ]\n",
      "index: 74  actual y: 4  answer y: 0  prediction: [86.08926     2.7716436   2.1091483   0.81779295  5.2917786   4.909936\n",
      "  1.4314387 ]\n",
      "index: 75  actual y: 0  answer y: 0  prediction: [95.707825   1.8077774  1.45919    1.8036972  1.2492205  1.7918587\n",
      "  1.6184267]\n",
      "index: 76  actual y: 0  answer y: 6  prediction: [16.045547   7.992824   2.681903   9.878689   1.3607947  7.794074\n",
      " 26.307604 ]\n",
      "index: 77  actual y: 5  answer y: 4  prediction: [ 0.5298139  1.5903152  1.3551048  1.2141433 96.99346    1.980371\n",
      "  1.6647311]\n",
      "index: 78  actual y: 4  answer y: 5  prediction: [ 0.43357942  0.8882952   2.339013    0.7168722   3.7791972  78.465675\n",
      "  2.0173209 ]\n",
      "index: 79  actual y: 2  answer y: 2  prediction: [ 0.6738948   1.0624585  96.83322     0.78223366  1.9062576   3.2247066\n",
      "  1.5283933 ]\n",
      "index: 80  actual y: 2  answer y: 2  prediction: [ 0.7350264   1.380201   89.56368     0.17080145  1.0762131   8.897329\n",
      "  1.171335  ]\n",
      "index: 81  actual y: 6  answer y: 6  prediction: [ 0.4369813  1.8146135  1.5261477  0.920239   1.2070473  1.9479951\n",
      " 95.70207  ]\n",
      "index: 82  actual y: 2  answer y: 2  prediction: [ 0.5392518   0.65380937 95.75277     0.59070355  1.9086434   2.6342518\n",
      "  1.0212361 ]\n",
      "index: 83  actual y: 1  answer y: 1  prediction: [ 1.5506623 94.76004    1.329878   1.3099838  1.1987133  1.9437002\n",
      "  1.8941396]\n",
      "index: 84  actual y: 3  answer y: 3  prediction: [18.148542   2.2595735  3.4576066 76.22443    5.1926417  1.3556262\n",
      "  1.1693064]\n",
      "index: 85  actual y: 4  answer y: 4  prediction: [ 7.7639      0.82165414  1.0525699   0.85762125 59.087288    3.3144004\n",
      "  1.5643225 ]\n",
      "index: 86  actual y: 3  answer y: 3  prediction: [12.145307   1.2473972  2.8409362 78.81267    5.503013   1.956506\n",
      "  1.1533493]\n",
      "index: 87  actual y: 4  answer y: 4  prediction: [ 3.9228215  1.1035578 20.853905   2.2836013 76.872894   2.968373\n",
      "  2.5868046]\n",
      "index: 88  actual y: 4  answer y: 4  prediction: [ 0.83794945  1.0331926   1.3488458   2.1995692  95.349815    1.1303891\n",
      "  2.227958  ]\n",
      "index: 89  actual y: 6  answer y: 6  prediction: [ 0.5038164  2.1934319  1.6753701  1.790997   1.4818479  2.8541927\n",
      " 94.64587  ]\n",
      "index: 90  actual y: 3  answer y: 3  prediction: [ 3.3937984  1.1156442  3.2218273 91.60308    5.732883   1.5512602\n",
      "  1.1415206]\n",
      "index: 91  actual y: 5  answer y: 5  prediction: [ 1.7437959  2.6727319  2.757284   0.5717211  0.4549771 92.38923\n",
      "  1.3507377]\n",
      "index: 92  actual y: 0  answer y: 0  prediction: [95.43157    2.025385   1.5328968  1.9333383  1.5248879  1.9908315\n",
      "  1.885584 ]\n",
      "index: 93  actual y: 3  answer y: 3  prediction: [ 2.7374039  1.2775735  2.2364604 53.742363  43.149143   1.0631601\n",
      "  1.488122 ]\n",
      "index: 94  actual y: 5  answer y: 5  prediction: [ 1.6060578   2.2281322   2.6521492   0.2585809   0.55307883 92.84536\n",
      "  1.4323143 ]\n",
      "index: 95  actual y: 0  answer y: 0  prediction: [94.308975   3.0564635  1.492512   2.2990267  1.2960062  2.161115\n",
      "  1.601403 ]\n",
      "index: 96  actual y: 3  answer y: 3  prediction: [ 1.4493884  1.1682919  3.3629563 91.68514    5.9254313  1.4883039\n",
      "  1.8142356]\n",
      "index: 97  actual y: 4  answer y: 4  prediction: [ 0.7276336  1.8045999  1.6761836  1.5719386 96.80725    1.7662704\n",
      "  2.101136 ]\n",
      "index: 98  actual y: 0  answer y: 0  prediction: [94.24678    3.015517   1.770606   2.5295305  1.524353   2.716792\n",
      "  2.2607322]\n",
      "index: 99  actual y: 1  answer y: 1  prediction: [ 1.253129  95.142235   1.9876553  1.3984678  1.1692833  2.4600382\n",
      "  1.8886168]\n",
      "index: 100  actual y: 5  answer y: 5  prediction: [ 1.8685474   2.4493601   3.0718997   0.26944524  0.5363459  92.62794\n",
      "  1.2903141 ]\n",
      "index: 101  actual y: 5  answer y: 3  prediction: [ 4.2563157  8.2547     5.3582897 36.378876   2.689432  21.556692\n",
      "  6.89383  ]\n",
      "index: 102  actual y: 2  answer y: 2  prediction: [ 0.8859003  1.4496546 97.10845    0.5196911  2.0877056  4.449748\n",
      "  2.3013473]\n",
      "index: 103  actual y: 5  answer y: 5  prediction: [ 0.68359554  1.0304035   2.5328548   0.24290726  4.880946   73.1501\n",
      "  6.5819464 ]\n",
      "index: 104  actual y: 2  answer y: 2  prediction: [ 0.48221666  0.64433205 95.594826    0.76011413  1.1590548   2.703129\n",
      "  0.73469776]\n",
      "index: 105  actual y: 2  answer y: 2  prediction: [ 0.53439254  1.2452762  93.849594    0.68229014  1.5459313   3.5287645\n",
      "  0.6358733 ]\n",
      "index: 106  actual y: 3  answer y: 3  prediction: [17.708666   2.6784885  3.934192  78.66508    4.856408   1.654973\n",
      "  1.9714617]\n",
      "index: 107  actual y: 2  answer y: 2  prediction: [ 1.0002317  1.1144829 95.65319    1.3889513  1.8990003  4.6664248\n",
      "  1.5919728]\n",
      "index: 108  actual y: 4  answer y: 4  prediction: [ 0.95456487  2.4423835   2.167766    1.431681   96.589035    2.0056217\n",
      "  2.399971  ]\n",
      "index: 109  actual y: 3  answer y: 3  prediction: [ 2.1010427  1.0624576  2.9108572 93.19728    3.8538435  0.8676019\n",
      "  1.1190703]\n",
      "index: 110  actual y: 5  answer y: 5  prediction: [ 2.521881    1.3438733   3.4558973   0.1437563   0.39993647 94.48239\n",
      "  1.5468246 ]\n",
      "index: 111  actual y: 3  answer y: 3  prediction: [ 2.10504    1.3577008  3.8298917 93.12773    4.498849   1.448879\n",
      "  1.5427115]\n",
      "index: 112  actual y: 1  answer y: 1  prediction: [ 1.422728  93.72644    2.1616051  1.5846076  2.3148906  3.2269764\n",
      "  1.8742098]\n",
      "index: 113  actual y: 5  answer y: 5  prediction: [ 1.3895334  2.3193288  3.6414366  1.4886177  1.230177  87.81548\n",
      "  1.2413621]\n",
      "index: 114  actual y: 6  answer y: 6  prediction: [ 0.22724916  1.7185535   1.8723499   1.5479096   6.791918    1.0335656\n",
      " 93.14814   ]\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.1041 - accuracy: 0.8609 - precision: 0.9015 - recall: 0.8540 - f1score: 0.8768\n",
      "loss: 0.104, accuracy: 0.861, precision: 0.901, recall: 0.854, f1score: 0.877\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# output = classifier.predict_generator(test_set, steps=1)\n",
    "# print(test_set.class_indices)\n",
    "# print(output)\n",
    "size = y_test[:,-1]\n",
    "print(size.size)\n",
    "\n",
    "\n",
    "# predict 10 random hand-writing data\n",
    "y_predicted = model.predict(x_test)\n",
    "for x in range(0,size.size):\n",
    "    \n",
    "   np.set_printoptions(suppress=True)\n",
    "   print(\"index:\", x,\n",
    "          \" actual y:\", np.argmax(y_test[x]),\n",
    "          \" answer y:\", np.argmax(y_predicted[x]),\n",
    "          \" prediction:\", np.array(y_predicted[x] * 100))\n",
    "\n",
    "_loss, _acc, _precision, _recall, _f1score = model.evaluate(x_test, y_test)\n",
    "print('loss: {:.3f}, accuracy: {:.3f}, precision: {:.3f}, recall: {:.3f}, f1score: {:.3f}'.format(_loss, _acc, _precision, _recall, _f1score))\n",
    "# print('loss: ', evaluation[0])\n",
    "# print('accuracy', evaluation[1])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 손실 그래프\n",
    "def plot_loss(history):\n",
    "   # 선 그리기\n",
    "    plt.plot(history.history['loss'], 'y', label='train loss')\n",
    "    plt.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "   # 그래프 제목\n",
    "    plt.title('Model Loss')\n",
    "   # x,y축 이름 표시\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "   # 각 라인 표식 표시\n",
    "    plt.legend(['Train','Validation'],loc=0)\n",
    "\n",
    "# 정확도 그래프\n",
    "def plot_acc(history):\n",
    "  # dir(history.history)\n",
    "    plt.plot(history.history['accuracy'], 'b', label='train accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], 'g', label='val accuracy')\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc=0)\n",
    "\n",
    "plot_loss(history)\n",
    "plt.show()\n",
    "plot_acc(history)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "animal_model_man",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
