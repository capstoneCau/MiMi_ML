{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-dgO35Q0iPCC",
    "outputId": "08cd65f4-867d-483c-a5fd-f5403e37cda6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batchSIZE is 512, Learning Rate is 0.001\n",
      "Found 1120 images belonging to 8 classes.\n",
      "Found 247 images belonging to 8 classes.\n",
      "Found 211 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os # miscellaneous operating system interfaces\n",
    "import shutil # high-level file operations\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import random\n",
    "from itertools import product\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Conv2D \n",
    "from keras.layers import MaxPooling2D \n",
    "from keras.layers import Flatten \n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.applications import MobileNetV2, Xception, DenseNet121,ResNet152V2,NASNetLarge,InceptionV3,InceptionResNetV2\n",
    "from keras.applications.mobilenet_v2 import preprocess_input\n",
    "from keras.layers import  Input, Conv2D, Conv2DTranspose, ReLU,AveragePooling2D, MaxPooling2D, Dense, BatchNormalization, GlobalAveragePooling2D, Concatenate\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "\n",
    "base_dir = \"/content/drive/My Drive/Colab Notebooks/datasets/man\"\n",
    "\n",
    "train_img_dir = \"/content/drive/My Drive/Colab Notebooks/datasets/man/train\"\n",
    "\n",
    "test_img_dir = \"/content/drive/My Drive/Colab Notebooks/datasets/man/test\"\n",
    "\n",
    "val_img_dir = \"/content/drive/My Drive/Colab Notebooks/datasets/man/val\"\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "training_epochs = 20 # traindata개수/batchsize\n",
    "batch_size = 512\n",
    "validation_steps = 20 # valdata개수/batchsize\n",
    "\n",
    "\n",
    "\n",
    "print(f'batchSIZE is {batch_size}, Learning Rate is {learning_rate}')\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "val_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "categories = ['dog','cat','bear','hamster','horse','wolf','dinosaur','racoon']\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(train_img_dir, target_size=(128,128), \n",
    "                                             classes=categories, \n",
    "                                             batch_size=batch_size)\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(test_img_dir,\n",
    "                                        target_size=(128,128), \n",
    "                                        classes=categories, \n",
    "                                        batch_size=batch_size)\n",
    "\n",
    "val_set = test_datagen.flow_from_directory(val_img_dir,\n",
    "                                        target_size=(128,128), \n",
    "                                        classes=categories, \n",
    "                                        batch_size=batch_size)\n",
    "\n",
    "x_train, y_train = next(training_set)\n",
    "x_test, y_test = next(test_set)\n",
    "x_val,y_val = next(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "RzBS39E1jIg4"
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "    \n",
    "\n",
    "    conv2_1 = Conv2D(32, 3, 1, 'SAME')(inputs)\n",
    "    conv2_2 = Conv2D(32, 3, 1, 'SAME')(conv2_1)\n",
    "    pool2_3 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv2_2)\n",
    "    br1 = BatchNormalization()(pool2_3)\n",
    "    \n",
    "    conv2_1 = Conv2D(32, 3, 1, 'SAME')(br1)\n",
    "    conv2_2 = Conv2D(32, 3, 1, 'SAME')(conv2_1)\n",
    "    pool2_3 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv2_2)\n",
    "    br1 = BatchNormalization()(pool2_3)\n",
    "    \n",
    "    \n",
    "    conv3_1 = Conv2D(64, 3, 1, 'SAME')(br1)\n",
    "    conv3_2 = Conv2D(64, 3, 1, 'SAME')(conv3_1)\n",
    "    pool3_2 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv3_2)\n",
    "    br2 = BatchNormalization()(pool3_2)\n",
    "    \n",
    "    conv3_1 = Conv2D(64, 3, 1, 'SAME')(br2)\n",
    "    conv3_2 = Conv2D(64, 3, 1, 'SAME')(conv3_1)\n",
    "    pool3_2 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv3_2)\n",
    "    br2 = BatchNormalization()(pool3_2)\n",
    "    \n",
    "    \n",
    "    conv4_1 = Conv2D(128, 3, 1, 'SAME')(br2)\n",
    "    conv4_2 = Conv2D(128, 3, 1, 'SAME')(conv4_1)\n",
    "    pool4_2 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv4_2)\n",
    "    br3 = BatchNormalization()(pool4_2)\n",
    "    \n",
    "    conv4_1 = Conv2D(128, 3, 1, 'SAME')(br3)\n",
    "    conv4_2 = Conv2D(128, 3, 1, 'SAME')(conv4_1)\n",
    "    pool4_2 = MaxPooling2D(pool_size=(2,2),padding='SAME')(conv4_2)\n",
    "    br3 = BatchNormalization()(pool4_2)\n",
    "    \n",
    "    flatten1 = Flatten()(pool4_2)\n",
    "    dense1 = Dense(units = 512, activation = 'relu')(flatten1)\n",
    "    dense2 = Dense(units = 1024, activation = 'relu')(dense1)\n",
    "    dr1 = Dropout(0.7)(dense2)\n",
    "    dense3 = Dense(units = 5, activation = 'sigmoid')(dr1)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=dense3) \n",
    "def mobile_net():\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "    mobileNet = MobileNetV2(weights = \"imagenet\", include_top=False,input_shape = (128,128,3)\n",
    "                            ,input_tensor = inputs)\n",
    "    for layer in mobileNet.layers:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    output = mobileNet.output\n",
    "    pooling = AveragePooling2D(pool_size=(16,16),padding='SAME')(output)\n",
    "    flatten1 = Flatten()(pooling)\n",
    "    dense1 = Dense(units = 256)(flatten1)\n",
    "    batch1 = BatchNormalization()(dense1)\n",
    "    relu1 = ReLU()(batch1)\n",
    "    dense2 = Dense(units = 64)(relu1)\n",
    "    batch2 = BatchNormalization()(dense2)\n",
    "    relu2 = ReLU()(batch2)\n",
    "    dense3 = Dense(units = 6, activation = 'sigmoid')(relu2)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=dense3)\n",
    "\n",
    "def xception():\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "    xception = Xception(weights = \"imagenet\", include_top=False,input_shape = (128,128,3)\n",
    "                            ,input_tensor = inputs)\n",
    "    for layer in xception.layers:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    output = xception.output\n",
    "    pooling = AveragePooling2D(pool_size=(8,8),padding='SAME')(output)\n",
    "    flatten1 = Flatten()(pooling)\n",
    "    dense1 = Dense(units = 256)(flatten1)\n",
    "    batch1 = BatchNormalization()(dense1)\n",
    "    relu1 = ReLU()(batch1)\n",
    "    dense2 = Dense(units = 64)(relu1)\n",
    "    batch2 = BatchNormalization()(dense2)\n",
    "    relu2 = ReLU()(batch2)\n",
    "    dense3 = Dense(units = 8, activation = 'sigmoid')(relu2)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=dense3)\n",
    "    # pooling = AveragePooling2D(pool_size=(4,4),padding='SAME')(output)\n",
    "    \n",
    "    # flatten1 = Flatten()(pooling)\n",
    "    # dense1 = Dense(units = 512, activation = 'relu')(flatten1)\n",
    "    # dense2 = Dense(units = 1024, activation = 'relu')(dense1)\n",
    "    # dr1 = Dropout(0.7)(dense2)\n",
    "    # dense3 = Dense(units = 5, activation = 'sigmoid')(dr1)\n",
    "    \n",
    "    # return keras.Model(inputs=inputs, outputs=dense3)\n",
    "\n",
    "def resnet():\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "    resnet = ResNet152V2 (weights = \"imagenet\", include_top=False,input_shape = (128,128,3)\n",
    "                            ,input_tensor = inputs)\n",
    "    for layer in resnet.layers:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    output = resnet.output\n",
    "    pooling = MaxPooling2D(pool_size=(8,8),padding='SAME')(output)\n",
    "    flatten1 = Flatten()(pooling)\n",
    "    dense1 = Dense(units = 128)(flatten1)\n",
    "    batch1 = BatchNormalization()(dense1)\n",
    "    relu1 = ReLU()(batch1)\n",
    "    dense2 = Dense(units = 32)(relu1)\n",
    "    batch2 = BatchNormalization()(dense2)\n",
    "    relu2 = ReLU()(batch2)\n",
    "    dense3 = Dense(units = 8, activation = 'sigmoid')(relu2)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=dense3)\n",
    "\n",
    "def densenet():\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "    densenet = DenseNet121(weights = \"imagenet\", include_top=False,input_shape = (128,128,3)\n",
    "                            ,input_tensor = inputs)\n",
    "    for layer in densenet.layers:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    output = densenet.output\n",
    "    pooling = MaxPooling2D(pool_size=(32,32),padding='SAME')(output)\n",
    "    flatten1 = Flatten()(pooling)\n",
    "    dense1 = Dense(units = 112)(flatten1)\n",
    "    batch1 = BatchNormalization()(dense1)\n",
    "    relu1 = ReLU()(batch1)\n",
    "    dense2 = Dense(units = 28)(relu1)\n",
    "    batch2 = BatchNormalization()(dense2)\n",
    "    relu2 = ReLU()(batch2)\n",
    "    dense3 = Dense(units = 7, activation = 'softmax')(relu2)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=dense3)\n",
    "\n",
    "\n",
    "def inception():\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "    inception = InceptionV3(weights = \"imagenet\", include_top=False,input_shape = (128,128,3)\n",
    "                            ,input_tensor = inputs)\n",
    "    for layer in inception.layers:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    output = inception.output\n",
    "    pooling = MaxPooling2D(pool_size=(32,32),padding='SAME')(output)\n",
    "    flatten1 = Flatten()(pooling)\n",
    "    dense1 = Dense(units = 1024)(flatten1)\n",
    "    batch1 = BatchNormalization()(dense1)\n",
    "    relu1 = ReLU()(batch1)\n",
    "    dense2 = Dense(units = 512)(relu1)\n",
    "    batch2 = BatchNormalization()(dense2)\n",
    "    relu2 = ReLU()(batch2)\n",
    "    dense3 = Dense(units = 8, activation = 'sigmoid')(relu2)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=dense3)\n",
    "\n",
    "def inceptionresnet():\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "    inceptionresnet = InceptionResNetV2(weights = \"imagenet\", include_top=False,input_shape = (128,128,3)\n",
    "                            ,input_tensor = inputs)\n",
    "    for layer in inceptionresnet.layers:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    output = inceptionresnet.output\n",
    "    pooling = MaxPooling2D(pool_size=(16,16),padding='SAME')(output)\n",
    "    flatten1 = Flatten()(pooling)\n",
    "    dense1 = Dense(units = 1024)(flatten1)\n",
    "    batch1 = BatchNormalization()(dense1)\n",
    "    relu1 = ReLU()(batch1)\n",
    "    dense2 = Dense(units = 512)(relu1)\n",
    "    batch2 = BatchNormalization()(dense2)\n",
    "    relu2 = ReLU()(batch2)\n",
    "    dense3 = Dense(units = 8, activation = 'sigmoid')(relu2)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=dense3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ResidualUnit(tf.keras.Model):\n",
    "    def __init__(self, filter_in, filter_out, kernel_size):\n",
    "        super(ResidualUnit, self).__init__()\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filter_out, kernel_size, padding='same')\n",
    "        \n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filter_out, kernel_size, padding='same')\n",
    "        \n",
    "        if filter_in == filter_out:\n",
    "            self.identity = lambda x: x\n",
    "        else:\n",
    "            self.identity = tf.keras.layers.Conv2D(filter_out, (1,1), padding='same')\n",
    "\n",
    "    def call(self, x, training=False, mask=None):\n",
    "        h = self.bn1(x, training=training)\n",
    "        h = tf.nn.relu(h)\n",
    "        h = self.conv1(h)\n",
    "        \n",
    "        h = self.bn2(h, training=training)\n",
    "        h = tf.nn.relu(h)\n",
    "        h = self.conv2(h)\n",
    "        return self.identity(x) + h\n",
    "    \n",
    "class ResnetLayer(tf.keras.Model):\n",
    "    def __init__(self, filter_in, filters, kernel_size):\n",
    "        super(ResnetLayer, self).__init__()\n",
    "        self.sequence = list()\n",
    "        for f_in, f_out in zip([filter_in] + list(filters), filters):\n",
    "            self.sequence.append(ResidualUnit(f_in, f_out, kernel_size))\n",
    "\n",
    "    def call(self, x, training=False, mask=None):\n",
    "        for unit in self.sequence:\n",
    "            x = unit(x, training=training)\n",
    "        return x\n",
    "    \n",
    "class ResNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu') # 28x28x8\n",
    "        \n",
    "        self.res1 = ResnetLayer(64, (16, 16), (3, 3)) # 28x28x16\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D((2, 2)) # 14x14x16\n",
    "        \n",
    "        \n",
    "        self.res2 = ResnetLayer(128, (32, 32), (3, 3)) # 14x14x32\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D((2, 2)) # 7x7x32\n",
    "    \n",
    "        \n",
    "        self.res3 = ResnetLayer(256, (64, 64), (3, 3)) # 7x7x64\n",
    "        self.pool = tf.keras.layers.MaxPool2D((2, 2)) # 7x7x32\n",
    "        \n",
    "        \n",
    "        self.res4 = ResnetLayer(512, (64, 64), (3, 3)) # 7x7x64\n",
    "        self.pool = tf.keras.layers.MaxPool2D((2, 2)) # 7x7x32\n",
    "        \n",
    "        \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(1024, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(5, activation='softmax')\n",
    "        \n",
    "    def call(self, x, training=False, mask=None):\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        x = self.res1(x, training=training)\n",
    "        x = self.pool1(x)\n",
    "        x = self.res2(x, training=training)\n",
    "        x = self.pool2(x)\n",
    "        x = self.res3(x, training=training)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        return self.dense2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2F8dolSzlByA",
    "outputId": "4bbdb79a-f92e-4d32-8b7e-ac43a89550bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "20/20 [==============================] - 7s 352ms/step - loss: 0.4818 - accuracy: 0.3301 - precision: 0.4071 - recall: 0.3744 - f1score: 0.3321 - val_loss: 17.3587 - val_accuracy: 0.1327 - val_precision: 0.1144 - val_recall: 0.3432 - val_f1score: 0.1716\n",
      "Epoch 2/120\n",
      "20/20 [==============================] - 4s 203ms/step - loss: 0.1948 - accuracy: 0.6953 - precision: 0.7988 - recall: 0.5729 - f1score: 0.6605 - val_loss: 1.3729 - val_accuracy: 0.1185 - val_precision: 0.0977 - val_recall: 0.1955 - val_f1score: 0.1303\n",
      "Epoch 3/120\n",
      "20/20 [==============================] - 4s 205ms/step - loss: 0.1040 - accuracy: 0.8672 - precision: 0.8941 - recall: 0.8254 - f1score: 0.8576 - val_loss: 0.7918 - val_accuracy: 0.2512 - val_precision: 0.2900 - val_recall: 0.0955 - val_f1score: 0.1383\n",
      "Epoch 4/120\n",
      "20/20 [==============================] - 4s 205ms/step - loss: 0.0641 - accuracy: 0.9355 - precision: 0.9464 - recall: 0.9156 - f1score: 0.9300 - val_loss: 0.6727 - val_accuracy: 0.2607 - val_precision: 0.2931 - val_recall: 0.1318 - val_f1score: 0.1749\n",
      "Epoch 5/120\n",
      "20/20 [==============================] - 4s 206ms/step - loss: 0.0514 - accuracy: 0.9277 - precision: 0.9442 - recall: 0.9280 - f1score: 0.9358 - val_loss: 0.4371 - val_accuracy: 0.2133 - val_precision: 0.2101 - val_recall: 0.1727 - val_f1score: 0.1882\n",
      "Epoch 6/120\n",
      "20/20 [==============================] - 4s 204ms/step - loss: 0.0467 - accuracy: 0.9375 - precision: 0.9564 - recall: 0.9263 - f1score: 0.9404 - val_loss: 0.3022 - val_accuracy: 0.4597 - val_precision: 0.5390 - val_recall: 0.4545 - val_f1score: 0.4894\n",
      "Epoch 7/120\n",
      "20/20 [==============================] - 4s 204ms/step - loss: 0.0584 - accuracy: 0.9336 - precision: 0.9543 - recall: 0.9118 - f1score: 0.9320 - val_loss: 0.4054 - val_accuracy: 0.4265 - val_precision: 0.7098 - val_recall: 0.2977 - val_f1score: 0.4142\n",
      "Epoch 8/120\n",
      "20/20 [==============================] - 4s 203ms/step - loss: 0.0577 - accuracy: 0.9219 - precision: 0.9427 - recall: 0.9041 - f1score: 0.9222 - val_loss: 33.5959 - val_accuracy: 0.1896 - val_precision: 0.1760 - val_recall: 0.2318 - val_f1score: 0.1980\n",
      "Epoch 9/120\n",
      "20/20 [==============================] - 4s 202ms/step - loss: 0.0451 - accuracy: 0.9355 - precision: 0.9536 - recall: 0.9340 - f1score: 0.9434 - val_loss: 1.0244 - val_accuracy: 0.3839 - val_precision: 0.4216 - val_recall: 0.3136 - val_f1score: 0.3559\n",
      "Epoch 10/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 0.0298 - accuracy: 0.9668 - precision: 0.9607 - recall: 0.9588 - f1score: 0.9593 - val_loss: 0.4555 - val_accuracy: 0.4360 - val_precision: 0.5603 - val_recall: 0.3841 - val_f1score: 0.4514\n",
      "Epoch 11/120\n",
      "20/20 [==============================] - 4s 200ms/step - loss: 0.0219 - accuracy: 0.9727 - precision: 0.9733 - recall: 0.9750 - f1score: 0.9741 - val_loss: 0.3136 - val_accuracy: 0.6256 - val_precision: 0.6612 - val_recall: 0.5909 - val_f1score: 0.6224\n",
      "Epoch 12/120\n",
      "20/20 [==============================] - 4s 199ms/step - loss: 0.0106 - accuracy: 0.9922 - precision: 0.9888 - recall: 0.9942 - f1score: 0.9914 - val_loss: 0.2583 - val_accuracy: 0.6540 - val_precision: 0.6796 - val_recall: 0.6409 - val_f1score: 0.6562\n",
      "Epoch 13/120\n",
      "20/20 [==============================] - 4s 199ms/step - loss: 0.0076 - accuracy: 0.9941 - precision: 0.9942 - recall: 0.9942 - f1score: 0.9942 - val_loss: 0.2126 - val_accuracy: 0.7251 - val_precision: 0.7453 - val_recall: 0.6705 - val_f1score: 0.7037\n",
      "Epoch 14/120\n",
      "20/20 [==============================] - 4s 199ms/step - loss: 0.0067 - accuracy: 0.9941 - precision: 0.9942 - recall: 0.9923 - f1score: 0.9933 - val_loss: 0.1799 - val_accuracy: 0.7630 - val_precision: 0.7924 - val_recall: 0.7114 - val_f1score: 0.7468\n",
      "Epoch 15/120\n",
      "20/20 [==============================] - 4s 200ms/step - loss: 0.0039 - accuracy: 0.9980 - precision: 0.9981 - recall: 0.9981 - f1score: 0.9981 - val_loss: 0.1653 - val_accuracy: 0.7725 - val_precision: 0.8190 - val_recall: 0.7545 - val_f1score: 0.7829\n",
      "Epoch 16/120\n",
      "20/20 [==============================] - 4s 199ms/step - loss: 0.0042 - accuracy: 1.0000 - precision: 1.0000 - recall: 0.9981 - f1score: 0.9990 - val_loss: 0.1517 - val_accuracy: 0.7867 - val_precision: 0.8343 - val_recall: 0.7341 - val_f1score: 0.7766\n",
      "Epoch 17/120\n",
      "20/20 [==============================] - 4s 200ms/step - loss: 0.0027 - accuracy: 1.0000 - precision: 0.9981 - recall: 1.0000 - f1score: 0.9991 - val_loss: 0.1974 - val_accuracy: 0.7393 - val_precision: 0.8075 - val_recall: 0.6886 - val_f1score: 0.7390\n",
      "Epoch 18/120\n",
      "20/20 [==============================] - 4s 200ms/step - loss: 0.0021 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1729 - val_accuracy: 0.7773 - val_precision: 0.8280 - val_recall: 0.7386 - val_f1score: 0.7788\n",
      "Epoch 19/120\n",
      "20/20 [==============================] - 4s 200ms/step - loss: 0.0016 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1619 - val_accuracy: 0.7867 - val_precision: 0.8229 - val_recall: 0.7477 - val_f1score: 0.7814\n",
      "Epoch 20/120\n",
      "20/20 [==============================] - 4s 200ms/step - loss: 0.0014 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1503 - val_accuracy: 0.7962 - val_precision: 0.8178 - val_recall: 0.7341 - val_f1score: 0.7708\n",
      "Epoch 21/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 0.0023 - accuracy: 1.0000 - precision: 0.9981 - recall: 1.0000 - f1score: 0.9991 - val_loss: 0.1412 - val_accuracy: 0.8057 - val_precision: 0.8439 - val_recall: 0.7477 - val_f1score: 0.7910\n",
      "Epoch 22/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 0.0021 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1412 - val_accuracy: 0.8104 - val_precision: 0.8401 - val_recall: 0.7477 - val_f1score: 0.7888\n",
      "Epoch 23/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 0.0011 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1421 - val_accuracy: 0.8009 - val_precision: 0.8375 - val_recall: 0.7386 - val_f1score: 0.7822\n",
      "Epoch 24/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 0.0011 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1427 - val_accuracy: 0.8009 - val_precision: 0.8307 - val_recall: 0.7341 - val_f1score: 0.7763\n",
      "Epoch 25/120\n",
      "20/20 [==============================] - 4s 202ms/step - loss: 0.0012 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1416 - val_accuracy: 0.7915 - val_precision: 0.8242 - val_recall: 0.7341 - val_f1score: 0.7730\n",
      "Epoch 26/120\n",
      "20/20 [==============================] - 4s 202ms/step - loss: 0.0013 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1436 - val_accuracy: 0.7915 - val_precision: 0.8150 - val_recall: 0.7295 - val_f1score: 0.7661\n",
      "Epoch 27/120\n",
      "20/20 [==============================] - 4s 203ms/step - loss: 0.0012 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1425 - val_accuracy: 0.7962 - val_precision: 0.8228 - val_recall: 0.7341 - val_f1score: 0.7726\n",
      "Epoch 28/120\n",
      "20/20 [==============================] - 4s 203ms/step - loss: 0.0013 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1421 - val_accuracy: 0.7962 - val_precision: 0.8297 - val_recall: 0.7386 - val_f1score: 0.7787\n",
      "Epoch 29/120\n",
      "20/20 [==============================] - 4s 202ms/step - loss: 9.1274e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1432 - val_accuracy: 0.7962 - val_precision: 0.8345 - val_recall: 0.7386 - val_f1score: 0.7806\n",
      "Epoch 30/120\n",
      "20/20 [==============================] - 4s 202ms/step - loss: 9.0963e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1426 - val_accuracy: 0.7962 - val_precision: 0.8344 - val_recall: 0.7341 - val_f1score: 0.7777\n",
      "Epoch 31/120\n",
      "20/20 [==============================] - 4s 202ms/step - loss: 8.1742e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1426 - val_accuracy: 0.7962 - val_precision: 0.8394 - val_recall: 0.7341 - val_f1score: 0.7799\n",
      "Epoch 32/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 0.0010 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1426 - val_accuracy: 0.7962 - val_precision: 0.8432 - val_recall: 0.7341 - val_f1score: 0.7818\n",
      "Epoch 33/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 9.3584e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1418 - val_accuracy: 0.8057 - val_precision: 0.8394 - val_recall: 0.7386 - val_f1score: 0.7825\n",
      "Epoch 34/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 9.2600e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1428 - val_accuracy: 0.8057 - val_precision: 0.8370 - val_recall: 0.7386 - val_f1score: 0.7818\n",
      "Epoch 35/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 8.3654e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1422 - val_accuracy: 0.8057 - val_precision: 0.8415 - val_recall: 0.7386 - val_f1score: 0.7837\n",
      "Epoch 36/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 8.7461e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1420 - val_accuracy: 0.8104 - val_precision: 0.8366 - val_recall: 0.7386 - val_f1score: 0.7819\n",
      "Epoch 37/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 0.0011 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1412 - val_accuracy: 0.8104 - val_precision: 0.8408 - val_recall: 0.7386 - val_f1score: 0.7835\n",
      "Epoch 38/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 8.6012e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1413 - val_accuracy: 0.8104 - val_precision: 0.8415 - val_recall: 0.7386 - val_f1score: 0.7837\n",
      "Epoch 39/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 0.0011 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1419 - val_accuracy: 0.7962 - val_precision: 0.8374 - val_recall: 0.7432 - val_f1score: 0.7844\n",
      "Epoch 40/120\n",
      "20/20 [==============================] - 4s 200ms/step - loss: 0.0011 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1442 - val_accuracy: 0.7820 - val_precision: 0.8385 - val_recall: 0.7432 - val_f1score: 0.7857\n",
      "Epoch 41/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 0.0012 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1436 - val_accuracy: 0.7820 - val_precision: 0.8392 - val_recall: 0.7477 - val_f1score: 0.7882\n",
      "Epoch 42/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 7.4605e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1426 - val_accuracy: 0.7915 - val_precision: 0.8392 - val_recall: 0.7432 - val_f1score: 0.7859\n",
      "Epoch 43/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 9.6736e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1429 - val_accuracy: 0.7962 - val_precision: 0.8392 - val_recall: 0.7432 - val_f1score: 0.7859\n",
      "Epoch 44/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 9.3114e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1430 - val_accuracy: 0.7915 - val_precision: 0.8392 - val_recall: 0.7432 - val_f1score: 0.7859\n",
      "Epoch 45/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 8.7012e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1431 - val_accuracy: 0.8009 - val_precision: 0.8392 - val_recall: 0.7432 - val_f1score: 0.7859\n",
      "Epoch 46/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 8.9172e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1425 - val_accuracy: 0.7915 - val_precision: 0.8392 - val_recall: 0.7432 - val_f1score: 0.7859\n",
      "Epoch 47/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 9.9785e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1423 - val_accuracy: 0.7915 - val_precision: 0.8411 - val_recall: 0.7386 - val_f1score: 0.7838\n",
      "Epoch 48/120\n",
      "20/20 [==============================] - 4s 202ms/step - loss: 0.0016 - accuracy: 0.9980 - precision: 0.9981 - recall: 1.0000 - f1score: 0.9991 - val_loss: 0.1432 - val_accuracy: 0.7962 - val_precision: 0.8289 - val_recall: 0.7386 - val_f1score: 0.7782\n",
      "Epoch 49/120\n",
      "20/20 [==============================] - 4s 203ms/step - loss: 8.5656e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1434 - val_accuracy: 0.7962 - val_precision: 0.8341 - val_recall: 0.7386 - val_f1score: 0.7806\n",
      "Epoch 50/120\n",
      "20/20 [==============================] - 4s 203ms/step - loss: 6.1454e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1431 - val_accuracy: 0.7962 - val_precision: 0.8303 - val_recall: 0.7386 - val_f1score: 0.7787\n",
      "Epoch 51/120\n",
      "20/20 [==============================] - 4s 202ms/step - loss: 8.4743e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1426 - val_accuracy: 0.8009 - val_precision: 0.8303 - val_recall: 0.7386 - val_f1score: 0.7787\n",
      "Epoch 52/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 6.8929e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1425 - val_accuracy: 0.8009 - val_precision: 0.8303 - val_recall: 0.7386 - val_f1score: 0.7787\n",
      "Epoch 53/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 6.7782e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1423 - val_accuracy: 0.8009 - val_precision: 0.8339 - val_recall: 0.7386 - val_f1score: 0.7804\n",
      "Epoch 54/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 9.4940e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1425 - val_accuracy: 0.8009 - val_precision: 0.8339 - val_recall: 0.7386 - val_f1score: 0.7804\n",
      "Epoch 55/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 8.2529e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1430 - val_accuracy: 0.8009 - val_precision: 0.8339 - val_recall: 0.7386 - val_f1score: 0.7804\n",
      "Epoch 56/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 7.2354e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1431 - val_accuracy: 0.8009 - val_precision: 0.8339 - val_recall: 0.7386 - val_f1score: 0.7804\n",
      "Epoch 57/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 7.3327e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1428 - val_accuracy: 0.8009 - val_precision: 0.8339 - val_recall: 0.7386 - val_f1score: 0.7804\n",
      "Epoch 58/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 0.0011 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1426 - val_accuracy: 0.7962 - val_precision: 0.8377 - val_recall: 0.7386 - val_f1score: 0.7824\n",
      "Epoch 59/120\n",
      "20/20 [==============================] - 4s 200ms/step - loss: 5.3013e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1430 - val_accuracy: 0.7962 - val_precision: 0.8422 - val_recall: 0.7386 - val_f1score: 0.7843\n",
      "Epoch 60/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 7.9840e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1428 - val_accuracy: 0.7962 - val_precision: 0.8422 - val_recall: 0.7386 - val_f1score: 0.7843\n",
      "Epoch 61/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 7.3054e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1428 - val_accuracy: 0.7962 - val_precision: 0.8422 - val_recall: 0.7386 - val_f1score: 0.7843\n",
      "Epoch 62/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 7.1833e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1428 - val_accuracy: 0.7962 - val_precision: 0.8422 - val_recall: 0.7386 - val_f1score: 0.7843\n",
      "Epoch 63/120\n",
      "20/20 [==============================] - 4s 200ms/step - loss: 0.0011 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1428 - val_accuracy: 0.7915 - val_precision: 0.8422 - val_recall: 0.7386 - val_f1score: 0.7843\n",
      "Epoch 64/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 7.4723e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1429 - val_accuracy: 0.7962 - val_precision: 0.8422 - val_recall: 0.7386 - val_f1score: 0.7843\n",
      "Epoch 65/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 6.2513e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1431 - val_accuracy: 0.7962 - val_precision: 0.8422 - val_recall: 0.7386 - val_f1score: 0.7843\n",
      "Epoch 66/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 7.5992e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1432 - val_accuracy: 0.7962 - val_precision: 0.8422 - val_recall: 0.7386 - val_f1score: 0.7843\n",
      "Epoch 67/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 8.2136e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1433 - val_accuracy: 0.7915 - val_precision: 0.8422 - val_recall: 0.7386 - val_f1score: 0.7843\n",
      "Epoch 68/120\n",
      "20/20 [==============================] - 4s 202ms/step - loss: 6.8454e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1433 - val_accuracy: 0.7962 - val_precision: 0.8422 - val_recall: 0.7386 - val_f1score: 0.7843\n",
      "Epoch 69/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 8.6500e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1433 - val_accuracy: 0.7962 - val_precision: 0.8422 - val_recall: 0.7386 - val_f1score: 0.7843\n",
      "Epoch 70/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 7.3584e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1433 - val_accuracy: 0.7915 - val_precision: 0.8422 - val_recall: 0.7386 - val_f1score: 0.7843\n",
      "Epoch 71/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 7.1035e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1433 - val_accuracy: 0.7962 - val_precision: 0.8422 - val_recall: 0.7386 - val_f1score: 0.7843\n",
      "Epoch 72/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 0.0014 - accuracy: 0.9980 - precision: 0.9981 - recall: 1.0000 - f1score: 0.9991 - val_loss: 0.1433 - val_accuracy: 0.7962 - val_precision: 0.8377 - val_recall: 0.7386 - val_f1score: 0.7824\n",
      "Epoch 73/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 7.6934e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1430 - val_accuracy: 0.7962 - val_precision: 0.8377 - val_recall: 0.7386 - val_f1score: 0.7824\n",
      "Epoch 74/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 7.0006e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1430 - val_accuracy: 0.7962 - val_precision: 0.8377 - val_recall: 0.7386 - val_f1score: 0.7824\n",
      "Epoch 75/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 8.9977e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1430 - val_accuracy: 0.7915 - val_precision: 0.8377 - val_recall: 0.7432 - val_f1score: 0.7850\n",
      "Epoch 76/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 7.5231e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1427 - val_accuracy: 0.7915 - val_precision: 0.8372 - val_recall: 0.7432 - val_f1score: 0.7847\n",
      "Epoch 77/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 0.0011 - accuracy: 0.9980 - precision: 0.9981 - recall: 0.9981 - f1score: 0.9981 - val_loss: 0.1420 - val_accuracy: 0.7962 - val_precision: 0.8372 - val_recall: 0.7386 - val_f1score: 0.7821\n",
      "Epoch 78/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 5.2109e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1419 - val_accuracy: 0.7962 - val_precision: 0.8372 - val_recall: 0.7386 - val_f1score: 0.7821\n",
      "Epoch 79/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 8.6386e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1421 - val_accuracy: 0.7962 - val_precision: 0.8372 - val_recall: 0.7432 - val_f1score: 0.7847\n",
      "Epoch 80/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 8.6241e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1420 - val_accuracy: 0.8009 - val_precision: 0.8372 - val_recall: 0.7432 - val_f1score: 0.7847\n",
      "Epoch 81/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 7.0934e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1420 - val_accuracy: 0.7962 - val_precision: 0.8372 - val_recall: 0.7432 - val_f1score: 0.7847\n",
      "Epoch 82/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 9.1011e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1422 - val_accuracy: 0.7962 - val_precision: 0.8372 - val_recall: 0.7432 - val_f1score: 0.7847\n",
      "Epoch 83/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 0.0011 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1422 - val_accuracy: 0.8009 - val_precision: 0.8372 - val_recall: 0.7432 - val_f1score: 0.7847\n",
      "Epoch 84/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 6.5657e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1422 - val_accuracy: 0.7962 - val_precision: 0.8372 - val_recall: 0.7432 - val_f1score: 0.7847\n",
      "Epoch 85/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 0.0013 - accuracy: 0.9980 - precision: 0.9981 - recall: 0.9981 - f1score: 0.9981 - val_loss: 0.1425 - val_accuracy: 0.8009 - val_precision: 0.8372 - val_recall: 0.7432 - val_f1score: 0.7847\n",
      "Epoch 86/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 8.7243e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1428 - val_accuracy: 0.8009 - val_precision: 0.8372 - val_recall: 0.7432 - val_f1score: 0.7847\n",
      "Epoch 87/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 8.1388e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1428 - val_accuracy: 0.8009 - val_precision: 0.8372 - val_recall: 0.7432 - val_f1score: 0.7847\n",
      "Epoch 88/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 6.2232e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1430 - val_accuracy: 0.8009 - val_precision: 0.8327 - val_recall: 0.7386 - val_f1score: 0.7802\n",
      "Epoch 89/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 8.9659e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1430 - val_accuracy: 0.8057 - val_precision: 0.8327 - val_recall: 0.7432 - val_f1score: 0.7828\n",
      "Epoch 90/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 7.8166e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1432 - val_accuracy: 0.8057 - val_precision: 0.8327 - val_recall: 0.7432 - val_f1score: 0.7828\n",
      "Epoch 91/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 7.5180e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1430 - val_accuracy: 0.8009 - val_precision: 0.8327 - val_recall: 0.7432 - val_f1score: 0.7828\n",
      "Epoch 92/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 7.4324e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1427 - val_accuracy: 0.8009 - val_precision: 0.8372 - val_recall: 0.7432 - val_f1score: 0.7847\n",
      "Epoch 93/120\n",
      "20/20 [==============================] - 4s 202ms/step - loss: 6.4834e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1427 - val_accuracy: 0.8009 - val_precision: 0.8327 - val_recall: 0.7386 - val_f1score: 0.7802\n",
      "Epoch 94/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 8.1744e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1429 - val_accuracy: 0.8009 - val_precision: 0.8294 - val_recall: 0.7386 - val_f1score: 0.7788\n",
      "Epoch 95/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 7.4570e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1428 - val_accuracy: 0.8009 - val_precision: 0.8372 - val_recall: 0.7386 - val_f1score: 0.7821\n",
      "Epoch 96/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 9.8334e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1426 - val_accuracy: 0.8009 - val_precision: 0.8372 - val_recall: 0.7386 - val_f1score: 0.7821\n",
      "Epoch 97/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 6.9405e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1426 - val_accuracy: 0.8009 - val_precision: 0.8372 - val_recall: 0.7432 - val_f1score: 0.7847\n",
      "Epoch 98/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 6.8887e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1426 - val_accuracy: 0.8009 - val_precision: 0.8294 - val_recall: 0.7432 - val_f1score: 0.7814\n",
      "Epoch 99/120\n",
      "20/20 [==============================] - 4s 202ms/step - loss: 6.4011e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1428 - val_accuracy: 0.8009 - val_precision: 0.8327 - val_recall: 0.7432 - val_f1score: 0.7828\n",
      "Epoch 100/120\n",
      "20/20 [==============================] - 4s 202ms/step - loss: 7.1043e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1427 - val_accuracy: 0.8009 - val_precision: 0.8377 - val_recall: 0.7432 - val_f1score: 0.7850\n",
      "Epoch 101/120\n",
      "20/20 [==============================] - 4s 202ms/step - loss: 7.3031e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1425 - val_accuracy: 0.8009 - val_precision: 0.8327 - val_recall: 0.7386 - val_f1score: 0.7802\n",
      "Epoch 102/120\n",
      "20/20 [==============================] - 4s 203ms/step - loss: 7.5874e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1425 - val_accuracy: 0.8009 - val_precision: 0.8327 - val_recall: 0.7386 - val_f1score: 0.7802\n",
      "Epoch 103/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 7.2028e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1427 - val_accuracy: 0.8009 - val_precision: 0.8327 - val_recall: 0.7386 - val_f1score: 0.7802\n",
      "Epoch 104/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 6.4410e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1428 - val_accuracy: 0.8009 - val_precision: 0.8327 - val_recall: 0.7386 - val_f1score: 0.7802\n",
      "Epoch 105/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 7.6737e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1429 - val_accuracy: 0.8009 - val_precision: 0.8327 - val_recall: 0.7386 - val_f1score: 0.7802\n",
      "Epoch 106/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 7.7166e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1430 - val_accuracy: 0.8009 - val_precision: 0.8327 - val_recall: 0.7386 - val_f1score: 0.7802\n",
      "Epoch 107/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 8.2435e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1432 - val_accuracy: 0.8009 - val_precision: 0.8327 - val_recall: 0.7386 - val_f1score: 0.7802\n",
      "Epoch 108/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 6.6848e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1433 - val_accuracy: 0.8009 - val_precision: 0.8294 - val_recall: 0.7386 - val_f1score: 0.7788\n",
      "Epoch 109/120\n",
      "20/20 [==============================] - 4s 202ms/step - loss: 6.7308e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1430 - val_accuracy: 0.8009 - val_precision: 0.8294 - val_recall: 0.7386 - val_f1score: 0.7788\n",
      "Epoch 110/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 5.9007e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1428 - val_accuracy: 0.8009 - val_precision: 0.8294 - val_recall: 0.7386 - val_f1score: 0.7788\n",
      "Epoch 111/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 6.6187e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1429 - val_accuracy: 0.8009 - val_precision: 0.8327 - val_recall: 0.7386 - val_f1score: 0.7802\n",
      "Epoch 112/120\n",
      "20/20 [==============================] - 4s 202ms/step - loss: 8.1761e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1431 - val_accuracy: 0.8009 - val_precision: 0.8327 - val_recall: 0.7386 - val_f1score: 0.7802\n",
      "Epoch 113/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 5.8220e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1429 - val_accuracy: 0.8009 - val_precision: 0.8327 - val_recall: 0.7386 - val_f1score: 0.7802\n",
      "Epoch 114/120\n",
      "20/20 [==============================] - 4s 202ms/step - loss: 8.1298e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1429 - val_accuracy: 0.8009 - val_precision: 0.8327 - val_recall: 0.7386 - val_f1score: 0.7802\n",
      "Epoch 115/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 8.5972e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1427 - val_accuracy: 0.8009 - val_precision: 0.8327 - val_recall: 0.7386 - val_f1score: 0.7802\n",
      "Epoch 116/120\n",
      "20/20 [==============================] - 4s 200ms/step - loss: 8.8137e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1430 - val_accuracy: 0.8009 - val_precision: 0.8327 - val_recall: 0.7386 - val_f1score: 0.7802\n",
      "Epoch 117/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 6.7092e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1428 - val_accuracy: 0.8009 - val_precision: 0.8327 - val_recall: 0.7386 - val_f1score: 0.7802\n",
      "Epoch 118/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 6.9425e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1428 - val_accuracy: 0.8009 - val_precision: 0.8327 - val_recall: 0.7386 - val_f1score: 0.7802\n",
      "Epoch 119/120\n",
      "20/20 [==============================] - 4s 202ms/step - loss: 7.7210e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1430 - val_accuracy: 0.8009 - val_precision: 0.8327 - val_recall: 0.7386 - val_f1score: 0.7802\n",
      "Epoch 120/120\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 7.0073e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1score: 1.0000 - val_loss: 0.1430 - val_accuracy: 0.8009 - val_precision: 0.8294 - val_recall: 0.7386 - val_f1score: 0.7788\n"
     ]
    }
   ],
   "source": [
    "def recall(y_target, y_pred):\n",
    "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
    "    # round : 반올림한다\n",
    "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "\n",
    "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
    "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
    "\n",
    "    # (True Positive + False Negative) = 실제 값이 1(Positive) 전체\n",
    "    count_true_positive_false_negative = K.sum(y_target_yn)\n",
    "\n",
    "    # Recall =  (True Positive) / (True Positive + False Negative)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    recall = count_true_positive / (count_true_positive_false_negative + K.epsilon())\n",
    "\n",
    "    # return a single tensor value\n",
    "    return recall\n",
    "\n",
    "\n",
    "def precision(y_target, y_pred):\n",
    "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
    "    # round : 반올림한다\n",
    "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "\n",
    "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
    "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
    "\n",
    "    # (True Positive + False Positive) = 예측 값이 1(Positive) 전체\n",
    "    count_true_positive_false_positive = K.sum(y_pred_yn)\n",
    "\n",
    "    # Precision = (True Positive) / (True Positive + False Positive)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    precision = count_true_positive / (count_true_positive_false_positive + K.epsilon())\n",
    "\n",
    "    # return a single tensor value\n",
    "    return precision\n",
    "\n",
    "\n",
    "def f1score(y_target, y_pred):\n",
    "    _recall = recall(y_target, y_pred)\n",
    "    _precision = precision(y_target, y_pred)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    _f1score = ( 2 * _recall * _precision) / (_recall + _precision+ K.epsilon())\n",
    "    \n",
    "    # return a single tensor value\n",
    "    return _f1score\n",
    "\n",
    "\n",
    "\n",
    "#model = create_model()\n",
    "#model = ResNet()\n",
    "#model = mobile_net()\n",
    "#model = xception()\n",
    "#model = densenet()\n",
    "#model = resnet()\n",
    "#model = inception()\n",
    "model =inceptionresnet()\n",
    "\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=learning_rate,\n",
    "                                                          decay_steps=training_epochs * 10,\n",
    "                                                          decay_rate=0.5,\n",
    "                                                          staircase=True)\n",
    "\n",
    "\n",
    "\n",
    "# lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "#     initial_learning_rate=learning_rate,\n",
    "#     decay_steps=100000,\n",
    "#     decay_rate=0.96)\n",
    "\n",
    "# optimizer는 Adam, loss는 sparse categorical crossentropy 사용\n",
    "# label이 ont-hot으로 encoding 안 된 경우에 sparse categorical corssentropy 및 sparse categorical accuracy 사용\n",
    "model.compile(keras.optimizers.Adam(lr_schedule), loss = 'binary_crossentropy', metrics=['accuracy', precision, recall, f1score])\n",
    "\n",
    "# model.compile(\n",
    "#     optimizer='adam',\n",
    "#     loss='binary_crossentropy', \n",
    "#     metrics=['accuracy', precision, recall, f1score],\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Train!\n",
    "history = model.fit(x_train, y_train, steps_per_epoch=training_epochs,\n",
    "         epochs=120, validation_data = (x_val,y_val),validation_steps=validation_steps)\n",
    "model.save('animal_model_man.h5')\n",
    "# epochs = 30\n",
    "# history = model.fit(\n",
    "#     training_set, \n",
    "#     epochs=epochs,\n",
    "#     steps_per_epoch=training_set.samples / epochs, \n",
    "#     validation_data=val_set,\n",
    "#     validation_steps=val_set.samples / epochs,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hnA6J84ElBkJ",
    "outputId": "09ff03a2-e446-4586-e243-c0bf293dd8b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247\n",
      "index: 0  actual y: 6  answer y: 6  prediction: [ 0.04964209  0.02988461  0.04372352  0.05028407  0.01361464  0.04509573\n",
      " 99.881676    0.2363162 ]\n",
      "index: 1  actual y: 5  answer y: 3  prediction: [ 0.10734244  0.12737079  0.0768847  66.95565     0.25239044  7.136361\n",
      "  9.226854    0.06078059]\n",
      "index: 2  actual y: 3  answer y: 3  prediction: [ 0.04875905  0.0528871   0.01582567 99.91568     0.03541859  0.01896344\n",
      "  0.00608019  0.10451549]\n",
      "index: 3  actual y: 6  answer y: 6  prediction: [ 0.44959593  0.053372    0.04450177  0.1742409   0.02715758  0.16485396\n",
      " 99.68181     0.05690145]\n",
      "index: 4  actual y: 4  answer y: 4  prediction: [ 0.01047434  0.11930259  0.06597275  0.05002017 99.90375     0.02143622\n",
      "  0.00833834  0.02310859]\n",
      "index: 5  actual y: 0  answer y: 0  prediction: [98.48652     0.11230215  0.05276363  0.14501576  1.0908067   0.31781346\n",
      "  0.08585168  0.17952764]\n",
      "index: 6  actual y: 2  answer y: 2  prediction: [ 0.03127529  0.02492953 99.90824     0.01012786  0.01497389  0.02891093\n",
      "  0.01834607  0.33664304]\n",
      "index: 7  actual y: 1  answer y: 1  prediction: [ 0.01351523 99.93364     0.08656023  0.00298394  0.0096118   0.11048454\n",
      "  0.01077082  0.01791157]\n",
      "index: 8  actual y: 7  answer y: 1  prediction: [ 0.24292204 94.62335     0.11522555  0.11330961  0.0959752   0.01796867\n",
      "  0.12300132  4.7081327 ]\n",
      "index: 9  actual y: 5  answer y: 5  prediction: [ 0.02274321  0.40211678  0.01888484  0.9438518   0.01501962 99.66582\n",
      "  0.02190816  0.0094988 ]\n",
      "index: 10  actual y: 5  answer y: 4  prediction: [ 0.01158422  0.2700737   0.06689382  0.73571324 97.845215    0.15569526\n",
      "  0.02418772  0.4804438 ]\n",
      "index: 11  actual y: 2  answer y: 2  prediction: [ 0.02245972  0.03991083 99.970894    0.01301993  0.06002411  0.01628415\n",
      "  0.01837474  0.10318614]\n",
      "index: 12  actual y: 5  answer y: 5  prediction: [ 0.01791721  0.03395228  0.79670286  0.08987834  0.06430366 99.56139\n",
      "  0.10486102  0.03094114]\n",
      "index: 13  actual y: 1  answer y: 1  prediction: [ 0.02502643 99.358215    0.6995844   0.38071173  0.05965923  0.52729636\n",
      "  0.04135182  0.03876309]\n",
      "index: 14  actual y: 0  answer y: 0  prediction: [99.42162     0.07164475  0.06147941  0.5747932   0.03184592  0.0412912\n",
      "  0.02439373  0.06146942]\n",
      "index: 15  actual y: 1  answer y: 1  prediction: [ 0.05054679 99.931564    0.01800234  0.04938992  0.0468065   0.04845737\n",
      "  0.00347327  0.01961501]\n",
      "index: 16  actual y: 6  answer y: 6  prediction: [ 0.02052768  0.10955705  0.01802965  0.39657155  0.04611595  0.01222435\n",
      " 99.81443     0.02397033]\n",
      "index: 17  actual y: 4  answer y: 4  prediction: [ 0.20497553  0.0566085   0.07126959  0.15747517 99.53852     0.13996536\n",
      "  0.02391673  0.09018277]\n",
      "index: 18  actual y: 7  answer y: 7  prediction: [ 0.00662095  0.0324715   0.04931413  0.00937039  0.15774544  0.04002404\n",
      "  0.03457471 99.967995  ]\n",
      "index: 19  actual y: 4  answer y: 2  prediction: [ 0.5938437   0.60705614 81.30417     0.6280027   2.124997    0.09292822\n",
      "  1.1954112   0.18280187]\n",
      "index: 20  actual y: 1  answer y: 1  prediction: [ 0.01709878 99.96215     0.02270137  0.02418656  0.06582133  0.03167233\n",
      "  0.01513886  0.03015113]\n",
      "index: 21  actual y: 6  answer y: 5  prediction: [ 0.46420488  0.19189371  0.1073566   1.3402945   0.6862608  65.83485\n",
      "  0.5491511   2.87819   ]\n",
      "index: 22  actual y: 5  answer y: 5  prediction: [ 0.00929321  0.49523896  0.02730838  1.5853723   0.67008275 98.77099\n",
      "  0.0165169   0.01801994]\n",
      "index: 23  actual y: 2  answer y: 2  prediction: [ 0.01632623  0.02037377 99.96683     0.00855526  0.03528428  0.03927434\n",
      "  0.00517033  0.01275143]\n",
      "index: 24  actual y: 0  answer y: 0  prediction: [98.61757     0.01932073  0.01625435  0.4683907   0.0280516   0.06218591\n",
      "  0.03834815  0.5476181 ]\n",
      "index: 25  actual y: 6  answer y: 6  prediction: [ 0.32950133  0.05845267  0.04355301  0.10358395  0.16449827  0.13850933\n",
      " 99.71973     0.0231408 ]\n",
      "index: 26  actual y: 4  answer y: 4  prediction: [ 0.0625243   1.0432969   1.4099866   0.01747712 95.86746     0.48123485\n",
      "  0.03353645  0.13240573]\n",
      "index: 27  actual y: 2  answer y: 2  prediction: [ 0.02738441  0.04127003 99.84553     0.02244917  0.20430106  0.04138531\n",
      "  0.01395351  0.13235442]\n",
      "index: 28  actual y: 0  answer y: 2  prediction: [13.500433    0.0869853  26.701536    4.5976133   2.2736342   0.430662\n",
      "  1.2272009   0.11855453]\n",
      "index: 29  actual y: 4  answer y: 4  prediction: [ 0.00873802  0.07519625  0.03101956  0.01535246 99.92992     0.06354465\n",
      "  0.03291336  0.02392195]\n",
      "index: 30  actual y: 3  answer y: 3  prediction: [ 0.09608032  0.09319374  0.04116448 99.69949     0.20347191  0.10098664\n",
      "  0.08385117  0.07868763]\n",
      "index: 31  actual y: 0  answer y: 3  prediction: [3.93461    7.462348   0.32248163 7.8652368  7.081971   0.1106545\n",
      " 0.3129915  0.3495857 ]\n",
      "index: 32  actual y: 1  answer y: 1  prediction: [ 0.07158229 99.89436     0.04610722  0.03638947  0.06392651  0.04719871\n",
      "  0.00518509  0.05865527]\n",
      "index: 33  actual y: 6  answer y: 6  prediction: [ 0.53439254  0.05777841  0.05681244  0.21540913  0.11992045  0.17628612\n",
      " 99.37857     0.11045192]\n",
      "index: 34  actual y: 6  answer y: 6  prediction: [ 0.0813188   0.17642464  0.19541283  0.7581986   0.2769195   1.4876279\n",
      " 94.59069     0.08438464]\n",
      "index: 35  actual y: 1  answer y: 1  prediction: [ 0.02824258 99.938225    0.00885028  0.00596933  0.10856092  0.02376745\n",
      "  0.02672888  0.04604194]\n",
      "index: 36  actual y: 3  answer y: 3  prediction: [ 0.02050949  2.00956     0.08338491 98.71144     0.65018356  0.02875389\n",
      "  0.00380355  0.04968757]\n",
      "index: 37  actual y: 1  answer y: 1  prediction: [ 0.01136551 99.94006     0.03299658  0.01014805  0.0489743   0.02696318\n",
      "  0.02405855  0.0400697 ]\n",
      "index: 38  actual y: 5  answer y: 5  prediction: [ 0.58532727  0.18135433  0.04537784  0.01169877  0.1316897  99.1937\n",
      "  0.14275059  0.08353525]\n",
      "index: 39  actual y: 3  answer y: 3  prediction: [17.623165    0.0552899   0.16375358 79.892654    0.2518261   0.2063908\n",
      "  0.0535624   0.08388162]\n",
      "index: 40  actual y: 5  answer y: 5  prediction: [ 0.42881554  0.10356008  0.10914391  0.0492506   0.03650215 99.434296\n",
      "  0.08357251  0.13314424]\n",
      "index: 41  actual y: 0  answer y: 6  prediction: [ 8.037784    3.2343123   0.06156831  0.2868641   1.6301765   0.03004588\n",
      " 67.07292     0.05681014]\n",
      "index: 42  actual y: 2  answer y: 2  prediction: [ 0.03343924  0.20959957 99.890396    0.03481861  0.07249411  0.06185799\n",
      "  0.02940586  0.01108943]\n",
      "index: 43  actual y: 4  answer y: 4  prediction: [ 0.01587342  0.02578913  0.39783362  0.28787586 38.9588      2.4999485\n",
      " 12.618825    0.9088597 ]\n",
      "index: 44  actual y: 3  answer y: 3  prediction: [ 0.04756294  0.03265335  0.01221564 99.91931     0.05498617  0.01823964\n",
      "  0.00493539  0.04797966]\n",
      "index: 45  actual y: 3  answer y: 0  prediction: [61.570816    0.14745677  0.5678493   1.0360371   4.611816    0.8103015\n",
      "  0.13606273  1.8643409 ]\n",
      "index: 46  actual y: 5  answer y: 5  prediction: [ 0.05469102  0.03753176  0.01326313  0.1105987   0.06464469 99.838806\n",
      "  0.03801987  0.03283244]\n",
      "index: 47  actual y: 5  answer y: 5  prediction: [ 0.01173414  0.00940833  0.00496734  0.00440801  0.01663245 99.977806\n",
      "  0.00960284  0.01855358]\n",
      "index: 48  actual y: 3  answer y: 3  prediction: [ 2.7879574   0.09013493  0.10432275 97.42167     0.07518668  0.41555753\n",
      "  0.02965127  0.1027616 ]\n",
      "index: 49  actual y: 4  answer y: 4  prediction: [ 0.04519648  0.12415744 17.088568    0.04283119 92.83729     0.07059887\n",
      "  0.0627705   0.03186113]\n",
      "index: 50  actual y: 2  answer y: 2  prediction: [ 0.02979974  0.05482655 99.931046    0.02508613  0.11605313  0.05438018\n",
      "  0.02379148  0.07489713]\n",
      "index: 51  actual y: 4  answer y: 4  prediction: [ 2.355955    0.08149322  0.03451107  1.6014684  94.96006     0.36965778\n",
      "  0.0368612   0.07951955]\n",
      "index: 52  actual y: 2  answer y: 2  prediction: [ 0.05547376  0.06302482 99.89721     0.01421811  0.02946384  0.11900324\n",
      "  0.02875633  0.06633985]\n",
      "index: 53  actual y: 3  answer y: 4  prediction: [ 1.019769    0.24111806  0.43612576  0.33127502 96.04139     0.20888548\n",
      "  0.01513437  0.08215866]\n",
      "index: 54  actual y: 3  answer y: 3  prediction: [ 9.454541    0.1708713   0.0257836  92.154366    0.14778613  0.08714445\n",
      "  0.03545372  0.02866444]\n",
      "index: 55  actual y: 7  answer y: 3  prediction: [ 2.4220803   0.76235545  0.03394998 36.607136    3.3801339   7.1087637\n",
      "  0.01200445  0.3902439 ]\n",
      "index: 56  actual y: 2  answer y: 2  prediction: [ 0.033451    0.05984936 99.94911     0.08071713  0.04693347  0.07709101\n",
      "  0.01693497  0.01686339]\n",
      "index: 57  actual y: 6  answer y: 7  prediction: [ 4.4509697   0.05979714  0.08584841  0.50968486  0.03271302  0.05913214\n",
      "  4.6095066  86.12297   ]\n",
      "index: 58  actual y: 0  answer y: 0  prediction: [98.701195    0.32388267  0.08120254  0.42405865  0.21534951  0.07448501\n",
      "  0.21714205  0.14755087]\n",
      "index: 59  actual y: 3  answer y: 0  prediction: [29.323288    0.06954587  0.4204262   0.77905416  5.2195864   0.08201899\n",
      "  0.04181096  7.3542857 ]\n",
      "index: 60  actual y: 1  answer y: 1  prediction: [ 0.10827262 97.159515    0.82273316  0.00681631  0.06669     0.16828327\n",
      "  0.26128837  0.10893262]\n",
      "index: 61  actual y: 5  answer y: 5  prediction: [ 0.01283069  0.02513785  0.00921902  0.00285627  0.01869643 99.97142\n",
      "  0.00825144  0.0269501 ]\n",
      "index: 62  actual y: 0  answer y: 0  prediction: [50.35594     1.6106805   0.05271417  3.581957    0.15039437  0.11499001\n",
      "  9.531717    0.07773297]\n",
      "index: 63  actual y: 4  answer y: 4  prediction: [ 0.04199683  0.01993019  0.03446172  0.02222709 99.76712     0.5838214\n",
      "  0.05203913  0.1371592 ]\n",
      "index: 64  actual y: 7  answer y: 7  prediction: [ 2.1861105   0.09883812  0.09085537  2.7378135   0.8204737   0.5764631\n",
      "  0.03467091 69.85294   ]\n",
      "index: 65  actual y: 3  answer y: 3  prediction: [ 5.4859395   0.85683346  0.0309807  76.34747     2.1985455   0.08033101\n",
      "  0.0050428   0.04538837]\n",
      "index: 66  actual y: 4  answer y: 4  prediction: [ 0.01404201  0.17427963  0.02951763  0.01756132 99.68525     0.16536523\n",
      "  0.5527938   0.19094098]\n",
      "index: 67  actual y: 4  answer y: 4  prediction: [ 0.06956569  0.09297517  0.22297324  0.03074577 99.293785    0.40685475\n",
      "  0.09217981  0.0323636 ]\n",
      "index: 68  actual y: 1  answer y: 1  prediction: [ 4.5950036  90.30856     0.08068791  0.04931004  0.01096309  0.09743321\n",
      "  0.0400621   0.5020364 ]\n",
      "index: 69  actual y: 4  answer y: 4  prediction: [ 0.00727997  0.04723859  0.04240181  0.8341117  99.48129     0.06490447\n",
      "  0.02086983  0.08402198]\n",
      "index: 70  actual y: 6  answer y: 6  prediction: [ 0.891638    2.0530977   0.16628861  1.2257459   0.08429165  1.7965975\n",
      " 54.36331     0.91101664]\n",
      "index: 71  actual y: 2  answer y: 2  prediction: [ 0.04334114  0.05022183 99.898834    0.08343863  0.12951411  0.07188781\n",
      "  0.02821548  0.03665468]\n",
      "index: 72  actual y: 5  answer y: 5  prediction: [ 0.01530547  0.1221353   0.13501163  0.02024404  0.01966345 99.865974\n",
      "  0.06719973  0.04877141]\n",
      "index: 73  actual y: 4  answer y: 4  prediction: [ 0.05772791  0.0929272   0.01046628  0.14922716 99.68742     0.03966325\n",
      "  0.02095016  0.07792476]\n",
      "index: 74  actual y: 3  answer y: 3  prediction: [ 0.002355    0.01196451  0.01164097 99.982925    0.01143403  0.01857817\n",
      "  0.00533709  0.04611849]\n",
      "index: 75  actual y: 2  answer y: 2  prediction: [ 0.13590337  0.44064784 99.84377     0.05660055  0.02308184  0.02634334\n",
      "  0.01676001  0.02192676]\n",
      "index: 76  actual y: 6  answer y: 5  prediction: [ 0.02241036  0.11662801  0.12698132  0.01087879  0.09551247 99.24791\n",
      "  1.3330108   0.13755766]\n",
      "index: 77  actual y: 3  answer y: 3  prediction: [ 0.67059016  0.12238748  0.02733498 98.58411     0.33849716  0.07472496\n",
      "  0.10366382  0.09758003]\n",
      "index: 78  actual y: 7  answer y: 7  prediction: [ 0.00825415  0.05396175  0.3939224   0.01414181  0.06066496  0.04685424\n",
      "  0.04535989 99.82454   ]\n",
      "index: 79  actual y: 4  answer y: 5  prediction: [ 0.1584155   0.44648224  0.03784185  0.01475537  1.2020696  99.09717\n",
      "  0.08040901  0.08633199]\n",
      "index: 80  actual y: 0  answer y: 0  prediction: [78.62484    20.127127    0.01261947  0.07987151  0.11167496  0.2660376\n",
      "  0.07611971  0.1533466 ]\n",
      "index: 81  actual y: 6  answer y: 6  prediction: [ 0.05767519  0.05385016  0.0463124   0.19054608  0.44338745  0.13154179\n",
      " 99.49339     0.08476025]\n",
      "index: 82  actual y: 6  answer y: 6  prediction: [ 0.08451548  0.0286862   0.02359005  1.2519234   0.01963005  0.09471276\n",
      " 99.59391     0.30924532]\n",
      "index: 83  actual y: 4  answer y: 4  prediction: [ 0.01917898  0.03291351  0.06785581  0.01139237 99.52726     1.7428199\n",
      "  0.1485752   0.03885307]\n",
      "index: 84  actual y: 4  answer y: 5  prediction: [ 0.15996394  0.1652615   1.8488413   0.03469732  3.1899252  74.28297\n",
      "  3.8280303   0.05662682]\n",
      "index: 85  actual y: 0  answer y: 0  prediction: [64.91695     1.4429946   0.03476411  0.24379325  0.02217386 11.110779\n",
      "  0.01080619  1.1443604 ]\n",
      "index: 86  actual y: 4  answer y: 1  prediction: [ 0.11232793 95.94857     0.68421733  0.08705184  0.7129389   0.46422297\n",
      "  0.11400481  0.3164076 ]\n",
      "index: 87  actual y: 0  answer y: 0  prediction: [94.84511     3.455562    0.0753234   0.26729792  0.05713034  0.06184896\n",
      "  0.05820007  0.27622366]\n",
      "index: 88  actual y: 7  answer y: 7  prediction: [ 0.17978887 18.503912    7.363922    0.1163295   0.35734555  0.03036452\n",
      "  1.5218005  40.962063  ]\n",
      "index: 89  actual y: 3  answer y: 3  prediction: [ 2.0058386  13.714274    0.04860677 42.579887    1.2978147   0.01068779\n",
      "  0.04311239  0.91835743]\n",
      "index: 90  actual y: 0  answer y: 7  prediction: [ 6.1618533   0.20652737  0.03423084  0.04683725 34.554317    0.67530507\n",
      "  0.05838645 39.60162   ]\n",
      "index: 91  actual y: 7  answer y: 7  prediction: [ 0.07737351  0.03048783  0.40789106  0.2284297   0.47941202  0.4225893\n",
      "  0.28214708 97.79925   ]\n",
      "index: 92  actual y: 5  answer y: 5  prediction: [ 0.01868771  0.10951666  0.10572759  0.01090971  0.09515746 99.550316\n",
      "  0.40057436  0.19531763]\n",
      "index: 93  actual y: 7  answer y: 7  prediction: [ 0.62605447  6.955022    0.0295767   0.14444736  0.10325325  0.19051996\n",
      "  0.520214   92.74365   ]\n",
      "index: 94  actual y: 4  answer y: 4  prediction: [ 0.00845313  0.02577601  0.02238774  0.0822772  98.79958     0.11321074\n",
      "  0.06721936  2.770564  ]\n",
      "index: 95  actual y: 6  answer y: 6  prediction: [ 0.01549952  0.03153476  0.01269316  0.02538387  0.01579764  0.02668951\n",
      " 99.97794     0.00768008]\n",
      "index: 96  actual y: 4  answer y: 1  prediction: [ 0.02407397 32.179546    0.22273359  0.01590966 11.3273535   0.00758483\n",
      " 29.910439    0.03952789]\n",
      "index: 97  actual y: 0  answer y: 0  prediction: [99.8999      0.01073038  0.03753141  0.11083653  0.04318437  0.0546625\n",
      "  0.01773568  0.03093878]\n",
      "index: 98  actual y: 1  answer y: 1  prediction: [ 0.03491234 99.91467     0.05868915  0.01269573  0.04544858  0.01659418\n",
      "  0.00878896  0.02085686]\n",
      "index: 99  actual y: 6  answer y: 1  prediction: [ 0.2884857  93.05151     0.38518232  0.02716652  0.09190767  3.6859126\n",
      "  1.4766921   0.03797319]\n",
      "index: 100  actual y: 3  answer y: 7  prediction: [ 0.167886    0.05773406  0.0340296  12.282112    0.96446955  0.12684235\n",
      "  0.00545831 76.533226  ]\n",
      "index: 101  actual y: 1  answer y: 1  prediction: [ 0.01422607 99.94361     0.03257264  0.00553596  0.04711272  0.01992206\n",
      "  0.01047355  0.01901669]\n",
      "index: 102  actual y: 6  answer y: 6  prediction: [ 0.05350253  0.02006101  0.02343315  0.02044404  0.15485178  0.08485743\n",
      " 99.93406     0.03019938]\n",
      "index: 103  actual y: 6  answer y: 6  prediction: [ 0.15157256  0.10529041  0.06404509  0.00847451  0.03052634  0.04229191\n",
      " 99.76141     0.15443079]\n",
      "index: 104  actual y: 7  answer y: 7  prediction: [ 0.11742103 15.686218    1.2751262   0.18725227  0.04140944  1.0456773\n",
      "  0.03691208 54.294006  ]\n",
      "index: 105  actual y: 5  answer y: 5  prediction: [ 3.4362476   0.08763995  0.04105264  0.077586    0.4763469  64.698265\n",
      "  6.2487783   0.29129955]\n",
      "index: 106  actual y: 6  answer y: 6  prediction: [ 0.26357073  0.05308626  0.06887603  0.96978366  6.2977924   0.31089094\n",
      " 91.85203     0.07262546]\n",
      "index: 107  actual y: 0  answer y: 5  prediction: [ 1.3452237  10.279626    0.3373193   0.5546505   2.0018585  26.98802\n",
      "  0.01822428  0.3450307 ]\n",
      "index: 108  actual y: 1  answer y: 1  prediction: [ 0.12151656 99.55035     0.08922359  0.03060539  0.06390556  0.20977417\n",
      "  0.10646669  0.06280823]\n",
      "index: 109  actual y: 4  answer y: 4  prediction: [ 0.06230005  0.0369283   0.04098132  0.04554446 99.92056     0.1080503\n",
      "  0.01695622  0.02968552]\n",
      "index: 110  actual y: 0  answer y: 0  prediction: [88.6962      0.20821077  0.06540948  1.3041587   0.19192284  0.35738206\n",
      "  2.5039403   0.3754424 ]\n",
      "index: 111  actual y: 4  answer y: 4  prediction: [ 0.80745816  0.02181857  0.03448535  0.48441496 98.46853     0.16919658\n",
      "  0.07343472  0.08578336]\n",
      "index: 112  actual y: 7  answer y: 7  prediction: [ 0.05127684  0.30389833  1.0537444   0.00436773  5.431096    0.14376834\n",
      "  0.05539097 92.32223   ]\n",
      "index: 113  actual y: 7  answer y: 5  prediction: [ 0.01780203  0.0052379   0.05123414  0.15133303  0.0658967  99.60173\n",
      "  0.0986238   0.24270192]\n",
      "index: 114  actual y: 2  answer y: 2  prediction: [ 0.0350395   0.04708152 99.933205    0.0550089   0.07325951  0.04107814\n",
      "  0.02007012  0.03331739]\n",
      "index: 115  actual y: 5  answer y: 5  prediction: [ 0.10878216  0.03458533  0.01862238  0.008944    0.14543301 99.556786\n",
      "  0.33778575  0.09792575]\n",
      "index: 116  actual y: 1  answer y: 7  prediction: [ 4.8136654   5.152136    0.21286044  0.09034649  5.8394375   0.9431611\n",
      "  0.70736146 11.712843  ]\n",
      "index: 117  actual y: 5  answer y: 5  prediction: [ 0.42048305  0.05485875  0.02682214  0.01063162  0.18916388 99.51838\n",
      "  0.03036336  0.17232323]\n",
      "index: 118  actual y: 2  answer y: 2  prediction: [ 0.0604846   0.03102542 99.95012     0.02062334  0.05268037  0.04889148\n",
      "  0.01260253  0.01518745]\n",
      "index: 119  actual y: 0  answer y: 0  prediction: [93.62491     0.05761171  0.12825772 10.310128    0.3642545   0.1837953\n",
      "  0.05748513  0.03704939]\n",
      "index: 120  actual y: 1  answer y: 1  prediction: [ 0.00699825 99.66667     0.19053927  0.03188073  0.41487467  0.0411239\n",
      "  0.04567859  0.10490618]\n",
      "index: 121  actual y: 2  answer y: 2  prediction: [ 0.02503075  0.11904049 99.9638      0.01551305  0.02794914  0.04169066\n",
      "  0.01213581  0.01865972]\n",
      "index: 122  actual y: 6  answer y: 6  prediction: [ 0.9361974   1.3434901   0.24611287  0.15114595  0.10970748  0.15552546\n",
      " 94.78708     0.97211516]\n",
      "index: 123  actual y: 5  answer y: 5  prediction: [ 0.01586937  0.04150395  0.09312686  0.00814459  0.09793507 99.92222\n",
      "  0.01321856  0.01173766]\n",
      "index: 124  actual y: 0  answer y: 0  prediction: [70.891785    1.8731813   0.08965232 19.651482    0.07928636  0.18274504\n",
      "  0.09303001  0.05255145]\n",
      "index: 125  actual y: 0  answer y: 1  prediction: [ 1.1576616  88.5158      0.03374176  0.03920734  0.09876654  0.22330086\n",
      "  3.562224    0.09123877]\n",
      "index: 126  actual y: 1  answer y: 1  prediction: [ 0.03145279 99.95376     0.01291192  0.02921152  0.05613511  0.04600918\n",
      "  0.00562187  0.02787633]\n",
      "index: 127  actual y: 2  answer y: 2  prediction: [ 0.01142163  0.04390416 99.96032     0.01256139  0.02658014  0.04095234\n",
      "  0.01323364  0.01920526]\n",
      "index: 128  actual y: 1  answer y: 1  prediction: [ 0.1750401  76.46175     0.03879873  6.4932547  24.14148     0.03426147\n",
      "  0.00973302  0.02989484]\n",
      "index: 129  actual y: 3  answer y: 3  prediction: [ 0.08180119  0.08031969  0.02085291 99.88669     0.08036815  0.03190947\n",
      "  0.00885475  0.02710693]\n",
      "index: 130  actual y: 5  answer y: 5  prediction: [ 0.01496161  0.06076964  0.00888828  0.01206479  0.00942547 99.96166\n",
      "  0.00865342  0.01483924]\n",
      "index: 131  actual y: 5  answer y: 0  prediction: [69.87987     0.10145961  0.05868071  2.9299815   0.14703576  8.726558\n",
      "  0.3269006   2.1063147 ]\n",
      "index: 132  actual y: 4  answer y: 4  prediction: [ 0.01617645  0.09092916  0.02563186  0.08058451 99.86672     0.05248279\n",
      "  0.03604694  0.05077837]\n",
      "index: 133  actual y: 5  answer y: 5  prediction: [ 0.77000684  0.05763876  0.02444506  0.04173456  0.73158944 97.33348\n",
      "  0.32408163  0.17035665]\n",
      "index: 134  actual y: 2  answer y: 2  prediction: [ 0.016551    0.0704514  99.966194    0.01283168  0.0158442   0.01439368\n",
      "  0.01253876  0.03392076]\n",
      "index: 135  actual y: 1  answer y: 1  prediction: [ 0.00711324 99.961334    0.02875301  0.003918    0.02848279  0.027186\n",
      "  0.02078395  0.0234373 ]\n",
      "index: 136  actual y: 0  answer y: 0  prediction: [75.776344    0.11977748  0.03149006  1.7706383   9.280562    0.40187547\n",
      "  0.06184192  0.18317072]\n",
      "index: 137  actual y: 4  answer y: 4  prediction: [ 0.44140428  0.2529904   0.69852185  0.07565074 64.101204   20.543657\n",
      "  0.16038808  0.15065   ]\n",
      "index: 138  actual y: 3  answer y: 3  prediction: [ 0.0324649   0.03737131  0.03214834 99.826744    0.04648456  0.17243886\n",
      "  0.01018156  0.13414556]\n",
      "index: 139  actual y: 5  answer y: 5  prediction: [ 0.01423157  0.06782105  0.00870419  0.00463648  0.00873508 99.97231\n",
      "  0.00803315  0.01683156]\n",
      "index: 140  actual y: 6  answer y: 6  prediction: [ 0.05662974  0.09704299  0.07830139  0.05144148  0.21685007  0.03954197\n",
      " 99.74119     0.05271347]\n",
      "index: 141  actual y: 5  answer y: 5  prediction: [ 2.2905889   0.0174662  26.185776    0.05196832  0.19716704 49.07316\n",
      "  0.35124913  0.88402045]\n",
      "index: 142  actual y: 4  answer y: 6  prediction: [ 0.0508406   1.0299995   0.02684887  9.107856    0.08227869  0.6322562\n",
      " 64.07938     3.272561  ]\n",
      "index: 143  actual y: 1  answer y: 1  prediction: [ 0.01899698 99.851234    0.06578846  0.07250962  0.13392894  0.03061558\n",
      "  0.04530788  0.03574014]\n",
      "index: 144  actual y: 7  answer y: 7  prediction: [ 0.2702114   2.2590013   0.01934582  0.00894684  0.04975399  0.06150101\n",
      "  0.04293641 98.71639   ]\n",
      "index: 145  actual y: 1  answer y: 1  prediction: [ 0.02258539 99.971504    0.02107496  0.0166025   0.10348428  0.04602899\n",
      "  0.00946745  0.01502153]\n",
      "index: 146  actual y: 3  answer y: 3  prediction: [ 0.01275305  0.00535839  0.00457657 99.98514     0.00716748  0.01026962\n",
      "  0.00635153  0.01838505]\n",
      "index: 147  actual y: 0  answer y: 0  prediction: [97.1949      0.20072488  0.02399636  0.1400131   0.03743484  0.12648465\n",
      "  2.8792245   0.05438433]\n",
      "index: 148  actual y: 2  answer y: 2  prediction: [ 0.01188561  0.04303568 99.94667     0.01406345  0.02402472  0.01688887\n",
      "  0.02185834  0.06425391]\n",
      "index: 149  actual y: 6  answer y: 6  prediction: [ 0.03899359  0.0519347   0.03887774  0.4229414   0.2742655   0.05571237\n",
      " 99.659035    0.02752555]\n",
      "index: 150  actual y: 0  answer y: 0  prediction: [99.61252     0.10426044  0.08545487  0.12325282  0.08492125  0.07663494\n",
      "  0.05350033  0.11768361]\n",
      "index: 151  actual y: 2  answer y: 2  prediction: [ 0.05666429  0.03507178 99.973854    0.01820881  0.02271181  0.02992923\n",
      "  0.02640855  0.05052087]\n",
      "index: 152  actual y: 0  answer y: 0  prediction: [98.160324    0.3085732   0.05222953  0.06826004  0.05131405  0.09839723\n",
      "  1.0980017   0.21178529]\n",
      "index: 153  actual y: 2  answer y: 2  prediction: [ 0.03869787  0.01861844 99.96488     0.0048008   0.02073662  0.13364212\n",
      "  0.02523334  0.01102694]\n",
      "index: 154  actual y: 1  answer y: 1  prediction: [ 0.02423303 99.949776    0.01915766  0.01745978  0.04367549  0.05291286\n",
      "  0.00548159  0.03092787]\n",
      "index: 155  actual y: 5  answer y: 5  prediction: [ 0.02215824  0.05268707  0.01065125  0.05278219  0.01367071 99.93728\n",
      "  0.01987667  0.01887956]\n",
      "index: 156  actual y: 2  answer y: 2  prediction: [ 0.03686703  0.06388078 99.80482     0.05429346  0.37798566  0.01621936\n",
      "  0.0082693   0.06393949]\n",
      "index: 157  actual y: 3  answer y: 0  prediction: [51.85564     0.9508688   0.06766639 14.79079     0.56506413  0.33502883\n",
      "  0.00852868  0.1309247 ]\n",
      "index: 158  actual y: 7  answer y: 3  prediction: [ 0.16006984  0.50744146  0.12383106 61.27433     0.47852513  4.4059525\n",
      "  0.01021811  6.136067  ]\n",
      "index: 159  actual y: 1  answer y: 1  prediction: [ 0.10997107 97.12433    21.640718    0.02341182  0.01229894  0.03587594\n",
      "  0.0092696   0.01317429]\n",
      "index: 160  actual y: 0  answer y: 0  prediction: [99.70085     0.08105842  0.06982051  0.08172363  0.08668639  0.05151068\n",
      "  0.06578922  0.10828277]\n",
      "index: 161  actual y: 1  answer y: 5  prediction: [ 0.08438061  1.3433195   0.18296629  1.9665228   5.9617963  65.17973\n",
      "  0.27580616  0.08175   ]\n",
      "index: 162  actual y: 7  answer y: 4  prediction: [ 0.1711163   0.03487395  0.00802219  0.06560852 65.300644   33.582283\n",
      "  0.03573326  3.9904754 ]\n",
      "index: 163  actual y: 2  answer y: 2  prediction: [ 0.09362739  1.1018428  98.62756     0.06011958  0.0655448   0.2572139\n",
      "  0.05818662  0.04427791]\n",
      "index: 164  actual y: 7  answer y: 7  prediction: [ 0.00732926  0.03937595  0.22780706  0.08303662  0.03373819  0.06167787\n",
      "  0.16649677 99.758575  ]\n",
      "index: 165  actual y: 4  answer y: 4  prediction: [ 0.00648679  0.04652565  0.01982524  0.04033948 99.84824     0.34004688\n",
      "  0.02798666  0.02402568]\n",
      "index: 166  actual y: 6  answer y: 6  prediction: [ 0.07381745  0.2824797   0.07420583  0.02139308  0.0589685   0.05286781\n",
      " 99.80675     0.05042247]\n",
      "index: 167  actual y: 1  answer y: 1  prediction: [ 0.00879397 99.97131     0.0199353   0.01543478  0.01351967  0.06140481\n",
      "  0.00403271  0.01506386]\n",
      "index: 168  actual y: 1  answer y: 1  prediction: [ 0.01064838 99.97947     0.00667104  0.01158214  0.04479694  0.03048838\n",
      "  0.00665969  0.01975039]\n",
      "index: 169  actual y: 5  answer y: 5  prediction: [ 0.03022991  0.05908827  0.01491638  0.02413812  0.01872821 99.92937\n",
      "  0.02069191  0.0187661 ]\n",
      "index: 170  actual y: 5  answer y: 5  prediction: [ 0.01920317  0.07296309  0.05878903  0.04372796  0.15484942 99.837364\n",
      "  0.05520434  0.04389357]\n",
      "index: 171  actual y: 0  answer y: 0  prediction: [86.935       0.27882951  0.0172437   0.00500288  0.02340981  0.03958574\n",
      " 14.210752    0.13585627]\n",
      "index: 172  actual y: 7  answer y: 7  prediction: [ 0.006295    0.02270293  0.01244757  0.01616571  0.02323838  0.02307594\n",
      "  0.0416038  99.98411   ]\n",
      "index: 173  actual y: 7  answer y: 7  prediction: [ 1.9911684   0.16778646  0.02281347  0.5224615   0.09614977  0.05119116\n",
      "  0.77438104 94.82066   ]\n",
      "index: 174  actual y: 3  answer y: 3  prediction: [ 1.5527574   1.7038656   0.021657   29.777473    2.5746834   1.5635884\n",
      "  0.00261434  0.8512217 ]\n",
      "index: 175  actual y: 4  answer y: 4  prediction: [ 0.04372296  0.03853438  0.11164619 15.855013   87.437584    0.5725361\n",
      "  0.04396417  0.10446671]\n",
      "index: 176  actual y: 7  answer y: 3  prediction: [ 0.380931    0.06107675  0.03043984 85.59493     0.23997152  0.1920569\n",
      "  0.6715218  10.77643   ]\n",
      "index: 177  actual y: 2  answer y: 2  prediction: [ 0.08444025  0.02917993 99.693474    0.02412139  0.19189315  0.21706426\n",
      "  0.02271646  0.22790064]\n",
      "index: 178  actual y: 5  answer y: 5  prediction: [ 0.00475624  0.03996403  0.01371651  0.00415754  0.00870306 99.98222\n",
      "  0.00528441  0.01002925]\n",
      "index: 179  actual y: 2  answer y: 2  prediction: [ 0.05775144  0.02889098 99.96083     0.04070649  0.02944343  0.10695002\n",
      "  0.02769149  0.04069537]\n",
      "index: 180  actual y: 4  answer y: 4  prediction: [ 0.00927412  0.03761293  0.02724519  0.04352333 99.90175     0.05676037\n",
      "  0.05691965  0.0325464 ]\n",
      "index: 181  actual y: 4  answer y: 5  prediction: [ 0.25153357  0.98788756  0.23499662  0.01145105  2.1582482  92.78819\n",
      "  0.9687887   0.09272556]\n",
      "index: 182  actual y: 7  answer y: 7  prediction: [ 0.0113212   0.06751572  0.32782087  0.29360712  0.10862071  0.3821558\n",
      "  0.3223287  99.00729   ]\n",
      "index: 183  actual y: 0  answer y: 1  prediction: [ 0.561468   93.71546     0.02671321  1.7013428   0.04204677  0.0491999\n",
      "  0.25041205  0.36589667]\n",
      "index: 184  actual y: 4  answer y: 4  prediction: [ 0.12691715  2.5387588   0.01693029  5.519148   40.785927    2.0524201\n",
      "  0.14452772  0.31219578]\n",
      "index: 185  actual y: 7  answer y: 7  prediction: [ 0.01249935  0.03048251  0.01047388  0.0149036   0.18675518  0.03780496\n",
      "  0.23261417 99.754745  ]\n",
      "index: 186  actual y: 1  answer y: 1  prediction: [13.233203   76.61732     0.01018921  0.00817125  0.01257297  1.2450117\n",
      "  0.07220804  0.26813477]\n",
      "index: 187  actual y: 2  answer y: 2  prediction: [ 0.03132268  0.04871336 99.96895     0.00999341  0.02857317  0.02396167\n",
      "  0.01154996  0.00951184]\n",
      "index: 188  actual y: 3  answer y: 4  prediction: [ 2.1232252   0.0626464   0.97384864  4.3295093  74.07217     0.05044307\n",
      "  0.0074699   0.09350917]\n",
      "index: 189  actual y: 2  answer y: 2  prediction: [ 0.00951441  0.04236824 99.97019     0.00463978  0.01227777  0.0436515\n",
      "  0.01970952  0.02561915]\n",
      "index: 190  actual y: 2  answer y: 2  prediction: [ 0.04213442  0.05464346 99.96802     0.02252234  0.03558224  0.06427842\n",
      "  0.04278647  0.02817452]\n",
      "index: 191  actual y: 7  answer y: 7  prediction: [ 0.15922974  1.9571437   0.287085   13.531121    0.1075756   0.01555168\n",
      "  0.11979619 81.07138   ]\n",
      "index: 192  actual y: 3  answer y: 3  prediction: [ 0.02307422  0.03201502  0.01388387 99.96211     0.04030744  0.03369234\n",
      "  0.01179736  0.0200949 ]\n",
      "index: 193  actual y: 2  answer y: 2  prediction: [ 0.0380409   0.0446957  99.92514     0.02689527  0.03260613  0.07711907\n",
      "  0.03788856  0.03219263]\n",
      "index: 194  actual y: 2  answer y: 2  prediction: [ 0.05368758  0.01944872 99.97795     0.01490541  0.02758269  0.03369072\n",
      "  0.0155342   0.01678032]\n",
      "index: 195  actual y: 6  answer y: 6  prediction: [ 0.19538659  0.18497707  0.07633148  0.03760411  0.32873264  0.10452525\n",
      " 99.575005    0.09468632]\n",
      "index: 196  actual y: 3  answer y: 0  prediction: [65.13225     0.12750463  0.18846169  0.7382192   0.21719489  0.2264096\n",
      "  0.02618768 11.423691  ]\n",
      "index: 197  actual y: 7  answer y: 6  prediction: [ 0.05402077  0.12338896  0.18260978 50.223637    0.05494224  0.04968771\n",
      " 58.03514     1.9041622 ]\n",
      "index: 198  actual y: 2  answer y: 2  prediction: [ 0.02906664  0.02081737 99.957016    0.0147271   0.03424842  0.14695102\n",
      "  0.02229223  0.01911872]\n",
      "index: 199  actual y: 5  answer y: 5  prediction: [ 0.02100855  0.02174501  0.01481893  0.00564615  0.07882652 99.82857\n",
      "  0.04993675  0.13103773]\n",
      "index: 200  actual y: 3  answer y: 3  prediction: [ 9.454541    0.1708713   0.0257836  92.154366    0.14778613  0.08714445\n",
      "  0.03545372  0.02866444]\n",
      "index: 201  actual y: 5  answer y: 5  prediction: [ 0.03550371  0.02188607  0.26158068  0.00378327  6.862314   97.77826\n",
      "  0.01207106  0.03901752]\n",
      "index: 202  actual y: 0  answer y: 3  prediction: [ 0.96155304  5.0007224   0.01617241 71.68115     0.01723282  0.85668844\n",
      "  0.0641344   0.25819832]\n",
      "index: 203  actual y: 2  answer y: 2  prediction: [ 0.02292233  0.02193431 99.940506    0.02323512  0.02342658  0.07054268\n",
      "  0.01034311  0.03273071]\n",
      "index: 204  actual y: 5  answer y: 5  prediction: [ 0.02406328  0.03504951  0.00786959  0.00533291  0.01419396 99.97471\n",
      "  0.00616898  0.00997827]\n",
      "index: 205  actual y: 5  answer y: 5  prediction: [ 0.02467675  0.02758653  0.00885052  0.04053492  0.03664901 99.87339\n",
      "  0.02738773  0.07683548]\n",
      "index: 206  actual y: 4  answer y: 4  prediction: [ 0.15439044  0.0246078   0.19067536  7.829934   24.218182   21.786547\n",
      "  0.09818934  0.973999  ]\n",
      "index: 207  actual y: 0  answer y: 0  prediction: [99.81239     0.08488872  0.30157796  0.13285212  0.01561212  0.05170235\n",
      "  0.08877461  0.08427303]\n",
      "index: 208  actual y: 0  answer y: 0  prediction: [82.68243     2.7809896   0.49885994  1.6344835   0.13631241  0.02443914\n",
      "  0.03890355  0.62788814]\n",
      "index: 209  actual y: 7  answer y: 7  prediction: [ 0.0168845   0.05254822  0.0044809   0.01942813  0.2508072   0.02910261\n",
      "  0.01586631 99.960846  ]\n",
      "index: 210  actual y: 7  answer y: 2  prediction: [ 0.03166056  0.34494364 78.08563     0.02179304  0.3323456   4.3967624\n",
      "  0.48352188  0.922363  ]\n",
      "index: 211  actual y: 3  answer y: 3  prediction: [ 7.9907255   0.12692206  0.07171599 90.522804    0.3561414   0.29811493\n",
      "  0.00877109  0.10369053]\n",
      "index: 212  actual y: 1  answer y: 1  prediction: [ 0.11190744 96.48741     0.07706919  0.02081338  0.39823765  0.06598963\n",
      "  0.7257616   3.7617042 ]\n",
      "index: 213  actual y: 3  answer y: 3  prediction: [48.042503    0.0306373   0.05158411 52.174248    0.34459388  0.01979053\n",
      "  0.00710456  0.19731066]\n",
      "index: 214  actual y: 6  answer y: 6  prediction: [ 0.631128    0.08904787  0.06110164  0.43846527  0.45633298  0.41678277\n",
      " 98.01253     0.20710535]\n",
      "index: 215  actual y: 1  answer y: 2  prediction: [ 0.12027623 13.72893    74.947685    0.08757597  0.19310059  0.29189235\n",
      "  0.5800816   0.15980609]\n",
      "index: 216  actual y: 4  answer y: 4  prediction: [ 0.03381633  0.02329268  0.01429181  0.0210309  99.95707     0.03745731\n",
      "  0.01052187  0.03113829]\n",
      "index: 217  actual y: 3  answer y: 0  prediction: [33.21602     0.03669202  0.09224983  5.34982    11.763988    0.6735811\n",
      "  0.00944758  0.15699556]\n",
      "index: 218  actual y: 4  answer y: 4  prediction: [ 0.17579275  0.0455645   0.05237205  0.03369045 99.47261     0.61421424\n",
      "  0.03960631  0.27272746]\n",
      "index: 219  actual y: 2  answer y: 2  prediction: [ 0.02216534  0.02999536 99.93969     0.0371882   0.1117733   0.0260571\n",
      "  0.01381679  0.09561882]\n",
      "index: 220  actual y: 6  answer y: 6  prediction: [ 0.09014786  0.39753407  0.11612965  0.01659725  0.2855571   0.11037105\n",
      " 99.56215     0.03497824]\n",
      "index: 221  actual y: 3  answer y: 4  prediction: [ 0.25263265  1.5842015   0.8154683  13.360405   14.062794   10.261739\n",
      "  0.10492007  0.07425287]\n",
      "index: 222  actual y: 1  answer y: 1  prediction: [ 0.06366892 99.860245    0.02154215  0.02620179  0.16610104  0.22890717\n",
      "  0.01055462  0.02816275]\n",
      "index: 223  actual y: 7  answer y: 7  prediction: [ 0.978019    0.07981282  0.13727255  1.5108652   2.3608391   0.4100206\n",
      "  0.05759943 83.18789   ]\n",
      "index: 224  actual y: 5  answer y: 0  prediction: [93.1849      1.3504239   0.00835156  1.6010387   0.01798547  0.33193895\n",
      "  0.00548295  0.11517736]\n",
      "index: 225  actual y: 2  answer y: 2  prediction: [ 0.02877679  0.02781996 99.96412     0.02364864  0.03621428  0.05086057\n",
      "  0.02141832  0.03454086]\n",
      "index: 226  actual y: 1  answer y: 1  prediction: [ 0.01341999 99.912384    0.02693787  0.01110744  0.10840917  0.07758057\n",
      "  0.03014524  0.13057472]\n",
      "index: 227  actual y: 1  answer y: 1  prediction: [ 0.00580163 99.98636     0.01073116  0.00372623  0.01613906  0.0516286\n",
      "  0.01130402  0.01240586]\n",
      "index: 228  actual y: 5  answer y: 5  prediction: [ 0.0087895   0.01831883  0.11473355  0.0112473   0.01252774 99.90775\n",
      "  0.02267249  0.04622293]\n",
      "index: 229  actual y: 2  answer y: 2  prediction: [ 0.04941979  0.03459683 99.96977     0.02538752  0.08873613  0.025153\n",
      "  0.01150756  0.03220424]\n",
      "index: 230  actual y: 0  answer y: 4  prediction: [14.157775    2.893061    0.02938274  0.3844251  39.551704    1.6496081\n",
      "  0.04848872  0.07463663]\n",
      "index: 231  actual y: 5  answer y: 5  prediction: [ 0.01348218  0.02599563  0.01490463  0.02781299  0.07423643 99.923706\n",
      "  0.10303178  0.01629668]\n",
      "index: 232  actual y: 1  answer y: 1  prediction: [ 0.05832893 99.851265    0.04584649  0.06461515  0.28055775  0.08872108\n",
      "  0.00760902  0.01496211]\n",
      "index: 233  actual y: 2  answer y: 2  prediction: [ 0.04490024  0.22417991 99.79954     0.04195513  0.02793854  0.14657478\n",
      "  0.02213655  0.06481063]\n",
      "index: 234  actual y: 3  answer y: 3  prediction: [ 0.07549611  0.11287019  0.02409512 99.63487     0.06201196  0.04978727\n",
      "  0.23201287  0.1428298 ]\n",
      "index: 235  actual y: 2  answer y: 2  prediction: [ 0.02748852  0.03090741 99.95919     0.0376087   0.02835025  0.02683777\n",
      "  0.01516574  0.06991042]\n",
      "index: 236  actual y: 7  answer y: 7  prediction: [ 0.43397126  0.09183519  0.01520139  2.428684    4.812932    4.3147745\n",
      "  0.02576291 39.296047  ]\n",
      "index: 237  actual y: 1  answer y: 1  prediction: [ 0.08673754 99.84534     0.03185712  0.08944207  0.1003034   0.01992031\n",
      "  0.0108216   0.02657988]\n",
      "index: 238  actual y: 4  answer y: 4  prediction: [ 0.00342938  0.03630684  0.00635881  0.01164934 99.97208     0.02777832\n",
      "  0.01055656  0.03451362]\n",
      "index: 239  actual y: 3  answer y: 6  prediction: [ 0.02431682  0.07014818  0.09918064 18.265434    0.16944514  0.66597295\n",
      " 90.51826     0.38512614]\n",
      "index: 240  actual y: 3  answer y: 3  prediction: [31.965717    1.7294211   0.06189195 71.99784     0.10664856  0.0349957\n",
      "  0.00751119  0.07189874]\n",
      "index: 241  actual y: 0  answer y: 2  prediction: [ 5.880896    1.2190934  72.54894     5.104734    0.17731147  0.04470633\n",
      "  0.04726471  0.2013016 ]\n",
      "index: 242  actual y: 0  answer y: 0  prediction: [96.92409     0.0530751   3.2488835   0.02099666  0.10803379  0.09396144\n",
      "  0.01186249  0.9326489 ]\n",
      "index: 243  actual y: 7  answer y: 3  prediction: [ 0.33015683  7.1234837   0.10100233 65.473526    0.01601093  0.10399365\n",
      "  0.00794944  3.8012512 ]\n",
      "index: 244  actual y: 6  answer y: 6  prediction: [ 0.00695918  0.03569611  0.01138981  0.00237684  0.00916192  0.01823456\n",
      " 99.992455    0.00991939]\n",
      "index: 245  actual y: 0  answer y: 0  prediction: [95.04392     0.03159338  0.15873717  0.5974095   1.6963121   0.16648054\n",
      "  0.29316926  0.16705243]\n",
      "index: 246  actual y: 6  answer y: 6  prediction: [ 0.01424434  0.05677769  0.06661414  0.00341205  0.19237751  0.02662174\n",
      " 99.92609     0.03225586]\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.1424 - accuracy: 0.8097 - precision: 0.8375 - recall: 0.7760 - f1score: 0.8055\n",
      "loss: 0.142, accuracy: 0.810, precision: 0.838, recall: 0.776, f1score: 0.805\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXyU9bn//9eVPRAW2RRZDCogArIYV1zQY48LCi6o0Fal2lr9aVtrPVXP8SjV+uux2mpPa22xuHE8xrUesLhS3GpRAiIgICCyBEEWIWwJ2a7vH/ckDtmYhNyZTOb99JEHM/c2152J857P577vz23ujoiIJK+UeBcgIiLxpSAQEUlyCgIRkSSnIBARSXIKAhGRJKcgEBFJcgoCSQpmlmtmbmZpMSw7yczeb4m6RFoDBYG0Oma22sxKzaxbjekfRz7Mc+NTmUjbpCCQ1uoLYGLVEzMbCrSLXzmtQywtGpHGUhBIazUNuDLq+VXAU9ELmFknM3vKzDab2Rozu8PMUiLzUs3sATPbYmargDF1rDvVzDaY2Xoz+6WZpcZSmJk9b2YbzazIzN41s8FR87LN7DeReorM7H0zy47MO8XMPjCz7Wa2zswmRaa/bWbfj9rGPl1TkVbQDWa2AlgRmfa7yDZ2mNk8Mzs1avlUM/t3M/vczHZG5vcxs4fN7Dc19mW6mf00lv2WtktBIK3VHKCjmQ2KfEBPAP6nxjK/BzoBhwOnEwTH9yLzfgCcD4wA8oDxNdZ9AigHjows86/A94nNq0B/oAcwH3g6at4DwLHAyUAX4OdApZkdFlnv90B3YDiwIMbXA7gQOAE4OvJ8bmQbXYD/BZ43s6zIvJsJWlPnAR2Bq4E9wJPAxKiw7AacFVlfkpm760c/reoHWE3wAXUH8CvgHOBNIA1wIBdIBUqBo6PW+yHwduTx34Hroub9a2TdNOBgYC+QHTV/IjA78ngS8H6MtXaObLcTwRerYmBYHcvdDvy1nm28DXw/6vk+rx/Z/pn7qWNb1esCnwHj6lluKfCtyOMbgZnxfr/1E/8f9TdKazYNeBfoR41uIaAbkA6siZq2BugVeXwosK7GvCqHRdbdYGZV01JqLF+nSOvkXuBSgm/2lVH1ZAJZwOd1rNqnnumx2qc2M7sFuIZgP53gm3/VwfWGXutJ4LsEwfpd4HcHUJO0EeoaklbL3dcQHDQ+D3ipxuwtQBnBh3qVvsD6yOMNBB+I0fOqrCNoEXRz986Rn47uPpj9+zYwjqDF0omgdQJgkZpKgCPqWG9dPdMBdrPvgfBD6limepjgyPGAnwOXAQe5e2egKFLD/l7rf4BxZjYMGAS8XM9ykkQUBNLaXUPQLbI7eqK7VwDPAfeaWYdIH/zNfHMc4Tngx2bW28wOAm6LWncD8AbwGzPraGYpZnaEmZ0eQz0dCEJkK8GH9/8ftd1K4DHgt2Z2aOSg7UlmlklwHOEsM7vMzNLMrKuZDY+sugC42MzamdmRkX3eXw3lwGYgzczuJGgRVPkLcI+Z9bfAMWbWNVJjIcHxhWnAi+5eHMM+SxunIJBWzd0/d/eCemb/iODb9CrgfYKDno9F5j0KvA58QnBAt2aL4kogA1hC0L/+AtAzhpKeIuhmWh9Zd06N+bcAiwg+bL8G7gNS3H0tQcvmZ5HpC4BhkXUeJDje8RVB183TNOx14DVgeaSWEvbtOvotQRC+AewApgLZUfOfBIYShIEI5q4b04gkEzM7jaDldJjrA0BQi0AkqZhZOvAT4C8KAamiIBBJEmY2CNhO0AX2UJzLkVZEXUMiIklOLQIRkSSXcBeUdevWzXNzc+NdhohIQpk3b94Wd+9e17yEC4Lc3FwKCuo7m1BEROpiZmvqm6euIRGRJKcgEBFJcgoCEZEkpyAQEUlyCgIRkSQXWhCY2WNmtsnMFtcz38zsv81spZktNLORYdUiIiL1C7NF8ATBnaXqcy7B7f76A9cCj4RYi4iI1CO06wjc/V0zy21gkXHAU5GBr+aYWWcz6xkZK77Nq6yEFSvg009h6VLYuzfeFSWu7t1hyBDo1w9Wr4bFi2HTpnhXFa6sLDjqKBg8GEpKgn1euRIqKupfp2PH4Pc0cCBs3Biss26/92SrX0oKHHFEsM2cnG/+lot1h4PQXHABHHdc8283nheU9WLfMdQLI9NqBYGZXUvQaqBv3741Z7dqlZWwdi1s2xY8374dXn4Znn8eNkTt6Td3TJTGaGiorLb8O61vvxva56as09w1yIE59NC2FwQxc/cpwBSAvLy8hBglb8UK+P73Yf582LVr33mZmTBmDJx/PhxzDAwaBO3a1b0daZj7N99uv/giaBUMGQKHHNK2P5B27w6+fS9eDNnZMHQo9O8P6en1r/P118G39mXLgt/PkCFw2GHBN/umKCsLWiGLFwf1DB4c/C3n5DRtexI/8QyC9ex7T9nefHO/2YTmDtddB598At/7XvA/yMEHBx9M6elwyilBM10OnBn07Bn8JJP27SEvL/iJVZcucOqpwU9zSE8PPvgHDWqe7Un8xDMIpgM3mlk+cAJQ1FaOD/z1r/D3v8Mf/gA33BDvakREGhZaEJjZM8BooJuZFQJ3AekA7v4nYCbBPVxXAnuA74VVS0sqLoabbw6a6j/8YbyrERHZvzDPGpq4n/kOtKnvy+5w772wZg3Mng1pCXEERkSSnT6qmsns2XDHHfDBBzBhAoweHe+KRERioyEmmsFTT8GZZwYtgUcegSefjHdFIiKxU4ugGfzxj8GpeHPnBhf6iIgkErUIDtBnn8GHH8KkSQoBEUlMCoIDNG1acEHOt78d70pERJpGQXAAKivhf/4HvvWt5LugSUTaDgXBAXjvveAA8RVXxLsSEZGmUxA0oKAg6Pr5+uu650+bFlzqf+GFLVuXiEhzUhBETJwII0fCggXB88ceg5NOgiuvDMYJGjMmODBcZeVKePZZuOSSIAxERBKVgoDgiuDXX4ePP4bjj4exY+Gaa+CMM+D994MhIz78MHi+YgUUFQXjgmdkwF13xbt6EZEDo+sICIYx3rYNJk8OhvZ99lm49tpg0Lj0dBg1KmgZjB4dXDjWv3/QInjzTTj88HhXLyJyYNQiIBijHYLhefPzgxvJ/OlP+47tPngwvPVWMO767NnBFcQaRkJE2gK1CPgmCAYPDv7t06fu5YYNg3/8AxYtgssua5naRETCpiAAliwJbtrRo8f+l9WNOESkrVHXEEGLYPDgtn1rQxGR+iR9ELh/EwQiIsko6YNgwwbYvh2OPjrelYiIxEfSB8GSJcG/ahGISLJK+iCoecaQiEiyURB8GvsZQyIibVHSB8GSJTpjSESSW1IHgc4YEhFJ8iDQGUMiIkkeBDpjSEQkyYNg3brg39zcuJYhIhJXSR0E27YF/3bpEt86RETiKemDwAw6dox3JSIi8ZP0QdC5M6Qk9W9BRJJdUn8EbtsGBx0U7ypEROJLQaAgEJEkpyBQEIhIklMQKAhEJMklfRB07hzvKkRE4itpg8BdLQIREUjiINizB8rKFAQiIkkbBFVXFSsIRCTZpYW5cTM7B/gdkAr8xd3/q8b8vsCTQOfIMre5+8wwa6qS6EGwvWQ7Tyx4gj/P+zNds7vywmUvcEjOIXGrZ23RWqbMm8LUj6eycdfGWvOvOOYKnrjwCVIsab97iLRaoQWBmaUCDwPfAgqBuWY23d2XRC12B/Ccuz9iZkcDM4HcsGqKlshB8MSCJ7hh5g3sKdvD8b2O5+ONH3Pco8cxfcJ0RvQcEdrrlpSX8KOZP+KFpS/Umrdj7w7cnTEDxjDykJH7zFtTtIYnP3mSAV0HcMdpd4RWn7RO7s6cwjn8seCPvLriVSq8AoBjDj6G6/Ou5+JBF5ORmhHnKpNbmC2C44GV7r4KwMzygXFAdBA4UDXSTyfgyxDr2UciBkFFZQW3vnUrv/nnbziz35nc/637GdlzJAs2LmDsM2M5aepJ9OzQE4ChPYYydexUurfv3iyvvXHXRi5+9mL+WfhPvnvMd+mSte9Ifd3adeOKYVeQ2zm31rruToVXcOfsOxl+yHDOH3B+o1//y51fcs30axh5yEjuPuNuUlNSY1rP3Xl/7fv8seCPzCmc0+jXrZKRmsFFR13EdXnX1bmPbcHiTYt5ZO4jFGwoYPyg8Vw94mq6tusKBH97M1fM5M/z/kyFV/CDkT/gggEXVP9uC74sqHe7e8v3smHXBjpkdODiQRfTKbMTFV7BaytfY+KLE+mc1ZnOWTp9Lxa/POOXfOeY7zT7ds3dm32jAGY2HjjH3b8feX4FcIK73xi1TE/gDeAgoD1wlrvPq2Nb1wLXAvTt2/fYNWvWHHB9TzwB3/serFwJRxxxwJsL3Y69O/j2i9/mbyv+xg3H3cCDZz9Iemp69fyNuzZy77v3sqN0BxWVFby49EUOyTmEGRNnMKTHEABKK0p5ednLPPbxY3y1+6tGvf66onXsKdvDUxc9xfijxze6/uKyYk55/BRWfr2Sj77/EQO7DaxzuZLyEp779DmeWfwMg7sP5rq869hesp1x+ePYvHszZZVlnD/gfJ6++Gk6ZgbfIbbu2cpjHz/Gi0tfZG/F3n22V1RSxBfbv6BzVmfOOfKcJn/z3Lx7M69//jruzpAeQ2IKos5ZnbnymCuZMGQCRXuLeHTeo8xYPoOyyrJ9lstIzeDCgRdyzchraJ/enmcWP8O0hdPYsXcHAD3a9+Dq4Vdz0aCL+HLnl/y54M+8seoNKr2y3tce2HUg1+Vdx+mHnY5F7sO6p2wP+YvzmbZwGttLtu+zfEl5Ccu2LCMzNZOjux/Nxxs/JjM1k6O6HYWZ8dWur9iwawOHdjiUtJQ01hatJTstm+LyYrpkd+HsI87e5++xppN7n8x3jvkOORk51dMqvZI3Pn+Dl5a+VOt9k7pdNewqzux3ZpPWNbN57p5X57w4B8HNkRp+Y2YnAVOBIe71/4Xn5eV5QUH93z5i9eCDcPPNsHVr6x+GetW2VYx9ZizLtizj9+f+nuuPu36/63y0/iMuzL+QnaU7yTs0eO+XbVnGxl0bye2cyzEHH9OoGrLSsrh11K2M7Dly/wvXY23RWvKm5NG1XVc+/P6H1R/kEHxzf6TgEe6cfSdbi7dyWKfDWL9zPeWV5aSnpNOzQ09mTJzB+2vf58ev/pjeHXvT76B+VFRW8NH6j9hbsZcTep3AwTkH7/OaaSlpnHfkeUwcOpF26e2aXDsEYfjo/Ef55KtPYlp++dblLNuyjE6ZndhdtpvyynJO6XsKXbL3/YPbsmcLH6z7gPSUdLLTs9mxdweDuw/miC7BN5RFXy3ii+1f0CW7C9uKt2FmnH7Y6XTI7FDn61Z6JR+s+4Cvi7+mf5f+9OrYC3fnk68+YXvJdgZ1G0T/rv33WccwRvUZxfdGfI9u7bqxeNNipsybwpqi4EtXdlo2lw2+jAsGXECKpTBzxUxeXvYyp+eezqVHX0p2enZjf53SwuIVBCcBk9397Mjz2wHc/VdRy3xKEBbrIs9XASe6+6b6tttcQXDnnXDPPVBeDqmx9TKEYk/ZHm5/63ZuOP4GBnQdUGv+O6vf4ZLnLqHSK3nhshca9W1g/Y71/PT1n1Z/++/WrhtXD7+ac448J+auleb29uq3OeupsxgzYAx/vfyvpFgKZRVl3DjzRqbMn8JZh5/F7afczhm5Z7Bh1wamzJvCiq9X8ODZD9KjfQ8AZq2axX3/uK/6W+SQ7kO4Lu86hh48NC77VB9355017/D4gsfpmt2V6/Kuq/M9hiCk/1TwJ4r2FnHNiGsY1WdU9Tf5Sq/ktZWvMW3hNI446Ah+eOwP6dOpT4OvXVxWzLOfPkv+4nyKy4sB6NOxD9ceey2n9j21etuSPBoKAtw9lB+C4w+rgH5ABvAJMLjGMq8CkyKPBxEcI7CGtnvsscd6c7jxRvdOnZplU01WWVnpE16Y4EzGb33z1lrzH533qKfdneYDfz/Ql29ZHocKw/G7Ob9zJuNnTzvbJ708yfOm5DmT8dvfut0rKiviXZ5ImwQUeD2fq6EdLHb3cjO7EXid4NTQx9z9UzO7O1LQdOBnwKNm9lOCA8eTIgWHrjVcVfzABw+QvzifrLQs3l3z7j7zbnvrNu77x32cfcTZ5I/Pb1MH0350/I9YV7SO55Y8x9ItS8lMzWTaRdP47jHfjXdpIkkptK6hsDRX19CYMbBhA8yf3wxFNcGrK17l/GfO55JBl9Cvcz9+O+e3FN1WRLv0dhTuKKTPg324ctiVTB07lbSUUC/3EJEk0FDXUNJe3RPPFsGj8x5lbP5YhvYYymPjHuP03NMpryznw8IPAXhl+SsA3DrqVoWAiIROQdCC3J2bXruJa1+5lrMOP4t3Jr1DTkYOJ/c5GcOqu4dmLJ/B4QcdzqBug1q2QBFJSgqCFvTmqjf53Ye/48bjbmTGxBl0yuoEBOebDztkGO+tfY/dpbuZtWoWFwy4QGd2iEiLSMogiNcQ1P+37P9ol96O+//1/lpdPqf1PY0P1n3AqytfZW/FXi4YcEHLFiciSSspg6C4GEpLWzYI3J0Zy2fwrcO/RVZaVq35px52KsXlxdz9zt10zOzIqYed2nLFiUhSS8ogiMc4Qwu/Wsi6Hevq/aZ/at/gg3/RpkUHNBSCiEhjKQhayIzlMwAYM2BMnfMPzjmYgV2D8XfULSQiLUlB0EJmLJ/B8b2Ob/CeAacfdjqplsq5R57bcoWJSNJTELSAjbs28tH6j/b7Tf+u0XfxxhVvVA/9KyLSEpLyaqWWDoK/Lf8bsP8un0M7HMqhHQ5tiZJERKqpRdAC/rbib/Tp2KfRQz+LiLSEpA6Czi00jtv8DfM5pe8pukBMRFqlpA2Cjh1b5j4Eu0t3s6ZoDUd3Pzr8FxMRaYKkDYKW6hb6bOtnABo3SERaraQMgu3bWy4Ilm5eCsCg7goCEWmdkjIIWrJFsGTzElItlSO7HNkyLygi0kgKgpAt3bKUI7scqSEjRKTVSsog2L695c4YWrplqbqFRKRVS8ogKC6G9u3Df52yijJWfr1SB4pFpFVLyiAoKYGs2iNBN7uVX6+kvLJcQSAirVrSBYF7ywXB0i06Y0hEWr+kC4KysiAMWiQIIqeOHtXtqPBfTESkiZIuCEpKgn9bqkXQp2MfcjJywn8xEZEmUhCESGcMiUgiUBCEpNIrWbp5KUd30xhDItK6KQhCsrZoLcXlxWoRiEirpyAIyaKvFgEabE5EWj8FQUieXvQ0nbM6k3doXrgvJCJygPYbBGZ2gZm1mcBoiSDYvHszLy19iSuPuZLs9OzwXkhEpBnE8gF/ObDCzH5tZgl/QnxLBMETC56grLKMH+b9MLwXERFpJvsNAnf/LjAC+Bx4wsz+aWbXmlmH0KsLQdhB4O5MmT+FU/qeoruSiUhCiKnLx913AC8A+UBP4CJgvpn9KMTaQhF2EMxePZuVX6/kh8eqNSAiiSGWYwRjzeyvwNtAOnC8u58LDAN+Fm55zS/sIJgybwpdsrsw/ujx4byAiEgzS4thmUuAB9393eiJ7r7HzK4Jp6zwhB0E7655lwsGXEBWWgtcuiwi0gxiCYLJwIaqJ2aWDRzs7qvdfVZYhYUl7CDYXrKdHu17hLNxEZEQxHKM4HmgMup5RWRaQgozCEorSikuL6ZTZqfm37iISEhiCYI0dy+tehJ5HNMNeM3sHDP7zMxWmtlt9SxzmZktMbNPzex/Yyu76aqCIDOz+bddVFIEQKcsBYGIJI5YuoY2m9lYd58OYGbjgC37W8nMUoGHgW8BhcBcM5vu7kuilukP3A6McvdtZhZ6n0pJCaSnQ2pq8297e8l2ADpntdANkUVEmkEsQXAd8LSZ/QEwYB1wZQzrHQ+sdPdVAGaWD4wDlkQt8wPgYXffBuDumxpRe5OEeXeyor2RFoG6hkQkgew3CNz9c+BEM8uJPN8V47Z7EYRGlULghBrLDAAws38AqcBkd3+t5obM7FrgWoC+ffvG+PJ1CzMI1CIQkUQUS4sAMxsDDAayzAwAd7+7mV6/PzAa6A28a2ZD3X179ELuPgWYApCXl+cH8oKhtgh0jEBEElAsF5T9iWC8oR8RdA1dChwWw7bXA32inveOTItWCEx39zJ3/wJYThAMoVHXkIjIvmI5a+hkd78S2ObuvwBOItKlsx9zgf5m1s/MMoAJwPQay7xM0BrAzLpFtrsqxtqbRF1DIiL7iiUIIidcssfMDgXKCMYbapC7lwM3Aq8DS4Hn3P1TM7vbzMZGFnsd2GpmS4DZwL+5+9bG7kRjhN01ZBgdMhNyPD4RSVKxHCOYYWadgfuB+YADj8aycXefCcysMe3OqMcO3Bz5aRFhtwg6ZHYgpe3cvkFEkkCDQRC5Ic2syMHbF83sFSDL3YtapLoQlJRATk442y7aW6RuIRFJOA1+dXX3SoKLwqqe703kEIDwDxbrQLGIJJpY+jBmmdklVnXeaIILu2tILQIRSTSxBMEPCQaZ22tmO8xsp5ntCLmu0IR9sFjXEIhIoonlVpUd3D3F3TPcvWPkeceWKC4MYbcI1DUkIolmv2cNmdlpdU2veaOaRBH2MQJ1DYlIoonl9NF/i3qcRTCY3DzgzFAqCllYQeDuQdeQWgQikmBiGXTugujnZtYHeCi0ikLkHl4Q7C7bTYVX6BiBiCScplz5VAgMau5CWkJZWRAGYQRB1YBz6hoSkUQTyzGC3xNcTQxBcAwnuMI44YR5m8qqcYbUNSQiiSaWYwQFUY/LgWfc/R8h1ROqMIOgauRRtQhEJNHEEgQvACXuXgHBLSjNrJ277wm3tOYXahDoXgQikqBiurIYyI56ng28FU454VLXkIhIbbEEQVb07Skjj9uFV1J41DUkIlJbLEGw28xGVj0xs2OB4vBKCk+LtAjUNSQiCSaWYwQ3Ac+b2ZcEt6o8hODWlQkn7GME6SnpZKdl739hEZFWJJYLyuaa2VHAwMikz9y9LNyywhF211CnrE60kUFaRSSJxHLz+huA9u6+2N0XAzlm9v+FX1rzC7trSAeKRSQRxXKM4AeRO5QB4O7bgB+EV1J4wm4R6ECxiCSiWIIgNfqmNGaWCmSEV1J4Qm8R6ECxiCSgWILgNeBZM/sXM/sX4Bng1XDLCkfYB4vVNSQiiSiWs4ZuBa4Fros8X0hw5lDCUdeQiEhtsdyhrBL4EFhNcC+CM4Gl4ZYVDh0sFhGprd4WgZkNACZGfrYAzwK4+xktU1rzqwqCzMzm3W55ZTm7SnepRSAiCamhFsEygm//57v7Ke7+e6CiZcoKR0kJpKdDampsy2/YuYEVW1fsd7kde3cAuqpYRBJTQ0FwMbABmG1mj0YOFCf01VKNvTvZLW/ewugnR1NR2XD+VY88qq4hEUlA9QaBu7/s7hOAo4DZBENN9DCzR8zsX1uqwObU2CDYsmcLX+78kvfWvtfgchpwTkQSWSwHi3e7+/9G7l3cG/iY4EyihNPYINhVGgy6mr84v8HlNOCciCSyRt2z2N23ufsUd/+XsAoKU1OD4IUlL1BWUf/wSuoaEpFE1pSb1yespgRB1+yubC3eyqwvZtW7XFWLQF1DIpKIFAQN2F26mwsGXkCnzE4Ndg99Xfw1oCAQkcSkIGhAVYvg4kEX89dlf6WkvKTO5RZ8tYCD2x9Ml+wuzVSpiEjLURDUo9Ir2V22m5yMHCYMmcCOvTt48/M361x2TuEcTux9ou5FICIJSUFQjz1lewDIychhxCEjAFhbtLbWclv3bGX51uWc2PvEZqtTRKQlKQjqUXXGUPv09uRk5ACws3RnreU+Wv8RgIJARBKWgqAeVUGQk5FDVloWKZZSPS3anMI5pFgKeYfmNWepIiItJtQgMLNzzOwzM1tpZrc1sNwlZuZmFuqnaWOCYHfpbiAIAjOjQ0aHuoNg/RyG9hha3WoQEUk0oQVB5E5mDwPnAkcDE83s6DqW6wD8hGCo61A1tUVQ9e/Ovft2DVV6JR8WfqhuIRFJaGG2CI4HVrr7KncvBfKBcXUsdw9wH1D3uZnN6ECDYFfZvi2Cz7Z8RtHeIgWBiCS0MIOgF7Au6nlhZFo1MxsJ9HH3vzW0ITO71swKzKxg8+bNTSrG/cCCoENm7a6hOYVzAB0oFpHEFreDxWaWAvwW+Nn+lo2Mb5Tn7nndu3dv0uuVlQVh0OizhjLaA3V3Dc0pnEPnrM4M6DqgSTWJiLQGYQbBeqBP1PPekWlVOgBDgLfNbDVwIjA9rAPGjb1NZZ1dQzVbBOvncEKvE0ixpDr5SkTamDA/weYC/c2sn5llABOA6VUz3b3I3bu5e6675wJzgLHuXhBGMQcaBDXPGqqorGDxpsWM7DmyWesUEWlpoQWBu5cDNwKvE9zs/jl3/9TM7jazsWG9bn0aGwS7y3ZjGNlp2UCkayjqgrKdpTup9Eq6t2taV5WISGtR783rm4O7zwRm1ph2Zz3Ljg6zlqa0CKquIYDaXUPV9yDQzWhEJMElTed2U4OgSoeMDuwu3U2lVwLf3J5SN6MRkUSnIKjHrtJd1WcMQdAicJzismJALQIRaTsUBPWo2SKoOfCcWgQi0lYoCOpRq2sos0P1dFCLQETaDgVBPapuSlOl6nF1EKhFICJthIKgHvV2DUWuLq66Yb1aBCKS6BQE9ajrrKGq6RB0DWWkZpCV1oibIIuItEIKgnrsKt1FTnrDXUPqFhKRtkBBUI+6Th+Ffc8aUreQiLQFCoI6lFWUUVpRut+zhtQiEJG2IGmC4Ior4IMPYguC3WXf3KaySp1dQ2oRiEgbEOpYQ61Jz57BTyxqjjwKkJGaQXpKevVZQ0UlRRzc9eBmr1NEpKUlTYugMeoKAtj3LmVqEYhIW6EgqEN9QRB932IdIxCRtkJBUIfq21Smt99netXtKisqK9hZulNBICJtgoKgDvV2DUXuUlZ1Cqm6hkSkLVAQ1KHBrqHSXdUDznXO6tzitYmINDcFQR12l9Y+fbTq+c7SnRpwTkTaFAVBHfZ31pCGoBaRtkRBUIfqg8UZNQ4Wp0e6htQiEJE2REFQh12lu0hPSScjNWOf6VVnDalFICJtiYKgDjWHoK7SIbMDxeXFfF38NaAWgYi0DQqCOuwqq9S+MHgAAA+pSURBVDsIqqat37keUItARNoGBUEddpfu3m8Q6KY0ItJWKAiA8spyfvXer3h28bNAA11DkbuUFe4oVLeQiLQZSTP6aH2KSoqY8OIEXlv5GgO6DuDyIZfXGwTVLYId69UtJCJtRlK3CLbs2cJJU0/irVVvMTp3NMu3Lmfjro37D4Kd69UiEJE2I6lbBK8sf4WlW5byysRX6NauGydOPZH31rxX6zaVVaruUranbI9aBCLSZiR1EGzavQmA0bmjyUjNoF16O95b+16tG9dXiW4lqEUgIm1F0gdBu/R21d/+T+p9Eu+ueZfdZQ2fNQQ6dVSkOZSVlVFYWEhJ1U3F5YBlZWXRu3dv0tPTY14n6YOgR/se1c9PO+w0Jr89Gag9zhB8c9YQqEUg0hwKCwvp0KEDubm5mFm8y0l47s7WrVspLCykX79+Ma+X1AeLawbBqX1PxSP/1RUE0ccNFAQiB66kpISuXbsqBJqJmdG1a9dGt7AUBFFBcELvE0hPCZpTdQVBWkpa9UVk6hoSaR4KgebVlN+ngqDdN0HQLr0dx/U6Dqg98miVqu4htQhEpK1I2iBw91otAgi6h6DuFkH0dLUIRBLf1q1bGT58OMOHD+eQQw6hV69e1c9LS0sbXLegoIAf//jHLVRpuEI9WGxm5wC/A1KBv7j7f9WYfzPwfaAc2Axc7e5rwqypStHeIsoqy2oFwejc0dz3j/vokt2lzvWqg0AtApGE17VrVxYsWADA5MmTycnJ4ZZbbqmeX15eTlpa3R+TeXl55OXltUidYQstCMwsFXgY+BZQCMw1s+nuviRqsY+BPHffY2bXA78GLg+rpmhV1xDUDIKzjzibVya+wumHnV7nelUXlalFINK8broJIp/JzWb4cHjoocatM2nSJLKysvj4448ZNWoUEyZM4Cc/+QklJSVkZ2fz+OOPM3DgQN5++20eeOABXnnlFSZPnszatWtZtWoVa9eu5aabbkqo1kKYLYLjgZXuvgrAzPKBcUB1ELj77Kjl5wDfDbGefdQXBGbGmAFj6l1PLQKRtq+wsJAPPviA1NRUduzYwXvvvUdaWhpvvfUW//7v/86LL75Ya51ly5Yxe/Zsdu7cycCBA7n++usbdS5/PIUZBL2AdVHPC4ETGlj+GuDVEOvZR31BsD86RiASjsZ+cw/TpZdeSmpqKgBFRUVcddVVrFixAjOjrKysznXGjBlDZmYmmZmZ9OjRg6+++orevXu3ZNlN1ioOFpvZd4E84P565l9rZgVmVrB58+Zmec2mBoHOGhJp+9q3/+aswf/8z//kjDPOYPHixcyYMaPec/QzMzOrH6emplJeXh56nc0lzCBYD/SJet47Mm0fZnYW8B/AWHffW9eG3H2Ku+e5e1737t2bpbiqIOjWrluj1svJyCE9JV03pRFJEkVFRfTq1QuAJ554Ir7FhCTMIJgL9DezfmaWAUwApkcvYGYjgD8ThMCmEGupZdPuTXTJ7kJ6auP68CYOmchdp9+li2BEksTPf/5zbr/9dkaMGJFQ3/Ibw9w9vI2bnQc8RHD66GPufq+Z3Q0UuPt0M3sLGApsiKyy1t3HNrTNvLw8LygoOODaLnv+MhZtWsTSG5Ye8LZEpGmWLl3KoEGD4l1Gm1PX79XM5rl7nee7hnodgbvPBGbWmHZn1OOzwnz9htR1MZmISDJqFQeL40FBICISSO4gaKcgEBFJyiAoryxna/FWtQhEREjSINiyZwvQ+GsIRETaoqQMgqZeTCYi0hYpCEQkaZ1xxhm8/vrr+0x76KGHuP766+tcfvTo0VSdvn7eeeexffv2WstMnjyZBx54oMHXffnll1my5JvxN++8807eeuutxpbfbBQEIpK0Jk6cSH5+/j7T8vPzmThx4n7XnTlzJp07d27S69YMgrvvvpuzzorb2fTJefN6BYFI63PTazexYGPzjkM9/JDhPHRO/aPZjR8/njvuuIPS0lIyMjJYvXo1X375Jc888ww333wzxcXFjB8/nl/84he11s3NzaWgoIBu3bpx77338uSTT9KjRw/69OnDscceC8Cjjz7KlClTKC0t5cgjj2TatGksWLCA6dOn88477/DLX/6SF198kXvuuYfzzz+f8ePHM2vWLG655RbKy8s57rjjeOSRR8jMzCQ3N5errrqKGTNmUFZWxvPPP89RRx3VLL+npGoRrCsKBkPdtHsTaSlpdM5qWpqLSNvQpUsXjj/+eF59NRj4OD8/n8suu4x7772XgoICFi5cyDvvvMPChQvr3ca8efPIz89nwYIFzJw5k7lz51bPu/jii5k7dy6ffPIJgwYNYurUqZx88smMHTuW+++/nwULFnDEEUdUL19SUsKkSZN49tlnWbRoEeXl5TzyyCPV87t168b8+fO5/vrr99v91BhJ0yL4r/f/i/v+cR9zfzC3+mIyjRck0no09M09TFXdQ+PGjSM/P5+pU6fy3HPPMWXKFMrLy9mwYQNLlizhmGOOqXP99957j4suuoh27doBMHbsN6PkLF68mDvuuIPt27eza9cuzj777AZr+eyzz+jXrx8DBgwA4KqrruLhhx/mpptuAoJgATj22GN56aWXDnjfqyRNi+DywZeTYilcmH8hn2/7XN1CIgLAuHHjmDVrFvPnz2fPnj106dKFBx54gFmzZrFw4ULGjBlT79DT+zNp0iT+8Ic/sGjRIu66664mb6dK1VDXzT3MddIEQb+D+vHs+GdZumUpb69+W0EgIgDk5ORwxhlncPXVVzNx4kR27NhB+/bt6dSpE1999VV1t1F9TjvtNF5++WWKi4vZuXMnM2bMqJ63c+dOevbsSVlZGU8//XT19A4dOrBz585a2xo4cCCrV69m5cqVAEybNo3TT6/7trnNKWmCAOCsw8/i12f9GoDu7ZrnvgYikvgmTpzIJ598wsSJExk2bBgjRozgqKOO4tvf/jajRo1qcN2RI0dy+eWXM2zYMM4991yOO+646nn33HMPJ5xwAqNGjdrnwO6ECRO4//77GTFiBJ9//nn19KysLB5//HEuvfRShg4dSkpKCtddd13z73ANoQ5DHYYDHYba3bnvH/dxcp+TOe2w05qxMhFpLA1DHY5WNQx1a2Rm3HbKbfEuQ0Sk1UiqriEREalNQSAicZVo3dOtXVN+nwoCEYmbrKwstm7dqjBoJu7O1q1bycrKatR6SXeMQERaj969e1NYWMjmzZvjXUqbkZWVRe/evRu1joJAROImPT2dfv36xbuMpKeuIRGRJKcgEBFJcgoCEZEkl3BXFpvZZmBNE1fvBmxpxnLiqS3tC7St/dG+tE7Jvi+HuXudY+skXBAcCDMrqO8S60TTlvYF2tb+aF9aJ+1L/dQ1JCKS5BQEIiJJLtmCYEq8C2hGbWlfoG3tj/alddK+1COpjhGIiEhtydYiEBGRGhQEIiJJLmmCwMzOMbPPzGylmSXUnWnMrI+ZzTazJWb2qZn9JDK9i5m9aWYrIv8eFO9aY2VmqWb2sZm9Ennez8w+jLw/z5pZRrxrjIWZdTazF8xsmZktNbOTEvV9MbOfRv6+FpvZM2aWlUjvi5k9ZmabzGxx1LQ63wsL/Hdkvxaa2cj4VV5bPftyf+TvbKGZ/dXMOkfNuz2yL5+Z2dmNfb2kCAIzSwUeBs4FjgYmmtnR8a2qUcqBn7n70cCJwA2R+m8DZrl7f2BW5Hmi+AmwNOr5fcCD7n4ksA24Ji5VNd7vgNfc/ShgGME+Jdz7Yma9gB8Dee4+BEgFJpBY78sTwDk1ptX3XpwL9I/8XAs80kI1xuoJau/Lm8AQdz8GWA7cDhD5LJgADI6s88fIZ17MkiIIgOOBle6+yt1LgXxgXJxripm7b3D3+ZHHOwk+bHoR7MOTkcWeBC6MT4WNY2a9gTHAXyLPDTgTeCGySELsi5l1Ak4DpgK4e6m7bydB3xeC0YizzSwNaAdsIIHeF3d/F/i6xuT63otxwFMemAN0NrOeLVPp/tW1L+7+hruXR57OAarGmh4H5Lv7Xnf/AlhJ8JkXs2QJgl7AuqjnhZFpCcfMcoERwIfAwe6+ITJrI3BwnMpqrIeAnwOVkeddge1Rf+SJ8v70AzYDj0e6uf5iZu1JwPfF3dcDDwBrCQKgCJhHYr4v0ep7LxL9M+Fq4NXI4wPel2QJgjbBzHKAF4Gb3H1H9DwPzgNu9ecCm9n5wCZ3nxfvWppBGjASeMTdRwC7qdENlEDvy0EE3yz7AYcC7andNZHQEuW92B8z+w+C7uKnm2ubyRIE64E+Uc97R6YlDDNLJwiBp939pcjkr6qas5F/N8WrvkYYBYw1s9UEXXRnEvSzd450SUDivD+FQKG7fxh5/gJBMCTi+3IW8IW7b3b3MuAlgvcqEd+XaPW9Fwn5mWBmk4Dzge/4NxeBHfC+JEsQzAX6R86AyCA4sDI9zjXFLNKHPhVY6u6/jZo1Hbgq8vgq4P9aurbGcvfb3b23u+cSvA9/d/fvALOB8ZHFEmVfNgLrzGxgZNK/AEtIwPeFoEvoRDNrF/l7q9qXhHtfaqjvvZgOXBk5e+hEoCiqC6lVMrNzCLpUx7r7nqhZ04EJZpZpZv0IDoB/1KiNu3tS/ADnERxp/xz4j3jX08jaTyFo0i4EFkR+ziPoW58FrADeArrEu9ZG7tdo4JXI48Mjf7wrgeeBzHjXF+M+DAcKIu/Ny8BBifq+AL8AlgGLgWlAZiK9L8AzBMc3yghaa9fU914ARnAm4efAIoKzpeK+D/vZl5UExwKqPgP+FLX8f0T25TPg3Ma+noaYEBFJcsnSNSQiIvVQEIiIJDkFgYhIklMQiIgkOQWBiEiSUxCI1GBmFWa2IOqn2QaNM7Pc6BElRVqDtP0vIpJ0it19eLyLEGkpahGIxMjMVpvZr81skZl9ZGZHRqbnmtnfI+PEzzKzvpHpB0fGjf8k8nNyZFOpZvZoZOz/N8wsO247JYKCQKQu2TW6hi6Pmlfk7kOBPxCMogrwe+BJD8aJfxr478j0/wbecfdhBGMQfRqZ3h942N0HA9uBS0LeH5EG6cpikRrMbJe759QxfTVwpruvigwCuNHdu5rZFqCnu5dFpm9w925mthno7e57o7aRC7zpwY1SMLNbgXR3/2X4eyZSN7UIRBrH63ncGHujHlegY3USZwoCkca5POrff0Yef0AwkirAd4D3Io9nAddD9T2aO7VUkSKNoW8iIrVlm9mCqOevuXvVKaQHmdlCgm/1EyPTfkRwl7J/I7hj2fci038CTDGzawi++V9PMKKkSKuiYwQiMYocI8hz9y3xrkWkOalrSEQkyalFICKS5NQiEBFJcgoCEZEkpyAQEUlyCgIRkSSnIBARSXL/DyhMsNZJ0eboAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# output = classifier.predict_generator(test_set, steps=1)\n",
    "# print(test_set.class_indices)\n",
    "# print(output)\n",
    "size = y_test[:,-1]\n",
    "print(size.size)\n",
    "\n",
    "\n",
    "# predict 10 random hand-writing data\n",
    "y_predicted = model.predict(x_test)\n",
    "for x in range(0,size.size):\n",
    "    \n",
    "   np.set_printoptions(suppress=True)\n",
    "   print(\"index:\", x,\n",
    "          \" actual y:\", np.argmax(y_test[x]),\n",
    "          \" answer y:\", np.argmax(y_predicted[x]),\n",
    "          \" prediction:\", np.array(y_predicted[x] * 100))\n",
    "\n",
    "_loss, _acc, _precision, _recall, _f1score = model.evaluate(x_test, y_test)\n",
    "print('loss: {:.3f}, accuracy: {:.3f}, precision: {:.3f}, recall: {:.3f}, f1score: {:.3f}'.format(_loss, _acc, _precision, _recall, _f1score))\n",
    "# print('loss: ', evaluation[0])\n",
    "# print('accuracy', evaluation[1])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 손실 그래프\n",
    "def plot_loss(history):\n",
    "   # 선 그리기\n",
    "    plt.plot(history.history['loss'], 'y', label='train loss')\n",
    "    plt.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "   # 그래프 제목\n",
    "    plt.title('Model Loss')\n",
    "   # x,y축 이름 표시\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "   # 각 라인 표식 표시\n",
    "    plt.legend(['Train','Validation'],loc=0)\n",
    "\n",
    "# 정확도 그래프\n",
    "def plot_acc(history):\n",
    "  # dir(history.history)\n",
    "    plt.plot(history.history['accuracy'], 'b', label='train accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], 'g', label='val accuracy')\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc=0)\n",
    "\n",
    "plot_loss(history)\n",
    "plt.show()\n",
    "plot_acc(history)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "animal_model_man",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
